Test test test All right welcome everyone. This is, I believe lecture six in the data literacy course. This will be one of two lectures that I will be giving. My name is Jakob McKay. I'm a professor for machine learning and science in the Department of Computer Science. And I'm will be teaching these two lectures and then next year, so one year from now on, I will be taking over this this course and then likely, Professor Hannigan, we will be alternating that and subsequent Yes. Okay. Have you had everyone had a chance yet to to check in with the QR code? Great. So today, I guess should be lecture six and lecture five will be on linear regression. The I'm not showing the results of the course evaluation from last last week, because I guess they refer to a lecture that I'm not giving so that this sort of less useful for me, if you want to participate in the evaluation of this and I guess next lecture, that's that's the QR code for that. Alright, allow now. So what's the plan today? Today, as the title suggests, it will be all about linear regression. In multiple forms of that, firstly, we'll talk about why it's important. And then we'll basically try to present two different views of how linear regression models are important. One is sort of more motivated by machine learn perspective. And that, hopefully, for many of you, is a recap. Hopefully, these are things that you've heard in other courses. Hopefully, these are things that you already think you know. And the second part will be about viewing linear regression as estimation linear Gaussian models and that's typically the way they're thought of in statistical terms when statisticians use them. And finally, and a little bit more quickly, I will talk about how one would estimate linear regression models from a Bayesian viewpoint. I will not be going into great details on that last point in particular, but I think it's important to know about it, okay. So what is regression on a very general level regression analysis is the question the screen is not shared via zoom. Oh, it was shared, but now apparently I shared it. Thanks for the comment share Alright, so now this chat, but I guess it's a bit too much of a shed Alright Okay, alright. So regression is very generally regression analysis aims to really estimate the relationship between two things typically an input x that we call the independent variable and an output T. And they will call the dependent output or the target. And in machine learning terms somebody can we often say we want to critic T from X, okay, so, in some ways this is so generic, almost a supervised machine learning problems get. So, slightly more mathematically, what we mean by that is we want to find some function f, such that, which has some parameters omega, such that the targets T are roughly the same as the function output. So, we want a function to be good at predicting, that means, we want the function to be close to the targets and the input x will typically in m dimensional vector and T will be a scalar. So, will be a single number, I will not be talking about the case where t is a vector, but obviously, that that's also an important case. Some jargon, just because he will read it, x is often called the predictor, the covariant, the explanatory variable or the feature, and T is often called the target the outcome or in particular statistic is often called the response, because the way this is viewed is that you apply some manipulation x, and then you observe your systems, you see what happens, and then the response of your system will be will be T, okay. And crucially important D in particular in statistics, more so than in classic machine applications, the goal is not only to predict T, but also to understand why this prediction works in which aspects of extra particular predictive and important for protecting it. So here's one example. I took this from a textbook, the classical textbook by witnessing the Purani. And they and they present the data set where they are the recorded or someone measured, how much a company spent on advertising different media. So they spent advert, they spent money on advertising and TV, in radio and newspaper, and for particular product, and then they try to predict how advertising affects sales, okay. So this is here plotted as three different plots, because they have for every product, they have both TV advertising, radio advertising, newspaper advertising, so we didn't put x in this case will be three dimensional, and the output will be one dimensional will be how much how much of that product they sell. Okay, and this is he plotted his three different plots. But in reality, this is one data set. And these are like three different views showing a scatterplot of each input each of the three inputs with the output. And the kind of questions that might be interested in asking is the first one, is there any relationship between advertising budget and sales? Okay, so is there any evidence statistically, that spending money on advertising has any effect on sales? How strongest relationship, it's not important that there's a relationship because you obviously want to know, kind of how valuable that advertising is. So how strongly sales effective advertising, one would like to know whether advertising which media is most strongly associated with sales, so that maybe when they make decisions, the future, they can decide how to allocate the budget. It might be on a tech level, interesting, where that relationship is linear, or there's some saturation effects. And there might be interest in on whether they're synergies or redundancies among different media. Okay, so if you spent, let's say, we have 100,000 units to spend, does it make more sense to spend them on TV on radio or newspaper or some combination of stuff? Or and these are the kind of questions that people might ask in that particular data set. There's many, and obviously, sort of probably more interesting examples, I collected a few together. And one regression problem that you might ask is, well, how will the COVID case numbers develop in the near future? And you can think of that as a regression problem, where your inputs are the case numbers in the past. And your output is, you're trying to predict how the case counts will develop in the future. And you might not only have case counts as your predictors, but you might have some other things, you know, you might know that holidays are coming up, you might know that some measures are planned, right? So these are covariates that you can feed into your model. But the thing that you're trying to predict in that case will be a case count. A second classical example in statistics is trying to predict vote share in upcoming elections, I guess, then at least four nationwide elections in Germany, that's another four years from now, but obviously, there's tons of literature on trying to predict what percentage of the vote each party will get and the kind of inputs that will We'll go into that will be things like, obviously, inputs from surveys, but also economic data, right? Because for example, the economic data, and how well a country's doing economically is a strong predictor, whether the incumbent party or another party will be, will be getting a bigger share in the next elections. And exam example from science. And this is kind of close to an example of the kind of application that we work with. And in my group is, if you show, if you show an image to a person, and you measure the brain activity, what's what brain activity associated is evoked by an image, you might want to try to predict? What brain activity, how does activity in the brain changed when we show an image? And then you might say, well, if it changes, we might be interested in what aspect of the image caused that change? Okay. And I said the word cost here. And that's a very dangerous thing to do. Because often in regression models, we try to predict something from something. And it's often very tempting to assume that this represents a causal connection, okay? Because if we can predict T from x, it's very suggestive of us thinking, well, x must have must have cost T. Okay. And it's just really important to keep in mind that really regression is picking up correlations. And as we all know, but like to forget, sometimes correlation is not the same as causation. And you will find and you might even catch me doing that when we talk about regression models, that it's very, very tempting to fall into this language of causation. But important thing to keep in mind is really regression is just the way to deal with correlations in data. But they do not in itself imply any causal relationship. Another example is, you might want to predict how students will be doing in an exam, let's say you might have an online course, where you have information about different students, and you might have might want to predict whether how students will be doing in an exam. And that might give you feedback about what parts of the course, might be informative for that person. Okay. So these are the kind of questions that we can do with regression models. And obviously, there's many more examples. But today, I will not talk I will be only talking about one particular regression models, and that's linear linear regression. Okay. And in linear regression, very simply, the function that we use for prediction is just a linear function. So given some data pairs of input x, and targets t, we try to predict, try to model the data with a with a linear function, and I write this either written out as just the sum of each parameter omega with each dimension of X, or we can just write this as a vector product. So the vector product of w, sorry, of omega and x, and that prediction will not be perfect, but we have some residual term epsilon. And so the target is expressed as that linear function plus some noise term or residual term. And sometimes, and often this is it's more natural to express as a matrix form. So we can stack all our targets t as a long vector. In that case, it will be an n dimensional vector, and write this as T being a matrix x times omega plus a vector of residuals. Okay. So we think of x being all the different inputs, input vectors extact. So x will be a matrix that has n different entries, and has M different sorry, N different lines and has M different roles, okay. And that matrix has a name. And that name is important to know, it's called the design matrix. Okay. And one thing I find often confusing about regression is that there's all these terms, these jargon terms used by statisticians and I don't always find them intuitive. But the way to think of them, the way they come from is often of this classical experimental design. So someone designs an experiment, where they design what manipulations that will do, and then we'll observe the response of the system. And they design the design matrix. That's why it's called the design matrix. And they have freedom of choosing that that's why it's called the independent variable, but the target of the response is called the dependent variable, because that's just what the system does what they observe. You don't control the measurement, but you can control the manipulation. But I will not I will try to avoid matrix notation as much as possible, but often, I mean, the compact way and sort of the easy way to write this would be in in, in this in this matrix form. Okay, so why this is just me some trying to argue why why is linear regression so important? Okay. And you might think, well, Vinnie aggression is so super simple, and we've got all these much fancier and better machine learning tools and regression fields. Why would you ever want to use linear regression? Okay, and there's a few reasons. The first one is because it's so simple. You should always always try it first. That's a can. I mean, this is not maybe not always, but almost always linear regression is the first thing to do. Even if later for many reasons to decide that it's too simple or not appropriate, it's often a very fast and good baseline. So it's, I often get surprised how people use extremely complicated analysis tools. And then you ask, have you tried linear regression? And the answer is, oh, no, I didn't try it. I didn't think it would work. And then you try it. And it turns out that it works almost as well. And I'll show you some example on the next slide. Where of particular famous example of that. The second reason for why linear regression is useful is because the statistical properties, for example, how to do hypothesis testing on it have been extremely well studied. There's probably few topics and all statistics that are as well mapped out as frequencies theory of linear linear Gaussian models. Linear Models are often easier to interpret that nonlinear models. But I will point out that it's not even linear models can be deceptively difficult to interpret, okay. But in some sense, if you can interpret a linear model fit your data, it can be often even much harder to interpret something nonlinear. So even in that case, as a suggestion is to start off with a linear model, and try to interpret that before you move to more complicated models. And finally, if you understand linear regression, or well, then that can be often be thought of as a building block for building more complicated models. Okay. Here's one example. This is not quite linear regression, but generalized linear regression. And we will talk about this in the next lecture. There was a paper in Nature last year, or a year before, where they used a multi layer deep neural network to predict aftershocks and earthquakes. So given the earthquake, giving, you know, half some information about the earthquake, you're trying to predict whether it's going to be a second earthquake, and aftershock. And there was a Nature paper showing that they got this very good prediction performance with their deep neural network with an area under the curve of point 847. So this was not quite a regression problem, a classification problem. And, and they use this very big, deep neural network. And then, a few months later, there was second paper that showed that you get exactly the same prediction performance, just using a linear model, a linear classification model, okay. And they call this one urine, because you can think of a linear regression or a generalized linear model as a one neuron neural network. And so they published a second native Nature paper saying, Well, you could have done all of this with linear regression, or, in this case, logistic regression, rather than with I don't know, a mini layer deep neural network, okay. And that's the kind of thing you don't want to happen to you that you do something very complicated. That's not impossible to understand, but difficult to understand. And then it turns out, there was a much, much easier answer. Okay. Are there any questions about anything I've said so far? Alright. Ah, okay. I would have to look it up in the capsule. But basically, there was also in this particular paper, there was also the argument, in particular by the authors of the original paper that said, that the liberal network doesn't only predict, but it provides insight into sort of what features they did this visualization, which is shown here and he, and they argued that from the neural network, they can get this visualization that shows what region of the parameter space particularly important, and it turned out that you get exactly the same visualization and F or very similar one from the linear model. So in that case, the argument was not only prediction performance, the same but also the insights that you get are, are the same, just one is one line of scikit learn and the other one is a lot of parameter tuning. Alright, linear regression, the machine learning view, I will try, I will rush over this a little bit, because my hope is that many of you have seen this or aspects of this, okay. And I want to start off with by pointing out the obvious, and that is that you can use linear regression, even in cases where you're trying to estimate a nonlinear relationship. So why is this this is an example this is taken from the bishop book on pattern recognition and machine learning. So here, you can just write your linear function your this is the prediction function, use y of x and parameters omega, and you can write this as omega plus omega 1x plus omega 2x squared, etcetera, etcetera, times omega m times x to the m. Okay, so this is a nonlinear function because there's polynomials in it. But we can write this as a linear sum of the omegas and x to the power of AI. Right? So we can just basically really able our data and say this is a linear function, where the parameters is linear in the parameters and apply to some nonlinear function of the axes. or you can redefine this as I write this as a linear term of omega vector and some feature function, phi of x, or I can we label this as omega times some new, some new variables that are just called it. Okay? So basically, if it looks like there's a nonlinear relationship, you can just change your data, and then do linear regression in this transform space. Okay, and that's a common trick. And I will get to back to that later. So even when your problem seems nonlinear, you can still often use linear techniques to deal with it. So how do we find this omega? Well, the classical, the classical approach in machine learning is trying to say, well, let's minimize the mean squared error. So let's try to find the omega such that our predictions, if you look at the residuals, so the difference between the prediction and the targets, and we square them, and we sum them up, then this is the total squared error, this term of a half year if sort of more is less important. And then let's try to find the omega omega that minimizes okay. And I will go through this calculation, it's the only calculation I will go through probably in both lectures, and hopefully the calculation you've seen, but the calculation that's so canonical, that I think it's very, it's useful to to know it well, and to be able to do it quickly. Okay. So what we want to minimize is this mean squared error, and we can write this, and this is just what I had in the previous slide. If you go from this equation to the next equation that looks like we're going in a weird direction, because we, this is just the number. So a number squared is the same thing as the number times the number to its transpose, right, for a single number, that's the same as the transpose. So we can write A squared as A times A transpose. And when we've written it like this, we can pull the transpose into inside the brackets, so this transpose moves here, then we have to switch the order of them, this is a number, so the transpose goes away, and then we write it out like this. And then we can multiply out all the terms. This half should not be here, sorry, I copied this. But we can, we can just multiply out all the terms, and we get this first term, which is quadratic and omega, we get the second term, which is, which is linear and omega, and we get this third term, which doesn't depend on omega. And then we can forget this half term, but it will drop out anyway. So we get these terms, and then we can pull this into a form where we where we pull together, all the terms that are inside that are quadratic, so this is like a quadratic term in omega omega transpose times some matrix times omega minus omega transpose times a vector plus a constant term. Okay. So this is really just massaging this error term such that we can write this as a quadratic form, Omega transpose matrix omega plus omega transpose vector plus a constant, okay. So nothing has happened yet, I'm just I've just rewritten this as a quadratic form. And then I, either you know how to do derivatives on in multiple dimensions, or this is actually useful just to remember, if you have a quadratic term in omega of that form, then the gradient with respect to omega is given by basically this form, so we drop off the second omega and take a times two, and here we just the second term survives. And this is very, very, this is very similar to the derivative calculations for numbers, this is just extensions to matrices, okay. But if you if you don't know how to do derivatives on vectors, I'm just trying to remember that this is the derivative of a compact form. And now I've just substituted back in what I had earlier. So this is the matrix A, and this was my vector b. And so I get this being the gradient, I can set it to zero, and then we get the omega that minimizes the sum of squared error, or the mean squared error is given by solving this for zero, so this is just the inverse or we pre multiply by this matrix. And we take this vector and pre multiply it by this of this matrix, okay. And this form and form like equations like this, you will see very often, and this this this, there's one thing that I find useful in fine to remember them, and that in particular, if the sets, let's now assume our date has zero mean, right? The sets, we can always do this easily by just centering them, right. If they don't have their mean, we could subtract the mean and we'll have zero mean, if they have zero mean, then this term here is just the covariance of my inputs. And this term here is just the covariance of the inputs with the outputs. So the omega, the linear regression, omega is just the inverse. Have the covariance of the inputs times the covariance of the inputs with the outputs. And let's say you're in a scenario where your inputs are have unit, everything is normalized. So inputs have unit variance, and they're not correlated, then this covariance here will just be a unit matrix, right? If everything if you if you are, if you've normalized the data, so you've subtracted the mean, and if divide, and if and all the variances at one, then this will be a unit variance. So omega will then just be the covariance for the covariance of your inputs with the outputs. And if you think about this, this makes a lot of sense, right? If you have, if you have an input that strongly correlated with your prediction with a target, then you want to give a very big parameter to that. So when you have some dimension of that, that strongly correlated with your approach, then you give it a very big parameter. If you've got one that's not correlated, so the covariance is zero, then you'd set the weight to zero. And then this term here, just correct for the fact that different inputs might have different variances might correlate with each other. Any questions about this? Awesome. Yes, so and you can, and you can, you can see the answer on the slide even. Because what happens if you change? If you change your, if you rescale your data, then you change in the covariance. So hence, you changing the this this bit here. So let's say you rescale, some your first dimension by 10, you make it 10 times bigger, then the first entry in the Koreans get 10 times bigger, hence, this, you divide by 10. And so your first parameter gets 10 times smaller. And that makes sense, right? If you try, I don't know. Maybe, let me think of an example. If you try to predict. You try to predict the height of a person by their weight, okay. And then you change your units of measurements from kilograms to milligrams. And then your prediction of the height would go up by a factor 1000s. If you don't correct for that by dividing by 1000s, right. So if you if you change your if you change your data by by multiplying by 10, by something, you have to correct for that scaling. And this empty divide here, because we really correct for the scaling of the data. So if the data is unit scaled, then there's no correction, but the data is has bigger variance or smaller variance, and we have to have to adjust for that. Okay, here's a question to the group. If we do linear regression, if we do PCA first, what difference will it make? For some someone? I mean, I think that's your that's. So if you do if you do PCA, What difference do you think they will make to the output of a linear regression? Well, I guess PCA will only be covered afternoon regression. So I think that hopefully, there will be more points of that, but, but let me try to answer this. So what piece of piece there's sort of two things that piece that you can do with PCA? Right. One is you have some high dimensional data and you would use it down to a low dimensional data by by by only taking leading data that corresponds to leading Eigen values, okay. And if you do that, then you would effectively shrink the parameter space over which you fit your regression model. And that will make a difference. Okay. Another way that people sometimes say they do PCA is that they take the data and they widen it. So they basically pre multiply by the square root of the inverse of the covariance. And then you get out some data that has unit variants, no correlations. And if you do regression on the data, at least if you do this type of regression, the it will not make any difference whatsoever. And the reason for that is that this term here, add just basic correct any any effect of taking PCA, so rescaling your data for linear regression like this should not make a difference. So the equation would simplify but would simplify. So overall things won't get easier if you do, because what what it gets simpler by exactly the same amount of work, you have to do that PCA. So you just shift work somewhere else. There are reasons for doing PCA. And if you do prior, if you apply as any model, for example, then whether you do PCA or not, doesn't make a difference. But basically, what you're doing PCA or pre multiplying by the covariance is, is two different ways of doing exactly same thing, mathematic. Change, yeah. Okay, but I will get to this point, but you make an excellent point that your parameters that you get out, always depend on the scaling of your data, that any parameter can only ever be evaluated in the context of how your data scales. So because the scaling is always arbitrary, you can say unit variants, one is maybe a particularly nice scaling, but whatever scaling there is, the parameters always depend on the scaling of your data. And but it's true that often people scale everything to have unit variants, because then it's, then it's easier to compare two different parameters, right, if one parameter has variants, 1000, the other one has variants, one, then even if they have the same importance, for prediction, they will have very different regression, a very different parameters. Alright, maybe going back slightly into this into the machine learning view of, of linear regression. And going back to the example that I showed, if you have this kind of nonlinear function, and you try to predict, and you try to capture using this sort of polynomial expansion, then there would be a free parameter hyper parameter, saying how many, how many inputs use, right how many predictors use, so in the extreme case, you would just use a constant line, that would be a prediction, you could say, I take, I take a line with a slope. So that would be taking to a two dimensional input, and you can take three, up to nine as a polynomial, and then you would get you would be using linear regression to fit this extremely complicated polynomial to your data. And obviously, that's not a satisfactory output, because you would like, you know, think this is a good predictor of values that are in between your measurements, okay. And any machine learning, the classical answer that you would do in that kind of situation is you would be using a held out test set, right, you would say, well, the mean squared error that about earlier on my training data set, will always go down, the more predictors I use, I'm down to, when I have nine data points and try to fit them with nine dimensional inputs, then I would get down to zero error. But obviously, what I care about when I want to do predictions at some point is that it will be my my generalization performance. So to assess that you would be using independent test set. And in this case, here would suggest that it did not take nine dimensions as our nine inputs, then you would get at a gigantic tester. So somewhere between three and six, based on this analysis, you would say is the is the is the number of inputs you want to use. So in machine learning, that would be the way you try to deal with model complexity. But there's no, there's no statistics test here directly, right, this is just you there's not there's not a hypothesis test or non explicit hypothesis test. A second way to deal with trying to control your model complexity. Oh, and I should say that the optimal model that you get, or the best, the model that takes is minimum error here will always depend on how much data you have. So when you have more data, you might suddenly favor a more complicated model. And one just has to be really careful that when you compare two different data sets, for example, and you say in the one data set, a more complex, more expensive data than this other data set, then that doesn't mean that the one dataset is more complex if it's a bigger dataset, right. And I, I mean, I didn't put a slide on this, but I know one example, that I can think of is a paper which looked at the importance of feedback projections in the brain. So these are projections which which go from higher brain areas to lower brain areas. And they argued that in this they had this difficult task and they argued that this difficult task, humans use the feedback connections It's much more. Okay. But then it turned out that they just collected a lot more data on the difficult task. So therefore, when they did the cross validation, it gave them a much more complicated model with feedback connections on the big data set. And on the smaller data set that they had, it gave them a model that was very simple, because it didn't have much data to fit it on. And that, that didn't use the feedback model. So always take into account that the best model you get out of this across validation, or even administratrix will always be a function of the size of your data set. Okay, and you have you have some, if you have a small data set, almost every statistical tool will favor a simple model. And that doesn't mean that a simple model on that data set is kind of like ultimately better than a complex one, if you were gonna get more data. So as you collect more data, what the best model is might change. Okay, there's a second way in which in machine learning, you deal with trying to control model complexity, and you add a regularization term. So you add a second term, that in this case just controls the variance or sorry, the absolute the norm of omega, and it will be a term lambda here, that trades of how much you care about prediction. So you mean squared error, and this and this regularization term. And then you can carry out exactly the same mathematics as before, to find out that the optimal, the optimal omega at the the omega that minimizes this term will be very similar to before, but now we get this extra term here, right? It's exactly the same equation that you saw before, the only thing has changed is that we have this lambda times the identity matrix here, okay. But everything else is, is the same as before. One thing you might observe is that this matrix here is always guaranteed to be a full rank, right? Because this here is always non negative definite. And this we add something to it. So this numerically is also better behaved than, than the solution. Okay. So in this particular case, just going back to the example, if you did take lambda, or log lambda of minus 18, that's the kind of fit you would get, if you take a log lambda of zero, so much stronger realization than you would over regularize right, you would take a solution that's very close to zero. And that raises the question of what what lamda one should use. And you can use exactly the same trick as before, that you can have a training, a training set. And then, in your training set, the more or less you regularize, the better your training error gets. But your test error goes. Sorry, your training error goes, your training error gets worse, the more realer it regularized but your test error will have some minimum and somewhere in between. And ask me for note how extremely big this minimum years, right? So it's very, very shallow, if someone asked you what's like the right model complexity, given the data set, and all you can say is, well, it's somewhere between here and here. But these are typically not sharp. So it's typically there's not a single, very finely tuned value that tells you what the right amount of regularization is. Okay. And to close off this, this basically, this this view of this machine learning, you have linear regression, and we showed polynomial basis function here, but you can often use other basis functions. So polynomial regression is what I use, you might be using some some Gaussian bumps, or some sigmoid so often, by thinking about what the right basis functions are, you can often get very far by using linear regression applied to your data after you push it through some basic assumptions. Alright, before we get to the next part, are there any questions on anything I've said? Alright, so let's go back to the more statistical view. And it's very simple in the sense that well, in the statistical view, we just say we model our data as before, and we just assume that now epsilon is additive Gaussian noise. So we can write this formula, we assume that epsilon each error term is normally distributed with mean zero and variance sigma squared. So our distribution over the targets given the input x, our parameters, omega, and I will sorry, I defined this on next slide. I use PETA to be the inverse of sigma squared, I should have put this I'd like Peter to the minus minus sigma squared, or you can think of sigma squared. So our distribution of a t given these parameters is just a Gaussian. The mean is given by the by the regression model, and the variance is just given by sigma squared. And then we can say, well, if you want to do and then this becomes an estimation problem and you forget how to do estimation there's different ways to do estimation the simplest way, not always the best way to do estimation is to maximum likelihood estimation. And it turns out that if you do maximum likelihood estimation in this model, then you get a log likelihood, the log likelihood of the data given your parameters omega, which has a term in it, which is very, very, which would you recognize from before, because it's some constant. And this is just the mean squared error that we had before. Okay. So maximum likelihood estimation with a Gaussian noise model is exactly the same as minimizing the squared error. And therefore unsurprisingly, our our term and the, the, our solution, the omega, that maximize the likelihood is exactly the term that we had before. I just realized that I switched, I switched, the should be a T, I tacitly replaced the T's within the next notice down. So really, nothing has changed so far. So this statistical, the maximum likelihood estimate is really the same as mean squared error. So what have we gained? Well, we've really gained and all the machinery that you've heard about in the last couple of lectures, and how to do hypothesis testing, is one that you can directly plug into this model. Okay, so everything that you've heard about calculating error bars, calculating, calculating, doing hypothesis tests, null hypothesis test, you can do all of that in a linear Gaussian model. And I should say that this extremely big theory of estimation, linear Gaussian models and that all these terms fit, okay. And there's terms like Innova and many, many other terms, and it's all kind of special language that destitution has come up with, because it corresponds to many special cases of this model. But in the end, you know, like this, if you try to read these papers, or try to eat these methods, most most of them will be special cases of the general theory of estimation that have heard about in previous lectures applied to a linear small, okay, so it's very important that literature because it deals with many cases in depth, but conceptually, most of what's in there is really the ingredients for it, you've already you've already seen. I should I've changed notation a little bit, because in statistics, in contrast to machine learning, maybe the most common notation is that they don't use omega, but they use PETA. Okay, it's just, it's not always the case, but they almost they very often call it beat. Okay. So the most common way is that they call the variance sigma squared, and the parameters they called Dieter, and the outputs, they called y and intracity. Colleagues, it's just, I mean, it's arbitrary, you could use any letter, but these are the most common. I should also say that the first entry of x is almost always a one. So when you think of this 10 dimensional input, let's say x, the first one is almost always a one. So why is that? Because then we get an input, the first entry of data is just multiplied by one. And this gives us a constant offset. So if our data does not have zero mean, then that's a constant offset 10 Just move the prediction to the mean prediction. Okay. So, I mean, if so you're, and that's why this offset term is basically as an easy way to correct for having non nonzero mean data. Because otherwise, if you didn't have that term, right, if you input zero, then your output would always be zero. So for input zero, you would always predict output zero, and that might not be what you want to do. Okay. So this is what I've just said, this is this extensive classical statistical theory for linear Gaussian models. For example, dealing with hypothesis tests, the most common hypothesis is to ask to set up the null hypothesis that all the betas are zero. And if you if we ignore the offset, the first one for the moment, that null hypothesis would mean that we can't predict the output from the inputs. Right? So the null hypothesis in regression is often is there any effect? Can we predict the output from the input at all? Or are they completely independent from each other? confidences will would be how well can we estimate each of these parameters better? Okay, so in sometimes you might find that one of those parameters, you can predict extremely well or you can constrain extremely well, the others you cannot constrained at all. And that's important to know because if you later want to interpret them, you need to know how well they actually constraint regulate. And this is, this is the point we were talking about a little bit earlier. So one question I have often is how, what's the interpretation of these of these parameters? Okay. And we can, I can, so if I do this on the board, I guess the people online cannot see it. Do we have? Okay? So if if we have this we have our prediction model, right? Let me use it sloppily for now y equals beta one times x one plus beta two times x. Okay? Peter to tennis. So if Peter one is very big, then it will have a very big effect on the prediction. Right. But only if x one and x two have the same variance, because it could be that x one has a tiny, tiny variance. So that even a big big beta one does not, does not result in a big output for y. Right. So how we interpret these data values will always depend on the scaling of the axes. And if we want to say that the betas tell us how important different inputs are, then we can really only do this if these inputs all have unit variance. Because that otherwise, we can't compare the absolute size of the betas. But it can be more complicated. So if the x one and the x two is very strongly correlated, so we have two inputs into the model that are very strongly correlated, then it becomes very hard to interpret these features. So why is that? Well, because let's say x one, let's say, these are very strongly correlated. And then I can basically predict y either from x one or x two, if x one and x two are basically the same number, then it's arbitrary whether I make a big big one or big Peter one or big beta two, okay, so when the inputs are strongly correlated, then it becomes very hard to interpret the the regression coefficients. And the extreme case of that is called Colony arity. Right. And that's basically refers to if for two variables, one is a copy of the other one, if I write this, this is x one, and this would also be x one, if I have two inputs, my model that are basically the same, then I can just write, I can take twice this and half half the second beat the one, I get the same model, right. So this interpreting, and what I want to convey is that interpreting regression coefficients becomes very difficult when your inputs are strongly correlated. Because then we have this arbitrariness, we can get the same prediction using different different models. Alright, finally, there's also a big one thing that's important that will not not cover today, but I think it's important for you to be aware of it is that it's, we always have to be careful whether when to check whether the model is valid, if we predict. And if we use a model for prediction, and we try to trip the parameters that we get, then obviously, any statistics test, for example, that we do, we'll make the assumption that this model that we wrote down is correct. So that we have a linear relationship here. And then we have Gaussian independent noise. And if that assumption is wrong, any statistics tests or hypothesis tests or confidence interval that we have might also be wrong. Okay, so you might get this beautiful test that says, I can reject the null hypothesis or I cannot reject the null hypothesis. And that and then it turns out that the errors are completely non Gaussian. And then that, that infants will not be will not be valid. There are okay, I thought about having a slide on this. I didn't. So they are, um, this basically, we can think about, okay, this basically three assumptions we're making for the errors, we're assuming that the Gaussian and there's tests for Gaussian 80 we assuming that they're independent. And we can just check weather independent by computing the correlation between them. So we have and this is often done in time series models that you plot. So you have your so let's say you have your 10 different data sets. So one one thing people often do is observe, this is like they look at the different data points that say we have five data points or seven in this case, and I just plot the excellence okay. And then if the epsilon look like this, for example. So let's, let's say that these are let's say that these are the errors that it has, then it looks like the errors, if they were independent, independently distributed, then they should all be scattered around zero, and there should be no relationship between one and the next. And if you look at like this, it seems like each error is very similar to the neighboring error. So we have a positive error here, we like you all to have a positive error and the next one, May with this exception. So in this case, the errors if you compute the correlation between the error and the next error, that will be strongly correlated. Okay, so we can so we can check Gaussian at by plotting the distribution of errors. just plot the distribution of the epsilon, get something like this, and do a test of Christianity. And we can check independence by by looking, for example, at the correlation between subsequent errors. And the third assumption we're making is that all the errors are equally big. Right? Because we all we assume that the errors all have this, the same variance sigma squared, so we assume that on average, the errors have all the same size, or the same variance. And if you observe the errors to be something like this, that these errors are small, and then they get bigger, then that suggests that there's not a constant sigma squared, but there's a sigma squared, that changes with time. Okay, so the three things checking in for Gaussian sanity, by plotting it, looking at it, and doing a test of Gaussian at looking of independence of errors by for example, calculating correlations, or doing an independence test. And the third one, checking for the variance being constant. And that often also is plotting it is always a good idea. And then maybe trying to even fit a time dependent variance it would be a histogram. Sorry, yeah. So you take, you take a histogram of all the errors, and you look at whether it's particles. Okay, I have a question from the online chat. It says why do they explain so much on the board when you know that we cannot see it? i Okay, given that I don't have a whiteboard, I can write on I can either not give an explanation to a question, or I can at least give a question that the people here can see I will take a picture and I will try to post it to you. But I thought the half explanation that half of you can see it's better than no explanation at all. I hope that answers your question. All right, alright. The Bayesian view. Okay. So until now, we've, I've, we've dealt with linear regression as minimizing the mean squared error, or maximizing some, some likelihood. But obviously, there's, there's there's another approach to this, and that would be using Bayesian statistics. And I, I think I agree with the general spirit of these lectures that when you can do something in a Bayesian way, then this is often preferable. And in the case of linear regression, this is a some aspects of this actually, relatively straightforward. And we can, as usual what we need when we do a Bayesian treatment, we need a prior in this case, it will be a prior on the parameters. Note that I switched back from betas to the omegas. Because I'm thinking more of them. Well, the domain whether use, because it's more more common in machine learning to call parameters omega. So we have a Gaussian prior on the omegas, we have an Omega on each in on each parameter, we have a Gaussian prior on each. On each omega, I just assumed that they all have the same variance, and they call the variance alpha to the minus one. And so I just assumed that they all have the same variance, but I could use any any other normal prior on them. So that in that case, my prior distribution over omega given this variance parameter is just given by a multivariate normal distribution over over the parameters. And then as before, I can when before I did the maximum likelihood estimate, in this case, I can compute the maximum a posteriori. I'm not going through the equations because the exactly the same equations that we had before. And as we found this regularization term before, that multiplied by lambda times the identity matrix. When we do this, the map estimate in the Bayesian sense, we get the identity matrix times these, the ratio of the the noise variance, or the noise position and the prior position. So beta here is one over sigma spread. I know that it's a little bit confusing. But I use Peter for parameters earlier in this interlude. But it's just, I thought it would be I don't know, I I kept. I hopefully it's clear from context that in on whenever he I have omega for my parameters, and then PTS is not it's not it's not the same data that we had before. But the most important thing here, aside from the rotation is actually that this regularized view of linear regression, one to one maps onto the Bayesian view of, of linear regression. So mean, having a mean squared error with a variation term is exactly the same as having a prior a Gaussian prior on the parameters. And we get exactly the same answer, just the interpretation of the answers different to before. And you might remember that in a Bayesian treatment, the getting the maximum the map a plethora is is okay, but it's nobody ultimately want to want to have what you really want to have when you can, instead to get a full posterior distribution over your parameters. Okay, so given our data, D, and given our hyper parameters, alpha and beta, we want a full posterior distribution over our parameters. In that case, because everything is linear Gaussian, that posterior distribution, actually is also Gaussian. So in that case, the posterior distribution will be a normal distribution over parameters omega, and will have a mean parameter, a mean, a posterior mean and a posterior covariance. Okay. And I'm telling you what the answer is now, I will not compute it, but well, I'll give you a quick idea of it can be controlled can be computed that the posterior mean, is given by this covariance term that you've seen before? And sorry, I have a there's a minus one. Missing here, sorry, this should be time sigma to the no this correct, sir. Um, it's pre multiplied by the posterior covariance. Okay. And the posterior covariance is given by this term here. And but aside from the details, what I think will help you get set exactly the same terms come up all the time, right? We always have this cross covariance term that looks at how are the inputs correlated with the outputs? We always have this term is like, what's the covariance of my inputs? So these two terms, this matrix, and this, this vector here, they always come up, irrespective what approach to linear regression you take. So you can see that they all basically different sort of different interpretations of the same underlying things. And we always get this constant additive offset factor valorize. Okay. So how would one calculate this? The quick answer is in a linear Gaussian model, the map, the maximum a posterior is the same as the posterior mean. Okay. If we look at a Gaussian, the peak of that Gaussian curve, and we'll be pointing at the board, is the peak of the Gaussian is the mean of the Gaussian, right? So if you find the map, if you find the peak of the posterior, you've already found to me. So in linear Gaussian models, the nice thing is that finding the map and find the mean is exactly the same thing. So if so there's nothing nothing new to do. And secondly, finding the covariance, the procedure covariance, is also something that you've at least been told how it can be done. And you might remember in one of the previous lectures that the plus approximation was mentioned, okay. So that if you want to approximate a posterior uncertainty, you can do this by looking at the hessayon. Okay, so that's the, you look at the second derivative of the log posterior of the log likelihood, depending on what application is. So that's something you've learned about I'm not sure whether you carried it out. But there's something if you've heard about in a linear Gaussian model, that Laplace approximation is also exact. So the nice thing about Gaussian models is that these approximate techniques give you the right answer. So you could get the map. That's the mean, you could do Laplace approximation, and that actually gives you the true posterior covariance. You can also compute them by hand, you should get the same answer. We will be doing either today. There is an exercise on your excess exercise sheet where you asked to verify that this is the correct mean and variance covariance so you don't have to compute them yourself. But you have to show given that these equations that they're that they're correct. Okay. And there's some hints on the exercise sheets that hopefully make this manageable. So what what does the full Bayesian treatment in this case by us? Well, I tried to visualize this with the sort of two pictures here, right. So the first one, view this as a regression fit to so so there's some data that's an inputs. So there's a one dimensional case. So there's an input x on the x axis, the outputs y on the Y axis. And the linear regression, in some sense, you can say it's just the same as fitting, fitting a line to it, right, so we fit some line to it. And then we can use that line to predict new data points. And the prediction is very easy. We look up the input, if the input is 15, we can look up what the prediction for that for the output will be. And we can also predict how much scattered it will be. Because the scatter will be given by sigma, we have the sigma squared term that tells us how much scattered is how much noises. So we can see that for any putative data point that we put in, we can predict how much how much variance that will be around that. So when we predict the input, let's say will be 20, we can look up well, the mean, prediction will be 10. And there will be a little bit of scatter around this, okay. In the Bayesian view, when we have the full posterior distribution over parameters, then we will not will basically model that we don't have the exact slope. So we will, there are some variants of slopes, there's a distribution of omega, so there's a distribution over slopes. So our prediction doesn't say there's a single regression line. But there's really a bunch of different regression lines, each of which are plausible under our model. So when we make a prediction, let's say we want to predict the output to an input at 15, we would have to say, well, the posterior mean will be this. But there's some uncertainty about what the slope is. So we don't quite know what slope to take. And so there's two sources of uncertainty. One is this error band that comes from the residuals, so the observation noise, but there will be an additional uncertainty term or variance terms that comes from the uncertainty about which parameters to take. And as you go further away from your data, you're extrapolating more. So if you have an error on your slope, it will make a bigger error. So this is really, I think, desirable property of using the full Bayesian prediction. And that the further you go away from your data, the more uncertain you accumulate, because your parameter uncertainty propagates to larger prediction, uncertainty, okay. And I'm not saying this, that a Bayesian view is the only way to get that behavior. But I think that behavior is really important when you want to make predictions away from wherever you obsessed, if you look at here. Like if you look the difference between these two settings here, where you have a lot of data, the difference is very small. Okay? Because if you'd really dominated by your residuals, as you go away from a data, the effect of parameter uncertainty will accumulate and will lead to very different prediction that's and that's really important when you try to predict because predicting is typically going away from the data you already have. And then if you don't model the uncertainty that you have about your model, you can run into the problem of being overconfident okay. And in this case, you would get you would really get this by just sampling each prediction line from from this posterior distribution and then and then getting different prediction lines and getting the getting the associated uncertainty you can given that this is a linear Gaussian model you can also do this in closed form. So that you will have you can also calculate this predictive variance here. So you don't have to sample you can also calculate Yeah, yeah, exactly. So here, the uncertainty here is just corresponds to the the decide this residuals, the sigma squared term is the variance of the residuals and that gives you the uncertainty Okay, yeah. So I, before I answer your question, let me there was a Christian So I just saw a question in the chat asking me to explain the seconds of source of uncertainty again. I just so I don't there's no timestamp here. Was this after I tried to explain it? Or was it before? So it's the question already answered? Or was my explanation of here? Well, I will go on and not answer. Okay. So I'll try again. And when we make a prediction, there's two sources of uncertainty, the one source of uncertainty comes from the fact that we know that the model isn't perfect. So when we try to predict something, or when we try to predict something, let me let me go back maybe, maybe, where's a good example here, when we predict something, there will be this, there will be this uncertainty, or there will be the scatter of the values around it, right, we have the mean prediction that comes from our model, but there will be some scatter values around this, and the scatter will be captured, that will be the variance of the residuals will be captured by this observation noise or this residual noise term, sigma squared. So that is, that's the. So even if you have if you have infinite amount of data, and we fit this function as well as we can, that scatter will not put will always be there, right, it might be the intrinsic scatter in the system. So we cannot, even if you observe more data, and we get a perfect, perfect, or the best fit that we can ever get that scatter, there will be some amount of unexplainable variance, and that that will be captured by sigma squared. And that's and that prediction error, there will be a prediction, and we always get new data points, because that would just be the intrinsic noise of the system, right? Or some, some some variable that we cannot, that we cannot caption our model. And however, and I'm jumping back to this, there can be a second source of uncertainty. And the second source of uncertainty can come from the fact that that we don't have, we don't know what the perfect model fit is. If we fit our model to find the data, then not only do we have the uncertainty coming from the residuals or the data around the mean, but there will be a second source of uncertainty. And that's parameter uncertainty or uncertainty about the model. And that comes from this. And that comes from the variance of posterior variance over parameter values. Okay. So and because we don't know the parameter values exactly, we don't know what exact parameter values we have to take for prediction. And therefore any prediction will be also affected by this uncertainty about what model we should be using for prediction. There was a question, an excellent question. And I preempted that question by putting this little star here. And because the question was, well, but where did you get your alpha and beta from? Where do you get that? Where do you get the prior variants from? And where do you get that the variance of the residuals from and I said, everything is nice and easy and Gaussian and can do all these calculations in close forms. But I always assumed that I get this alpha and disputa from somewhere. But in reality, you will not in particular, this noise variance will not be given for you for you all to have to estimate from data, okay. And that's when things can get a little bit more complicated. And that's what the star here is thanks to If so, this the fact that everything can be calculated plus forms only if you fix or no alpha and beta, if you do not know them, but have to infer them, we have to use something more complicated such as using Markov Chain Monte Carlo sampling variation approximation, okay, I decided to not talk about them, because I think the more fit into a probabilistic machine learning course, another course, and they are standard packages for doing this. So if you have to do this in a particular situation, and there are ways to deal with it, but you lose, if you do inference over alpha and beta, then you lose this nice community property. Fixing out some video taking one particular sample of alpha and beta gives you everything nicely analytical Gaussian, but the distribution including alpha and beta will be non Gaussian. And you can think about an ASME B you can either do a point estimate or you can get a distribution over them. So, whether you do Bayesian estimate or maximum likelihood estimate, the same questions apply to alpha and beta, but you lose this attractive property of everything being nothing else. Assumption of caution I will hopefully talk about this in the next lecture, I'll have to see about whether I get I have enough time talk about this, but the so there's certainly a problem with Gaussian models and that they're often picked for convenience. Okay, so there are I mean, The Central Limit Theorem and others often gives us theoretical reasons for why Gaussian assumption might be warranted. And empirically often Gaussian assumptions are also often a bad assumptions. But there's also this attraction that when you take a Gaussian assumption, then everything works out nicely. So there is a danger to sometimes take a Gaussian assumption because one wants to have like an easy time fitting the model. Okay, so one has to be wary of the possibility that one takes a Gaussian assumption when it's really not warranted, just because it makes our life easier. And that certainly can happen that, that people pick Gaussian priors in cases when there shouldn't really have done that. Okay. So, and there are other priors sometimes that are more appropriate. One example, that that's sort of a class of examples is when we assume when we think it's a good assumption to assume that some sparsity in our signal. So when we, when we when we think that some of the values are, that there will be lots of zeros in the true parameters, for example, because lots of regression coefficients do not matter. And then a prior distribution that captures that, that tries to prune many parameters zero, can be a better can be a better model than a calcium. But there certainly is a danger of picking Gaussians when one maybe should. Alright, are there any further questions on this? They are different. Sorry, they're different. So when you sample different parameters from your posterior distribution, you get different slopes, for each slope, you get a different prediction line. So I write, when you when each each linear regression model is align in two dimensions. So when you sample from the posterior distribution, you sample different lines. So each of those different lines is a different is a different prediction model, the one in in with a non dashed line with a full line that is the posterior the posterior mean model. And the other ones are sampled from the posterior, or models whose slopes are sampled from the posterior. Yeah, for example, exactly. So you would build a I mean, you would go to Iraqi model, and then you would think about how you infer both these hyper parameters and the other parameters that because. So this gets in there's a danger, this gets into in very into very deep territory, and where you're where your priors come from, right. So there are so in the, in the, in the so in the in the very, I mean, let me let me not say fundamentalist, Bayesian viewpoint, but the very sort of clear, very fully Bayesian viewpoint, you would say, you have some prior distribution that just encapsulates your assumptions about the world. You have, you have some beliefs about how different parameters are distributed in the real world. And, and you might say, I think that height, I think, I think that this these are my beliefs for for how things are distributed. Sometimes you might have data to back up your priors. So sometimes, one parameter might be the height of people. And you can say, well, there's an empirical distribution of how tall people are. And it's Gaussian with a mean and a variance. So there's some external evidence that you can use to justify choice of your price. Okay. So there are situations where either that fire comes from your subjective beliefs, or there's some some outside knowledge that you can use to get the prior. But there's a third type, where price comes from, and that's really, arguably the most common usage of statistical practice. And that is, we use statistics as a way to explore data. So we have some ideas for modeling our data, right and a prior, and then we play around with the data and we find that maybe our our intuitions are not, do not quite make sense. And we just saw prior. So there's often this. There's often this interplay between setting priors and looking at the outcome of changing the prior on to our predictions. And that's on the one at the very dangerous land. Because if you think of an hypothesis test, you're not allowed to change things after the fact, right? If you change your beliefs as you do something, then that's from a, let's say, a hypothesis test point of view is, is, is just simply not allowed by theory, right. So you might massage the data into a form that that just is convenient. But in, in reality, it just, it just corresponds to the like the, the, the the realistic situation of data analysis, often being this interactive process between working with the data and updating our beliefs or priors. And there's many situations where it's really impossible, until you can explore it in an exploratory analysis, it's really impossible to separate the two. And in those cases, it's just really important to us. Or to keep in mind that these are tools to explore data. But they do not give us something like they're not they don't give us something like like an objective answer. Like if you put in something subjective, not something objective comes out. And maybe to sort of step back, it can be useful to, to separate exploratory analysis, where this is interactive process of us updating our beliefs will work with the data. And analysis is explicitly not extract, exploratory, but where we really want to set a hypothesis, test a hypothesis. And that second type of analysis, where we just do hypothesis testing, without any exploration, that often has very stringent rules and how to do this to exactly avoid this interplay, and the classical or maybe the canonical example of an analysis situation where we want to be very sure that we're not changing things on the way our clinical trials. Okay, so in clinical trials, and they have been covered in the course. And all aspects of that have been covered in the course before. In clinical trials, you people have forced very precisely write down everything before they touch the first data point. And that would be a situation where you have to write down your prior before you see the first data point. So for exploratory analysis, it's just natural that things move on the way we just have to keep in mind that we exploring or testing, when we do testing, we have to fix everything ahead of time. And then every outcome of that test will be contingent on on what we put in at the beginning. If you put in a nonsensical trial, then the hypothesis test will be nonsensical. Okay. So there's no there's no way out, right? If you make bad assumptions, everything that you learn of the data after that will be conditioned on your bad assumptions. Alright, I'm nearly done. I just some, I'm trying to recap what I think are the most important points I made. Like almost whenever you have any regression problem, almost always start with a linear regression model. Okay. When I talk to students in my group, and they show me anything that's not a linear regression, I always ask them, what's the outcome if you just try linear model. Even if your problem is nonlinear, then you might use in basis functions, such as polynomials often converted into linear problem with the extra work. If you think of regression, in this, what I call the machine learning view, obviously, that's in parenthesis, and with a little bit of grain of salt, the idea of linear regression is to minimize the mean squared error of the sum of the squared errors, and use regularization and cross validation to pick the right model. But there's no explicit hypothesis testing there. And there's no explicit Gaussian model. And in the statistical view, we often think of linear regression as doing inference in linear Gaussian models by estimation, linear LC models. And there's a big literature on how this can be used, for example, for hypothesis testing, for example, to test which, if any of my parameters are significant significantly away from zero. It is important to be careful when interpreting parameters, we have to be careful that we check the modeling assumptions. And we have to be careful that whenever you fit a model to data, that's just the model is really just a way to restate the correlations in the data. But in itself, does not imply causal evidence. And it's very tempting in regression models to interpret my parameters as being causal. Okay. When we spend more money on media advertising, then we sell more product that looks like the causal segment. And you might get a causal diagram from it. But it might just be correlation that comes from something else. So it's really important to not quickly jump to causal conclusions from correlation observations and regression models as cool as they are, in the end, the correlations and we saw this right when you saw where the parameters came from. It was always the covariance inverse covariance times the covariance this covariance vector. So my parameters are literally just a restatement of correlations in the data. So there's nothing added by the model. In that sense. There's no extra causality coming in. It's just a restatement of correlations in the form of a model. I spoke about Bayesian linear regression a little bit, which has this idea of modeling uncertainty over parameters. And, and that uncertainty can be important when we make predictions, because it tells us that there's two sources of error one coming from the error in the observations, the other one coming from our uncertainty about the model. And in the linear Gaussian model, we get close contributions. If we know if we assume that the prior and noise variances are known. Um, next week, I will talk about linear ish models. So generalized linear models, which which are a little bit more complicated linear models, but share many of the attractive features of linear models. And I will try to convince you that regression is really a special case of conditional density estimation. And when you view it in that way, you can see how you can very quickly go from linear models, generalized linear models, more complicated models. Thanks. Thank you. Are there any questions about anything? All right. There's there's one more question in the chat that I will promise I'll answer it next week. Okay. Thanks. 

