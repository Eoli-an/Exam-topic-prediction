Good morning in the call. Can everyone hear me? Yes. Good. Thanks a lot. I'll mute myself again for a moment while everyone else is coming in and then I'll unmute again when we start in about 10 minutes. All right. So I will get started. There are still people coming in, but that's fine. There's also a bunch of people in the call, I hope that you can hear me. If you can't hear me, then maybe unmute yourself or write something into the chat. Well, obviously you won't, if you even just someone typed into the chat that you can hear me. There is the chat. Good. Thanks enough. I'll have to move this chat away again, because well, I'll get to that in a moment. Let me get started with the with the lecture. So I hope everyone in the room is signed into Helios by scanning this QR code. And now I'm just wondering why my isn't working. Let me just give me just a second. Now it's working good. Let's hope that it keeps working. So what Ah, interesting. I will start every week with something like this. So this is the feedback form that you filled out in the last lecture. And you read this, should I switch off the lights in the front? Question, everyone says, Okay, I can read this. Now keep it like this. This was great. These are the answers for the first lecture, the first lecture is always evaluated as pretty good. That's not a surprise, because it's the first lecture and there is much content. This is for the second one, which is also still a very good outcome. I'm not going to show you the detailed feedback you gave for the other lecture, because that would mean me talking about electric given by someone else, but I can show you what you wrote as the comments for the first lecture, the non content lecture almost. So okay, so there are some things you like, a lot of people actually like this this COVID example. Although there was also at least one person who thought this is not a good topic to start the lecture with, because it's political. There'll be a lot of political topics, of course, this lecture, actually, it'll get way more political towards the end of this year. So we'll get to that. And someone said that you weren't able to scan the QR codes, because you said the left side, this is why I'm now using both of these projectors. They're showing the same stuff. So in case someone's wondering, like, I hope that this works out for everyone, if there's still someone who can scan the QR code, and let me know why add another feedback, which for which I'll have the QR code at the end of the lecture. Someone also said that the virtual questions in the chat weren't actually several people said that, and that's right. So that was a software, our user error that I made with Zoom. It turns out that if this chat window was open on my screen here, which I thought was a good thing, because then I was supposed to see the chat, Zoom always marks, everything is red. So I never saw a red, a red icon somewhere. And I thought there's no questions. So now I've closed the chat. And if you type something into the chat, hopefully I'll see it on here. And then I can open it, and we can see what the questions are. Someone also said that in the other feedback, actually, that we should repeat questions asked in the room for zoom, I'll try and do that. If I don't do that, then maybe remind me here in the room as well. Okay, and then someone said, one thing that was very unclear was how you're going to be graded on the assignments. And I recommend that you just read that one slide again for the first lecture that actually happens in detail. So you just mean, Jimmy, just hope I get it right. So you need to do 30% of the math questions, to be allowed to be going to the exam. You don't have to do the coding questions, but you get bonus points on the on the coding questions. So I hope that everyone does a few coding questions because They give extra points. You don't have to do the example. It's also really straightforward. And well, if you don't want to do the example, that's your own problem, right, because they are example questions for the exam. So good. So that's feedback. And I also sneakily use this feedback, usually for the first five minutes so that people can still come in, if they come in too late, or if they're stuck in the 3g queue, which thankfully, isn't that long this morning. So well, not last week, but two weeks ago, you had a lecture by a German yami and Luca stats, we spoke about how to design random polls. So how to design experiments in which you strike a sample from a population. It is exercise with the phonebooks. Hopefully, you've noticed that it's actually not so straightforward to withdraw uniformly from a population, if you don't have a magical algorithm that does it for you. But if you can, actually, I can take off this thing. If you maybe that's better for my voice as well. If you can draw on the population, then you found out that you can construct from samples that come from populations or from x eyes that are drawn from the population estimators or random numbers, or sampling sampling estimators of Monte Carlo estimators, which allow you to estimate well functions of those observables X. And that's this, this way of doing things was presented as good actually with a little bit of a caveat. Good because of two nice properties. The first one is that these estimators are unbiased. And I'm sure that Luke has pointed out this word bias is a bit badly chosen. It's an emotionally loaded word. But actually, it's in this study, it's a purely mathematical concept, that just means the expected value of this random number is equal to the actual value of this expected value from the expected value of the sorry, So expected value of this random number. This, like sum over random draws of which evaluate the function is equal to the expected value under the population of f. So the integral over f of x dp of x, that's nice, because there's this random thing on average, right? But it's never actually right. In practice, it's just like on average, like this old joke of the statisticians, right where to go to transitions go into the woods and like try and shoot a hawk and one shoots to the left and on to the right. And then they say great, because the right on average, there's also another good thing about these Monte Carlo estimators, which is that as n increases, they get closer and closer to the truth. And they converge at a rate, as I think Lucas showed you, that is given by this stochastic minimax optimal weights of what the error drops, like one over the square root of the number of sounds, which sounds good at first, although I mean, for your computer scientists, you probably know that a rate of square root something is not a good rate. But it's good for an unbiased estimator. In fact, it's the best rate you can hope for an unbiased estimator. So what we'd like to do, what I'd like to do today is pick up from this thought, and think about what we can do if the situation is a little bit more complicated. So and that's going to be two questions I'd like to answer in this lecture. The first one is, is it actually always the right thing to try and find an unbiased estimator? And then the other question is, What if I can't actually observe the thing that I care about X, right? So if I get X is handed to me, then I can evaluate any f of x, right? But what if I don't see the thing I need to estimate? So let's do these one after another. So maybe if you're trying to guess what the outcome of an election is going to be, it's actually going to be exercise this week, then it may be good to get draws it draws for the population. But not all inference questions. Not all questions that involve data are of this form. There are many other questions that people ask and use data to solve. Like, for example, scientific experiments, right? If you'd like to know what the mass of the Higgs boson is, on these days, actually, nobody cares about exports anymore, but you know, what the co2 concentration distributed over the over the planetary surfaces, then maybe drawing IID from the population isn't always the exact right thing to do. Another way of thinking about this is also mathematically I've already hinted that unbiased estimates might be good because they have these nice mathematical properties, but they're not necessarily the best thing to do because this rate have one over the square root of n is actually not great. Right, that means to drop the aerelon, my factor of two, you have to square the number of samples. So So you have to take four times as many samples. So sometimes there are settings in which you can, you might just decide not to proceed in a random fashion, but to actively choose experiments, such that the estimate you might end up with hopefully converges more quickly towards the truth. And this is particularly the case, if randomness is not such a prominent feature of your experiment. It might still be there, but it's more like a nuisance. As opposed to, you know, polling is actually all about random loss. But if you do an experiment, then maybe there's a measurement error somewhere. But actually, there's a big underlying quantity that you really want to design an experiment for, to to observe. So I really just want to point out that it's, of course, also allowed sometimes to really think about what you need to do to get to the bottom of the question you're trying to answer. And the formalism for these kinds of experiments, which are in the machine learning community, sometimes called active learning or experimental design, also, the natural sciences are the framework for this is information theory. So who's heard about information theory, the undergraduate bachelor courses, or anywhere else before who is? Okay, that's about a third. And we've seen this man before. On the right. So Shannon, another, like big father, grandfather of computer science, he wrote a wonderful paper in I think, 1940, something seven or so the b 51, in which he was one of these wonderful papers that introduced a concept solve all possible questions you might have about it, and then thereby end the field in one single paper, it's actually really short paper as well. He method information theory and also discussed basically anything you might be interested in. Because a and this this paper is sometimes the subject of theory of computer science courses, depending on which university you go to, is this notion of information content. So if assuming we have we have some probability distribution was the Python app there. Now it's you. As soon as there's some probability distribution from which we can draw a sample, so he finds this notion of information content, which is the binary logarithm of the inverse of the probability or minus the binary logarithm of this probability, which means that things that are extremely unlikely have a high information content. Another way of another word that we use for this is surprise things that happen rarely surprise you when you observe them. But he brings this notion of surprise into the context of information of communication, and says that now let's say we have a very specific distribution for which we get to see these events. And then we can talk about the individual events that define this object information content, we can also talk about the entire distribution, and say, expected value of this information content is the new quantity that you can actually call the entropy. Entropy is a word that was also used in physics, it's a little bit overloaded, over overly overloaded. But actually, these notions of entropy in information for you in physics are largely connected to each other. And there's also a version of this entropy called the conditional entropy, which is not the entropy of the conditional distribution, but it's this object. So it's the expected value of over like if you take an expectation of a to the variables of the binary logarithm of the ratio between the joint and the module. So this is this year is the conditional distribution, but the expectation we take is not over the conditional distribution, but over the joint. Thanks. Okay, so once we've defined these objects, Shannon has this wonderful theorem that is basically half of his paper that says, if you're communicating across a channel, many of you may have seen this before, then the length of a perfectly compressed file. So the number of questions that the receiver of this message has to ask about the quantity being communicated. Finally, questions in from a source that has distribution p is the information content of the message. And so if we communicate information, why are such questions about quantities x, then the number of binary questions to identify an outcome is at least the entropy of this distribution. This is random variable. So random variable distribution here are directly connected to each other because one identifies the other than we can talk about. So this is the thing that is talked about usually in theoretical computer science is called the Shannon source coding theorem. There's also a noisy channel version of it, what happens if the channel itself loses information. And this is a result that is used to derive statements about compression algorithms. So this will give us a lecture in the 90s. It might be about you know, Zippy algorithms that compress sizes of files. But actually, it's also a statement about how to design experiments in the real world to reveal information and get essentially rephrase this, like the following statement into a more weak statement that's up here. And then I'll try to formalize this a bit more. It says she wanted me minus the number of questions or experiments you need to do to identify unknown quantity and outcome from the source key or acquire an unknown identity, something unknown quantity is something you have a distribution over right, a prior some some unknown quantity, then you can ask them about the viability such that the question that you're asking maximizes the entropy of the outcomes. Why, because now we have to use this notion of conditional entropy, the conditional entropy is by definition, right, it's the expected value under the other over the joint of the conditioner. So in the previous slide, I had written it with in the sort of form of the definition of the conditioner. But of course, we can rewrite this up here using the product rule of probability as p of x given y times p of y, and then or x given theta times p of theta, and then use properties of the logarithm to rearrange. Now you can see that this integral separates into three different integrals that we can think of as the entropy of X, given theta. So let's see how how, what kind of distribution the outcome of our experiment would have, if we knew the value of the thing we don't know Sita is the current thing we care about an extra stuff we can observe? So if we knew this, how uncertain we'll be about the outcome? Plus, how uncertain are we about the unknown thing in the first place? So that's usually a constant. This is something like what do we know before we start the experiment, it has nothing to do with x. And then finally, minus the entropy of the outcomes. So in many experiments, this thing up here in the front is at least a constant, because it's just some measurement noise, something boring, or it might even be zero. So if you knew what the thing is you care about, maybe you could predict perfectly what your measurement is going to be. And if that's the case, so if this is a constant, that doesn't depend on the data, and this is just, well, something that doesn't depend on x, then the goal of the experiment is just going to be to maximize the entropy of the observation, because then that's going to maximally reduce the entropy about the unknown thing. So by asking questions about whose answer we're going to be maximally uncertain, we maximally learn about the thing we would like to know. And basically, all of active learning boils down to this task of trying to come up with policies that maximizes inflammation game. They're really elaborate schemes for this in machine learning. There's not a time here in this lecture to do to go through them. Instead, I'd like to do just one experiment with you. Also, with you online, there's no question that in the chat, to drive home this point, that sometimes to do experiments that actually help, you should come up with policies that maximize information content. That's all. So really, the goal of this first 45 minutes of this lecture, is just to get across that not every question has to be answered with a Monte Carlo estimate or sampling estimate derived by drawing IID from some population. It's just one way of thinking about how to collect information. Another one is to try and design active experiment that maximize information. And experiment I'd like to do is one that I really like because it's really boiled down to a precise question. And it comes from a textbook by the wonderful David MacKay, who unfortunately is not with us anymore. He died about five years ago. And in his textbook, he has this this question. Yeah, it is these. So here's an experiment, you're given a scale, a balance of English. And this thing, it's one of these, these things that you see on the on the market square and keeping in if you buy if you buy groceries there. So you can put things on either side of it. And it can like and then the two ends of the scale can tip in one direction or they can tip in the other direction or maybe they stay the same. And you're going to your You're given 12 balls is obviously an ideal, an idealized question. So someone puts maybe a few balls, think of them is like billiard balls are numbered, so we know which ones which. And we know that they're all the same. They all weigh the same, except for one goal. But that goal is either heavier or lighter than the others. So now your task is come up with a policy, so you're not allowed to touch the box. Right? I'm going to be up here mentioned, we have some actually here, right? And I have the same here. And now your question, your task is going to be designed experiments such that give me Tell me which balls to put on which end of the scale, and then we'll observe the outcome tips doesn't have one way or the other. And your goal is to design a policy such that we need as few uses of the scale as possible. So I'm going to give you five minutes to talk to each other. And the question I'm going to ask in the end is your policy first of all, how many questions do you need to identify with certainty which of these balls is the heavier or the lighter one and to identify whether it is heavier or lighter? So it's not enough to just figure out that this is the the oddball but also tell me whether it's heavier or lighter than the other books? And then afterwards, I'm gonna ask you what your policy is. Okay? Good. So talk to your neighbor for five minutes, come up with a policy that minimizes the number of questions you have to ask. And there's a question in the chat so I'll ask those. What do you mean with binary questions? I mean, questions that have a binary outcome so that an outcome that is either zero or one yes or no? And someone asked Are we supposed to see the camera? No, I can switch on the camera but you're probably not going to see anything because like now I look at the camera but then I'll walk around and you'll not see me anymore so I'll keep it switched off RIGHT? Everyone right All right? Morning Whoa, okay, so our five minutes I need you over ask everyone to come back into the conversation. Okay, good. Thanks a lot. Now, I'd like to ask if there was also a question in the chat that I'll get to in a moment have already answered it in the chat. And I've written the answer here on the blackboard for you. And question to the room and also to people in the on the Zoom call. So raise your hand or click your green checkmark icon, if you have a policy that can identify the identity of the ball and whether it's heavier or lighter, in okay, how we do that, let's say less than seven questions, less or equal than seven questions, I put it like that less than or equal than seven questions. A little bit further so I can see the hands. Okay, that's still quite a lot of people. It's also like five in the in the chat. Oh, and everyone else needs longer than seven more than seven questions. Okay, raise your hands if you think your policy needs more than seven questions. Okay, good. And missing out? If you really don't know, you just haven't, you just don't know what to do. Okay. It's also a few, there's still a few people who haven't raised their hands. So there must be a fourth option. I'm not considering at the moment. Look at tokens options not being considered next. So, okay, but there's still quite a few people who can do seven questions. Who needs less than or equal than six? less or equal than five? For? Now, we're down to one people, one person in the chat and 345 here at the first row, of course, less or equal than three? No one, so no one has a policy that can do it in three questions. No, okay. So Okay. Before we get to how we do this, let's think about what Claude Shannon tells us. So for that I need to faff around with slides again. I know what the problem is, Zoom is really bad. Really, it's to me stealing my focus. So So Shannon's information theory tells us not to mess around with complicated policies, but just try and maximize the entropy of the outcomes. So maybe the first thing that is a little bit difficult for you to think about is that there are three possible outcomes. The scale could tip this way, or it could tip this way. Or it could stay the same stay level. That's three possible outcomes. And actually, someone in the chat wrote, so Mr. Stuber wrote in the chat, actually, you can measure in other units as well, like, other than bits, you can measure in nuts or in bits, actually don't know what bits are probably base 10. And depending on the type of question, the base of the logarithm may change, that's true. And if you change the face of the loan, and what you see on the blackboard, when you have to divide by the natural logarithm base, the base of the right, so notice that because you're dividing by a number, any policy that minimizes the right hand side, the natural logarithm is also going to minimize the log base something else logarithm, so the strategy is not going to change. It's just the numbers we're computing are going to change. So if we have three possible outcomes, then every possible answer we get any an outcome that has that has three possible answers that all happen with the same probability. So the expected or actually the information gain of an outcome that happens with probability 1/3 Each is not phase two. Fleet is 1.59 bits. Well, another way of thinking about this is that read to collect actually, it's actually it's not. That's not a sentence again. So there are 24 possible outcomes, right? Because it could be either the first pole is heavy, or the second boat is heavy, or under the 12 pole might be heavy, or the first one slide, or the second is that 2024 possible outcomes, then the rope is 254 is 4.58 bits. So if we could ask binary questions, we will have to ask five questions under the optimal policy. But you can ask questions that have three possible outcomes. So we can divide this 4.58 bits by 1.59, because of this thing here. And that means we have to ask at least the ceiling of 2.8, which is three. So there might be a policy that does it in three questions, actually. So Shannon's statement is just at least, it could be that our problem has some structure that makes it impossible to do this in three questions and don't have to ask for Usually this happens because of some annoying divisibility things that we have been asked perfect questions that divides the outcome space equally, we'll see this in a moment. And but actually, in this case, it works. It's possible to answer this in three questions. So I can show you the answer. But maybe before I show you the answer, let's first talk about how you do that. So those of you who had a strategy that managed to use only four questions. What's your, what's your first question you'd like to ask? The first experiment? So now I know, which is Ah, so for those people in the chat, the idea is we take six balls each and put them on either side of the scale. And then you say, now, you know which one is the odd one. But what happens? So how do you know that right, so if the scale tips like this, so it could be that this side? Is, is the head is the heavy one, or this side contains the light one? Right? My second step is to check the capital. Again, the 123. Echo. That means on the one hand, the light golden. Yeah, so notice how your strategy is I'm not going to repeat it for the people in the question, because the important thing is that the very first question you're asking doesn't allow for the special outcome where it stays the same. So you're essentially asking a binary question when you do that. And that means if you keep using a strategy that only allows these binary outcomes, you will need five wings, right? If even if you do it perfectly to get to the outcome, but there is this third possible outcome where everything stays the same. So how can we design a strategy where it might say the same? Exactly. So for those in the call, again, actually, I can show you the answer now. And you can check if whether you had this this idea as well. Interestingly, it's I should not see the full thing is work works. Ah, hmm. Hang on. Protector. Ah, hmm. It doesn't even zoom. Well, okay, I'll put this slide on the on us. But we can actually talk about the white on the Blackboard as well. So and for those in the call, I think you can still see the important bits actually, I also need to get this zoom thing out of the way. So where do I put this, it's now stuck for the year. This might help a bit. I have a little bit Hybrid Hybrid lecturing. Okay, so the first thing we do is we identified if we have 24 states that's on the left hand side of the of the screen, right, I actually enumerated them 1-234-567-8912 light or the other one. And our view is actually following your strategy. So we put the first four on one side, he second four on the on the right hand side and then four balls we leave to the side. So now there are three possible outcomes for this Because this way or this way, or this way the same and they are equally likely. Why did we do that? Because I don't I discrete distribution with k possible outcomes in this case three when each outcome is a probability P as maximum entropy if all possible outcomes are equally likely, because then we're on average maximally surprised. So, if the scale takes, like this is this instability cannot see this. In fact, I'll try and close the fullscreen thing. And then maybe I can change the window. Doesn't seem to allow me to do that though. Okay. How do you record anything? So? Ah, good. Nice. Okay. Close it again. Okay, let's see if we can. Uh, huh. Ah, better. Okay, so now we just have to get the zoom thing out of the way. So now, here we go. Okay. So he, okay, so it's the scale tips this way, then we know that one of those eight Falls is the problem or is the odd one. So that's good. Because it's one in eight, right? That video reduces our outcome from 24 states to eight. And that's a that reduces the number of states by three, we can't expect anything better than that from the first question. So now their remaining hypotheses are eight of them, either 1234 is light, or 5678 is heavy. There might also be other outcomes. If it's this outcome, actually, it's completely symmetric to this. This is like renaming the balls from first from one to four to five to eight. So I'm not going to talk about this any further, or if the same, and then we'll get to this later. So then now we just know that the one of the remaining four balls are the odd ones, but we don't know when which one is whether they're heavier or lighter. So now if we are in, let's say, we stay in this arm of the of the experiment. Now the next thing, two ways to think about these eight possible states. Now annoyingly, eight doesn't divide by three. So this is one of these situations where it might be that we can't actually design the optimal experiment. By the way, I'll put this slide on ETS afterwards. It's not an alias yet, because I didn't want to give you the answer during the lecture. So we'll have to come up with a strategy that somehow splits this into outcomes with three possible answers and two possible answers. Right, it's just that's just the nature of discrete experiments. So one such experiment is to put 125 and six on the left hand side. And I do it this way, because this allows the outcome or the scale seeps down to be just as likely as it could possibly be. And seven, four and 10 and 11, on the other. So seven, and four are either a heavy or light ball. And then two of which we know that you're not just to counterweight. So now there are three possible outcomes again, and they have the maximum entropy possible outcomes. So either it tips this way, there are three possible outcomes that are possible six states that explained this outcome, or this way, also three, or it stays the same. And that's two, we can't get an outcome, like it's totally pretty likely, because there just aren't nine states. And then after this, we're essentially done. So now we have to wait TWICE, Like what's the third waiting. So if the outcome was like this, so that we know that either one or two, or like balls, or seven is a heavy one, we can now weigh one and two against each other directly, and just see which side of the scale tips down. And we know which one is the right one. Or if it stays the same? We know that seven is the hidden one, because it's the only remaining hypothesis. And the same works the other way round is symmetric again. And if it's if there's if the scale is stays the same, then we know it's either three or eight. So we could wave either one of them against one of the known to be normal balls. And we know if we are in this arm down here, where in the very first experiment, we observed that not that the scale stays the same, but the remaining hypothesis space still contains eight states. We don't know which ones have your life. So one possible way to expect to do an experiment again, we have to deal with this business that eight is not divisible by three to redesign one outcome that has three possible states assigned to it. With another one that has to be assigned to it, another one has to assign to it. So for example, nine and 10, against 11, and one, so one is known to be normal now, then we, after this is outcomes, we will again have a situation as above, so we have three remaining states or two. And then, you know, after that it's straightforward. So there is in this case, I a strategy that identifies the correct ball and its property being heavier light in three questions actually, in every single case. So maybe this is a point where I should maybe summarize first, and then give you the caveats. So I'm always tempted to go with caveats first, but we should first summarize, so I have a slide for this actually. So if your goal is, if you don't care about unbiasness of your estimator, if you're not trying to sample it, even numbers, then if your goal is just to identify a quantity of unknown quantity as quickly as possible, so if experiments cost money, which they usually do, and you want to be resource efficient, then you could consider this insight by Claude Shannon, that the conditional entropy of a posterior off the capacity, the conditional entropy over the unknown quantity, Sita, given that you find experiments, X separates into entropies of the likelihoods the prior minus the evidence, by Bayes theorem, and in experiments, which are quite frequent in in scientific settings, where this quantity here is a constant, or even zero. So in our case, it was zero, right? Because if I know which ball is that heavy or light one, and whether it's heavy or light, I can totally predict for the outcomes that were then, and this is just a concept, because it doesn't depend on x, then your goal should be to design experiments that maximize the entropy of the outcomes of the experiments. And if you do that, then the optimal policy to find the value of theta requires asking at least this entropy number of questions. So the motivation for this strategy, in stark contrast to the thing we did last week, his cost in a week since the strategy last week was worst case, robustness. So let's make sure that on average, we're always right. And we converge with the the best way we could possibly hope for given that we stay on average, right all the time. So MonteCarlo. Here, it's the opposite. It's the most aggressive possible strategy, let's collect as much information as possible from each experiment. The danger of this is that your strategy rests completely on the assumptions you're making about the problem. And if your assumption about this, that this relationship between theta and x is not right, so if your tentative model is not right, if the likelihood or the prior was not right, then actually the likelihood in this particular case it is not right, then your experiment might just lead to completely in the wrong direction. So for example, in our setting just now with these balls, imagine if it actually turned out that there are two balls that are and one of them is heavier, the other one is lighter. Now, of course, all our experiments are just going to take they're just going to lead to completely wrong answers. And so this is a price we pay for efficiency in information being we rely more on assumptions, we create estimators that are arguably biased. But in return, we get a much more efficient policy of figuring out the right answer. So of course, in reality, in actual applied setting, if you're working for a company or doing scientific experiments, you're going to try and balance these two goals with each other, you're going to do a few exploratory experiments that you use to ensure to test your your your model your likelihood, and see whether it's actually correct or not, whether you can rely on it. But then you're also going to try and let yourself be guided by information gain of experiment. These two examples you did last week, and this week are maybe extreme corner cases that kind of span the space of possible experiments you could go for. Okay, that was the first half of the lecture. Apparently no questions in the chat, I think also can see the chapter. So the first thing I wanted to do today is to counter as like, poll There's two ways of doing experiments, one with unbiased estimates at any point in time that just converge with the best possible way you can hope of unbiased estimates another one with a, like, aggressive goal of collecting information. And now for the second half of this lecture, I'd like to do like to turn to basically a third direction, which is what do I do? Yes. Okay, so we could, we could have just randomly drawn loadings off the scale, you could decide for a policy that just takes, let's say, four, or eight balls out of 12. And just randomly assign them to the, to the sides of the scale. So there, that's going to be groups of three, right, one group stays outside of the scale the articles put onto the scale. And maybe I'll leave it as a thought experiment for you, or maybe a numerical one, you can simulate yourself, how long it will take, on average, or in the worst point, the best is using the information you gained from these weightings to learn about the identity and whether it's heavier or lighter. And you can of course, use the same rules of reasoning when you can, each outcome of the wane exclude certain states in the in the in the state space. And you can think about how many exclude on average. The first one is going to be just as informative as the other one, right. But then after that, what happens? Okay, so somehow I still can't switch my slides with these not with the clicker. And now it's waiting, waiting African on those why. So the next question, I'd like to ponder as a question in the chat. What's a good real life data collection example for when you have to treat bias for more information gain? Cool, okay. Let me think about this. And actually, maybe towards the end of this lecture, there'll be an example that maybe goes in the right direction, let's see if the example we're going to talk about now actually leads us to the left. So lots of windows. So what do we do if the thing we actually care about isn't directly observable? So, I mean, actually, the experiment with the balls was a little bit like this, because we don't directly observe just observed outcomes of the wings. But it was so trivially connected to the, to the identity of the unknown quantity, that it really wasn't so interesting, like inference was kind of straightforward. And actually, maybe I could just ask this question. So what do you do if the quantity you would like to see is not the one you can measure? The simple answer based on the first question, first lecture. It could be like one of these, you know, in high school, the IDs, binomial coefficient PubMatic completion questions, that little nuts formula in German or the PQ formula, but you get you get told in school that you if someone wakes you up at night and asked this question, you need to know it by heart. This is one of these questions. So if you'd like to know a quantity, but you can only observe related quantities, not the quantity you care about directly, what what do you do? Yes, sorry for the the answer is actually so yeah. For the people in the chat, the answer here in the room was do try to find a find a functional relationship between these two quantities. The annoying thing is that this relationship might not be deterministic, might be somewhat connected with like additional sources of uncertainty. And the answer in the chat is actually the one I was looking for Bayes Theorem. You do Bayesian inference? And actually, the entire goal of the next 45 minutes is to convince you that that is the right answer. And it's actually surprising how surprising that answer still is to many people. And I think actually, the reason it's so surprising is that the last century also has been like, has been a bit of a like a, like a detour for human society. When thinking about this fundamental scientific question And the reason for it is that these poor people, until quite recently didn't have proper computers. So let me tell you what I mean by that. So what we're going to do in the next few lectures is a tour of classic statistics is actually got me to the sort of the sort of stuff that you would find in a sequence of chapters of basic intro book to statistics, it's going to be essentially three lectures. Today, we talk about estimation. Next week, we talk about confidence. And we talk about testing. The reason we talk about these three concepts is that they are fundamental to statistical estimation. They're basically the whole toolbox of statistics. I mean, it's just three words, I haven't filled them yet with math. And if you go to a stats course, which this one isn't, then you'll hear about these notions a lot, actually get to see a lot of math. And my main message, the reason we do it this way is that people you might encounter in your professional lives will expect you to know about this stuff, because you're working with data. And actually, it contains a lot of interesting insights. But quite often, I'll actually stop and say the right answer to your problem is always Bayesian inference. And these concepts here are just divine steps that you do. Because some aspect of your problem somehow requires them to do because you to do them. Sometimes, you're required to do these kind of steps, because of the the answer someone expects from you isn't fully provided by Bayesian inference. Quite often you do it, or people used to do them, because they thought they couldn't provide the Bayesian answer. And I'll show you what I mean by that. So many, like typical courses in statistics, at least, that's my personal experience, are, at least for people with whose mind works like mine, maybe it's similar for you are quite tedious and annoying, because they come along this long laundry list of things, maybe you'll find a stats course, in particular, those of you who've done the neuroscience, cognitive science, medical, kind of any kind of connection to the to the life sciences, you've probably heard about different test regimes and estimators, and z scores, and Fisher's exact test and p values, and all these things. And they just come at you one after another seemingly unstructured. A lot of statistics, it's just this long list of tests. Actually, if you go on Wikipedia, you can find lists of tests, and lists of estimators and lists of ways of estimating confidence that seem to be disconnected from each other. And they only apply in very specific settings. But actually, what they always boil down to is the following process. You are faced with some inference problems. So there's a quantity you'd like to know you don't know it yet. But you can you have some data or someone has collected data for you. By the way, we're not switching away from how to do experiments and collect data to what you do once you have the data. And you'd like to estimate this unknown quantity. So my way to do this is Bayesian inference. So what does patient infants entail? We have to write down a generative model a prior and the likelihood? A lot of statisticians now get hung up on this prior business. But what's the prior going to be might be wrong? And what is the prior isn't the right one. And actually, it turns out quite often, we don't have to worry about the bias. If the problem is sufficiently contained already, by its definition, often priors can be neglected, or they only apply in very low sample chips. The tricky business is the likelihood. And anyone who does statistics, even the people who don't do Bayesian inference has to use a likely they have to write it down. And then once you have your likelihood, you can do various things with it. With this likelihood you can use it to construct estimators to construct confidence sets to construct tests. And what these words mean. I'll tell you like over the next this lecture, the next. Annoyingly, the question that always kind of falls between the cracks is the one on the second line. What is the likelihood isn't right? What if I make a point, if I've made the wrong assumption, you will often get to see motivations for statistical tools that argue that they are better than Bayesian inference in some sense, because they are robust under some assumptions. They have they work without making any assumptions, but usually there's always a hidden assumption inside, which is that the likelihood is correct. Or that at least that the likelihood lies within some space of benign cases in which everything's going to work. So yeah, and so a good way for me at least to understand statistics is always been to think that first the goal is always Bayesian inference. And then once you know what your posterior is, all these estimators and confidence estimates and tests or whatever you might want to construct, they are just properties of the posterior, or the likelihood. And then their properties become very natural and easy to understand once you've seen the posterior. So what do I mean by this? Yeah, I would like to do another example. And it's another example. Actually, that comes from David McKay's textbook, but I've changed it slightly, I've adapted it. And it's really a concrete example of scientific inference or technical engineering inference problem you might have. So let's imagine you're on some old timey sail ship. There's no GPS, no fancy computers. And you're, let's say, you're somewhere in the German bytes in the in the North Sea. And your goal is you're, you're the person sent sitting in the top of the spaceship in the crow's nest, with a with a pair of binoculars. And you're supposed to find out why and why. That's a very important task for sailors, right? So here is what happened, what you can see, you look around, and you can see a bunch of light houses on the horizon, let's say three of them, or two, or five, whatever. So those of you who come from not this part of Central Europe, but somewhere higher up in the north probably know that lighthouses have uniquely identifiable sequences, so they, they don't just shine a light in regular intervals. But actually, the interval between the lights identifies individual icons. So you have a map, you know, which lighthouse is where, and you can see I see those three lighthouses at the moment. Right now your goal is going to be figure out where you are. So actually, well, I can I can show you I've got a piece of Python code I can show you. It's here. Annoyingly, the system small probably, and I now don't quite know at the moment how to make it larger. But you can see over here, maybe I should switch off the lights, a plot that's supposed to show the situation. So you're you're flipping your map, every box on this map is one of these lighthouses let's say you've seen three of those lighthouses. And each of the features that you have a bearing right, you know which angle from your crow's nest is the one pointing towards the lighthouse? Right now, on your map, you can draw a line from the lighthouse to that bearing that minus pipe to go turn it around the right way to your supposed position. And then of course, what's going to happen typically, is that those three lines not overlap perfect. You get this kind of triangle. Apparently, at least British sailors used to call this a cocked hat, like the hats that you wear at the back of your head. Now, where are you? Apparently the classic rule for sailors is that you're somewhere in the middle of the triangle, you kind of eyeballed by the middle of the triangles, and then nominally any hopefully intuitively clear that that's a answer. And sort of the bad one, but somehow it's unsatisfying, right? Because we know that what you're not exactly there, right? So where am I? Actually this idea of let's just put our finger in the middle of the of this triangle. That's what an estimator is. Right? It's a essentially a random number. It's random, because those three measurements are random. And now we could do figure this analysis. So we could talk about what's the distribution of this thing appointing going to be like, what are its properties? And if I, for example, a typical question I figured this might ask is, if I increase the number of lighthouses, for me to infinity, right, where n is the number of lighthouses is this strategy of always pointing into the at the weighted average of the measurements going to be good? It turns out, it's good actually, it's a very, very good estimator. But only a few lighthouses are not going to be infinitely many. And this is very difficult for data, scientific applications in the real world. It's not like you can always choose how many data points are going to add. So one, first thing you could do is, you can ask this poor sailor in this process, well, what's your measurement precision? And what how precisely Do you think you know which direction you're pointing? And I think that's something you could probably do, right? You kind of have, like 10 degrees plus minus something, maybe. And what why is it that way? Well, because, you know, like, the ship is moving, you're doing this with like, I was just trying to point down the line, right? So of course, there's going to be an error. So one thing we could do is we could come up with worst case bounds, kind of like this is that The code in which the right answer must be. So here in this picture, I've drawn dotted lines. Actually, I've assumed that they are arrows, actually. But you can draw these arrows up here, arrows are 10 degrees in each direction, plus minus, and then plotted them here in this picture. So now we can see that they're actually kind of three cones of lights of information that overlap with each other. But that doesn't really help, right? It's only made the problem harder. Now we just have more lines in our picture. So I'm clean way to answer this. Now I'll go back to my slides is to define a probabilistic inference problem. And for that, we'll have to make assumptions we'll have to write down electrical, you also have to write on a clock. So what's the prior going to be at? It's just flagged, right? Maybe you know, something were you supposed to be because you did the same measurement two hours ago, and you kind of know where you were on the map. And you could make this complicated. But for our purposes, I'm just going to assume there's no prior, there's kind of a box in our plots, like the the box has has bounds, left and right and up and down. Or we know that we're in the North Sea, where else should we be. So we could just put as you put a uniform prior over this whole space, that's not a good thing to do. Because if you know something a priori, you should be using it. But maybe it's a deductive thing to do, because I want you to understand that the prior is not the problem. But you can just set it to whatever we like. It's just a nuisance, because we have to define a measurable quantity. The tricky business is the likelihood. So we'll have to make an assumption about how our measurements relate to the actual quantity. In this case, the actual quantity is the bearing from the ship to the lighthouse. Now make several assumptions. My first assumption is that the measurements are independent of each other. So you measure in one way you turn around when you measure again, you measure it turn around be measured for the third lighthouse is actually really bad assumption, probably, because the measurement error has something to do with how the ship moves. And that's probably shared between all three lighthouses. But I'll make the assumption anyway. And if you want to change it, actually, it's pretty straightforward. If you know about what's coming next, and the code on ETS, I invite you to change that if you think there is a correlation, for example, between those measurement errors. The second assumption we can make is a much, much more painful, I'll assume that the measurement errors like ocean, so there's this little bell shaped curve that distributes the actual measurement around the correct bearing to the to the electrons. So why do I Gaussian assumption? Because I have to make some assumption. And torsions are kind of good, like your bell shaped they used to be on the money, they must be important somehow. And of course they are, you know, there are all sorts of fancy technical arguments for why Gaussian distributions are a good thing. So one technical answer that efficient might use is that among all the continuous probability distributions that have a mean, and the variance, a given mean a variance, nu sigma, or Y, or X and sigma x here, and sigma, they're the Gaussian distribution is the one that maximizes differential entropy. So it's the most uncertain distribution of all distributions that have mean and variances. That group. But everyone uses Gaussians. And actually, for what we're going to do next, it doesn't matter what kind of distribution you use, I invite you to take the code, you can find an alias and change it to something else, you could change it, for example, to a box, right, or in angled space, you could change it to a lab to a Laplace distribution with heavy tails, you could change it to whatever, right, do whatever you like, that's actually going to be the point of this exercise. And then I just need to write down how the thing I care about my position on the map is related to the measurement I make, which is the parent to the Legos. So because the position on the map is in Euclidean space, xy, and the angle is in polar coordinates, things will get messy. And there is this so now, my lateral is going to be right up like Okay, so what is the likelihood this is the distribution over the measure angles, theta, given my correct true distribution, x and y. And I assume that that the annex three measurements in this case because the picture I showed you, I've made three measurements, I assume that they're independent of each other. That's why there's a product there, and then that each measurement is a Gaussian distribution over the angle and measured and center on the correct end. angle. So the correct angle is given by this nasty quantity. And so that's actually the tricky business. So that's why I wrote the code for you, it could also have been an exercise, it's all about the stupid thought to try and angles into positions. And you will note that, you know, if you've got a level, so you know that this is kind of annoying, and this tangent and cosine, actually, there's a, for the computer scientists, there's a nasty problem with signs. That's actually why I always show you only the top left corner of this plot, because my code doesn't quite work and all quadrangles is a stupid plus and minuses. I invite you to fix that if you want to in the code. But it's just a technicality, right? It's just good software design, or in my case, bad software design. And that's it. The second time, I've just written down the Gaussian distribution again. So for those of you who don't know what this end is, of course, you will notice this bell shaped thing. And it's actually a function of probability density function, seven by a normalization constant, which is one over the square root of two pi times the standard deviation squared for each of these right here. And this is the fancy integral that comes with it goes solved for us. And that's why we can do inference. And then it's the exponential of a scale negative square distance between the thing we measured and the thing, be careful. Okay. And then I can plot that I can just plot this function, this likelihood, for every point on the map. And this is what it looks like. And that's it. That's the posterior after normalization. Why is it the posterior? Well, because the posterior is the prior times the likelihood, the prior is the flat function, so doesn't matter. So it's multiplying everything by a constant. And now the posterior actually has to be normalized by the evidence. So the evidence is the mass of the entire region. One way to fix that is to just take this picture, some of the values that this heat map has on every pixel and divide by. So that's all possible. And okay, someone asked in the in the chat, could you repeat what what this arc tan angle represents? It so the, there's a not there's a fancy function in, thankfully, you know, this is actually okay, I'm going to use this to make my larger point. So if you open up a statistics textbook, and try and learn about, you know, tools of statistics, one thing you have to keep in mind is that these books were written 1020 3040 50 years ago, and the concepts they present were invented 3040 50, maybe 100 years ago. And the people who came up with these concepts, didn't have computers, or maybe they had computers, but the computer is filled entire rooms and equipped, like evaluate, you know, one function in a half an hour. And that's why they never made these plots. Because these days, we've got computers, and it's straightforward to do this. Actually, this is an animated interactive plot, it just took me a line to make this animated indicator. Where's my mouse? Yeah. So we can change one of these angles, move it around, right, we get a different distribution, you can, you can change the arrow of one measurement, right? If the first angle is measured with big arrow, then of course, like we don't, the first angle is the one that comes from below this one, you make it precise, it's going to, like shrink everything down. And this all, you know, you can see that this all works in real time, right? You can create kind of nasty situations where it's kind of weird. Like, I don't know, what's up big arrows. It's a badly shaped and unlikelihood, whatever, play around with this, if you like on the Jupyter Notebook. And we can do all this on a laptop in real time. Because it's not complicated. It's just the image. But the people who came up with estimators and, you know, optimal convergence rates of Monte Carlo numbers, they were at the very best, they were building in the 1950s, trying to build nuclear bombs. And they had to build a machine and move around on measurement paper, and dice to throw to produce random numbers. That's actually how they design their nuclear bombs. And he says straightforward to do this, of course, it's not always straightforward. So if you talk about the procedure distribution assigned by some deep neural net to a bigger dimensional space, it's still hard to do this to this day, I need to think a little bit about how we do that's what machine learning research contemporary times is about. But it's not hard to solve for the full posterior anymore. And it's also easy to find an estimate in this posterior so this red dots that you may have noticed moving around. And this happens to be the mode of this distribution. It's just a table lookup and just looking for the largest value in this domain. Okay. So that's Bayesian inference. And it's not hard. This is actually the most important thing. But now, what did we do to turn this quantity into the red.so? Sometimes you're in a setting where Are you working for some company? You have a boss, the boss tells you Yeah, I finally made a nice, nice plot. But I want an answer to my question. Where are we on the map? Right? Maybe the captain of your ship as it's fine. But where, right? And then one thing you might do is just find the mode of distribution director. Maybe they're right. might be wrong, right. But it's not so bad, because it's like the most likely point on the map. And of course, the next question that immediately comes up is the one we won't answer today, which is, how precise is this measurement? Can you give me an error bar for it? We'll talk about how to get those error bars next week. But actually, you can already see how you get them. That's the error bars. Right? And now we can turn these error bars into lines, because you only want to convey two numbers like an x and y. And you can imagine how to do that, right? It's not. So all of statistics is about this two are taking this quantity. And somehow making some numbers out of it that you can communicate to your boss to your captain, to your I don't know, usually advisor or whatever. So how do you do that? So typically, you take the Pusteria? Well, actually, you take the likelihood, because the people you might work with don't like priors, because they've been told that priors are somehow nasty to you only use the likelihood, and then do something with it to reduce it to a point. That's for estimation, you construct an estimator. One way to do that is what it actually is, and what is an estimator. So here's a bunch of stuff that you might find in a textbook, actually, I'm not going to do that much, I'm just gonna rush through it. estimator is a function that produces a random number by taking measurements, which are random numbers, and applying some function to it. Actually, this is similar to the answer you provided before you find a functional relationship between the thing we measured. And the thing you would like to estimate. Annoyingly repeated measures is a random number. So the thing your estimate is also a random number. And that's it, the distribution that is produced about like over this random number by those measurements is called the sampling distribution. And it has a so called bias, which you've already encountered in the last lecture, is the difference between the expected value of the random number and the actual value you'd like to estimate. In practice, you often don't know what the spices are, typically don't know what the spice or the spices. But if you make assumptions about Partitas, then you can try and identify what the biases, but those are, of course, resting on assumptions. For example, we'll do that maybe in a few minutes, you could think about what the bias might be if the measurements are actually Gaussian distribution. Okay, and an estimator is called consistent if it converges in probability. So that means basically, if you make more and more measurements, eventually you will converge to the truth. That's pretty much the basic property you want from any estimator, right? If it never converges to the truth, that may be a bad thing. Okay, so the most common estimator, the one that typically shows up in the first half of any textbook is the maximum likelihood estimator, it's the obvious things, what we just did, you just take your likelihood function defined, it's more convenient way to do this is to often take the log likelihood. So instead of maximizing this quantity, we maximize the logarithm of this quantity. Why is this good? A because we'll take a logarithm then, products 22, sums and sums are easier to deal with analytically speaking terms of derivatives and find the most. And another thing is that this logarithm often makes this distribution nicer because these distributions tend to be tiny numbers between zero and one numerically speaking, it's nicer to take logs, and they use the entire floating point range. And why is this an AI? But why are we allowed to do this? Well, because there's a little line down here that says, the maximum likelihood estimate is invariant. Another way of thinking about this is the logarithm is a monotonic function. So it's an invertible function, which is noted by the exponential function. And the location of the mode does not shift under the application of a monotonic function. The value does, but we don't care about the value just about the location. So if you take this entire likelihood and take the log of it, then the location where it's maximized, does not change. Okay. And so now we enter the statistics textbook. Good thing, it's only 15 minutes left. So I can do some simple statistics with you and gives a little bit of math. Of course, you should care about that math, because it's what statistics might ask about it in the exam. But the most important part of the lecture has already come across hopefully, that those estimators are post talking the crucial object to think about the posterior or the likelihood. And now what we're doing is we're calling wanting to find this object, finding its mode. Next week, we're going to find its shape. And we'll talk about how to integrate bits of it, how to think think about hypotheses. So one key thing is like about the maximum likelihood estimate is that it's consistent. It has this property that also the previous slide, that if you increase the number of measurements asymptotically, it will converge to the correct number. And here's a classic way to prove this, which you don't have to follow fully, you don't like but maybe sometimes you would like to see proof. And that proof to make be able to make this proof have to use this nasty concept that I might as well show you now, because it'll show up many times, if you keep doing machine learning lectures about the Cooper divergence, who knows about the KL divergence, we've heard this term before the half of the room. So just like they are distance measures in Euclidean space, we can measure distances between two points, there are notions of this similarity between functions, in particular between probability measures, and this is one of them. Notice I've said notion of this similarity not metric, because this is actually not a metric. It's weaker than a metric. It's a function that measures how dissimilar two distributions are p and q. And it happens to be defined like this right over here, Q or PDFs or density functions. Actually, we're not going to use more properties of this thing. The only thing that's interesting about it is that it's not symmetric, that's just like a warning, basically. And it's always larger, it's always non non negative. So it's larger than or equal to zero. And the only point where it becomes zero is when P and Q are the same, everywhere. But the almost is a technical thing about measure theory that you don't have to worry about. So what is almost mean, like you're talking about a continuous function on the real line, The annoying thing about the real numbers is that you can hide infinitely many things in there, and they still take an infinitely small fraction of the entire space. So you could have a function that is different, for example, on every rational number, but it's the same on every real non rational number, and then it will show up in here and FF, the same property. Okay, that's annoying mathematical details. So how do we use this, this quantity to show that the maximum likelihood estimator is consistent, this is a classic result to define, for example, a very Bosmans book. And if this were a stats lecture, we would have spent 45 minutes a day about this proof rather than five minutes at the end. So here's the theorem, assume that the data is actually generated from the likelihood notice how that sentence always shows up at the beginning of such theorems. And if the actual nasty one is, let's assume everything is right, then I show this nice property about this thing that I care about, talk about, okay, assuming that everything is correct, we further assume that the model has this nice property that you can actually identify the correct location. So that means like one way to the Disable godmother is that this KL divergence is zero if and only if fika embsay, are the same, then the maximum likelihood estimate is consistent, converges to the true value. And why do we see that? Well, so the maximum likelihood by definition estimate is the Maximizer of the log likelihood as well. So it's the arg max of the sum of the individual log likelihoods. And so we can take this quantity, and we can add a constant. So here, you see this is this is the this quantity is divided by a constant, so that doesn't shift the location of the mode. And then we've added constant, which is minus the law of key of like a true like, your measurement, even the true value, that doesn't shift the location of the maximum because it's just a constant. And now we notice that this thing looks almost like the KL divergence. It's a Monte Carlo sampling estimate. But this is the same kind of relationship between estimator and expected value that you had in the last lecture. Right? This sum is an estimator of this integral. And as n goes to infinity, you know from last lecture, this quantity converges to this quantity. And this quantity is always larger equal than zero, and it's only zero if the two are the same. So the point where this is maximized is the point where theta is equal to zero. Okay, so that's a nice theoretical property. If we get an infinite number of samples, then our maximum level estimate is going to be correct. So in our setting, if you had an increasing number of lighthouses on the horizon, and you kept multiplying in those terms, no matter how they are distributed, actually, both lighthouses as long as they are not all on the same repeated locations. So that's what incentives identify. Eventually, this, this clip Out of knowledge, what where you are, is going to converge to a point. And then so that's a nice property to have. But you only have three lighthouses. So like maximum likelihood estimators, even though they have this good property has a few caveats that in particular apply if you only have finite many samples, and they actually also a special cases where the assumptions of the theorem don't work that makes things. So this week's theory exercise on the exercise sheet that is not only this is about one such situation where the maximum likelihood estimate isn't a good thing to use. Because it's continues to be wrong all the time. And it's actually infinity. And it's a model historically relevant example that many of you may have heard about before. So I recommend that you do this theory exercise this week, it's actually involved. And it's very insightful. It's actually the answer is very easy. Once you understand what the answer is, but it takes a bit of brain cycles to think about. Okay, so at this point, I would basically be done. But I know that doing like me waving my hand about about mathematical concepts like this isn't great for everyone. So before I get this kind of feedback, I know that sometimes it's really useful to just do an example of what is meant by maximum likelihood estimation. And I'll deliberately take a very simple example. And then, on the exercise sheet, you could do a slightly more complicated example. It's the kind of example where I normally would use a blackboard. But because we are in hybrid mode, I'm not going to use the Blackboard. But instead, I'm going to try and slow down as if I were using the Blackboard. While I show you the relation, let's see if that works. I could also just actually vibe with a relation on the blackboard, but it's probably pointless to do so. Here's the slide, a standard example. For a maximum likelihood estimate is the one that you've all seen before. Let's assume you have a Gaussian random number. This is basically the example that we had with our lighthouses just without the whole polar versus Euclidean, nasty business. So let's assume there is an unknown quantity, and quantities for a new pair about that number. And we measure it with a bunch of measurements X, Y, x's are a sequence of them, they're drawn IID from this distribution, and 1000 division as a mean, that we care about, that actually also has an error, which we might not know. So that's, of course, a typical situation like you are up there in your process leadership, you also go about the hours. So one interesting question might be the overtime like to figure out how precise your measurements actually are. And it's possible to estimate that error at the same time, as it is estimated. And if you really want to, like work your brain a little bit, I might take the code that I put on, on on VR. So there's no, by the way, it's not nice code at all. I voted like fighting it before, like before the weekend started, it's really not good. You can look at it. And think about how you would change that code to also estimate the error of the measurement. And hint, is not going to work if you assume that every single measurement has a different error, right, because then you have six samples to estimate, and you only have three measurements. So that's not going to be identified. But if you make the assumption that there is only one position we care about, because an x and a y coordinate, and one fix our that is the same for everyone. So not a number of error, like a standard deviation of zero, then you can actually do this computation. And you'll get a different kind of let's say you want to estimate the full bottle use, which is more the textbook state situation, or definitely, then how do we find the maximum that you estimate? So here we go. If I tell him what the statement says. So there's a Gaussian distribution. And the likelihood is given by the product and IID over these individual terms for every single day to exci for Ifeoma. So the log likelihood is going to be a logarithm of that product. So that that's the quantity that is easier to optimize. And we see already that it's easier to optimize the log likelihood is a product rather than a sum over the logarithms of Gaussian distributions. So from a few slides ago, we know that Gaussian distributions are present here are given by this one over the square root of two pi times sigma squared times the exponential of a square distance. So if you take a logarithm of that exponential goes away, and this year becomes One half or the square root times logarithm of two pi sigma squared, I already have that on slide. So here is the squared is this, we're assuming everything is scalar. So that's nice. Measure the distance between the measurement and the actual thing we care about squared divided by two times the standard deviation, minus a constant log of two times minus one half times the logarithm of the variance. That's analytic description of the situation. And if you want to do a textbook derivation, it has to look as structured as this. But you could do the same kind of event in your head, if you want to follow along, think about this kind of computation. In the picture that I showed you in the Jupyter. Notebook. It's the same kind of object, it's just that there's going to be an annoying Arcos tangled summary. Okay, so why is the mode of that so here on a piece of paper on a whiteboard, we can take the gradient of this function with respect to these two parameters, and see if we can set it to zero. And if that's actually possible, it turns out, so if you take the derivative of this object with respect to mew, then you notice that there is no mew in here, no mew in here. So those two terms don't matter. We're just left with the derivative of this object. So what you do, as a square here is a chain rule, the derivative of the square is the two comes down, cancels with the one half, and then either derivative is minus one. So the minus fences, right? And follow along, then, hopefully, you're taking the math or machine learning class. Okay, so this sum over x my x i minus nu over sigma square, we can all take into two sums, right? So there's one term of the exci and one term of the new. And because what we're going to do is to solve for zero, we can get rid of the sigma already. Because if this is this expression is equal to zero, we can multiply by sigma square to the sides. So this expression is the one we have to solve for you. And that's straightforward. So notice that if you have n terms, where there's a new in there, they're all the same. So it's just just n times new, it can be arranged and defined that our maximum electric estimate from U is one over n times the sum of the exact, this is often called the sample mean. And of course, you've all seen it before. It's paid for. What is our estimate for the variance? Well, so we take this expression, here, we take the derivative with respect to sigma square, I'll take the derivative with respect to respect to sigma square rather than two sigma, which makes things easier. Analysis assume that I've already set new to export the sample. Because if that's like the case, where the function is maximized, the new and I'm only looking along this manifold of maximize. So here, again, there's no sigma in here, but there is a sigma term over here, and on the left hand side, so this one is the nasty bits. This is a one over sigma square, so the derivative of that is minus one over sigma square squared to the fourth power, this thing inside of the brackets stays the same, we can take the sigma to have to sum because it's in every single term. This one doesn't depend on sigma, and here we get the one over sigma square. But this term is also still inside the sum. So there's just in copies of this. So well, now we rearranged set this equal to zero, so we rearrange that we find that our maximum likelihood estimate for the variance is one over n times the, this is called the empirical variance, sometimes it's the sum of the squared distances between the actual measurements and their sample. And these two quantities, by the way, so these things are often called sufficient statistics. Because if you have those two numbers, x bar and a square, then they are what you need to estimate those two quantities. Why are they called sufficient statistics? Because notice, if you wanted to write a computer algorithm that estimates these quantities, then you wouldn't need to keep around the whole data set x, you could get rid of it and go through the whole data set once and as you do that, build up your estimate for x bar. And then simultaneously, actually, well, okay, it's not for yourself, but it was possible to do with simultaneously also compute this quantity. And that's the whole operation. This has been your cost. If you don't believe that that's possible to do in, in the same room, same password data, you can also make a few passes, but it's actually possible for one of us. Okay, so that's how we get our maximum likelihood estimate. It's this thing. Now, I know you for our picture. We can't do it this simple way. Because this x i here contains these are two sentences, but actually, we can just replace this x i with The art can have the measurements. And that's it. That's always there. And actually, that gives you exactly the point for that. It's not the way I computed the point in the code. Because again, it's much easier to just call this numpy array and ask it to tell you whether movies. Okay, so maybe you've seen this before in a textbook or in your high school classes on statistics. Some of you may have had a, you know, maybe if you've had 13 classes in Germany, then some at some point, your math teacher had time to tell you about some aspect of this, that is not good. Anyone remember something bad about those estimators? Yes, so this is just to tie everything together so that you know what the connection is to the last lecture, it turns out that this estimate sigma heads right here or elsewhere, is a biased estimate of the variance. So actually, okay, I just showed you that. So is the derivation all about I'm not reading in detail, you can look at this later. So what do we need to do to show that it's biased, we're going to actually compute the expected value of this random number called x bar, and the random number called S square under the Gaussian distribution. And you can do that actually, I'm not going to do the derivation for you, you can look at this slide later, if you want to, without the save some time. And you'll find that this x bar the sample mean, is actually an unbiased estimator. So that's good. But if you do the same derivation for s square, then you just have to do actually a little bit of pedestrian, like number crunching, but the trick is always to rest basically two tricks. The first one is to use the expected values and some integrals and sums commute, at least the simple case where everything is nice and well behaved. And the other thing to use is that the expected value of the square of x is not the square of the expected values. But instead, it's given by actually, on the previous slides. For Gaussian distributions, I just tell you that the expected value of X is new. And the expected value of x squared is sigma squared plus mu squared, it's the second moment of the distribution. So if you plug this in, and you just get carefully make sure that you don't lose any sums in this process, you'll find that the expected value of s squared so this random number we use to set our maximum likelihood estimate is actually given by the true variance times one minus one over n, or n minus one over n. So that means the actual variance is larger than the one we estimate, it's always larger than ODST. And intuition for this, that you may have heard about before, is that this estimate for the mean is x bar. It's always a little bit closer to the data you've seen, then, actually, why because you construct it actually as the minimizer of distances of square distances to all the data points. So it's the point that is as close as possible to the center of mass of all the solutions. But because the samples are random numbers, they're mean is actually not going to be exactly that number, which happens to minimize the distance all the points. Now, if you use that to estimate this x i minus u squared, which is the actual value of sigma, then you get a number that's a little bit too small, every single time. And it's too small by this factor, which conveniently we actually know it's n minus one over n. So what we could do is, we could just multiply our estimates a square by N over N minus one, and then get an unbiased estimator. And you will know this because at some point in your high school history, you had to use one of these maybe still, I don't know, if you don't get calculators anymore in high school, okay, do you remember that they had this. So, this is I think, Texas is always has one this is the these yellow numbers those are the statistics functions, you may have noticed. And they so here you see x bar right. So to be able to estimate, you can compute the sum of squares and the sum of x or the sufficient statistics. Actually, there are three sufficient statistics. There's also n the number of samples to know about that, and then you can choose to use either is maximum likelihood estimate of the variance or the unbiased estimate of the randomness. And you can think for yourself that of course, this new estimate is unbiased one which is slightly larger is also consistent, because as n goes to infinity, this number goes to one. And so if this is an this square is a consistent estimate by theory, then this is also a consistent estimate by theory, basically. Okay, so that's inside on at the end was meant to connect your thoughts about the law Lecture to this lecture on bias, this is a nice property, you might want to have an estimator. Consistency is another one you might want. And it turns out that just by doing maximum likelihood, you don't always get an unbiased estimate. But it's still a good estimate because it's consistent. Nevertheless, it's just an estimate. Because you're really just computing a number, when instead, the real goal of your computation should be to compute the posterior distribution, which is often basically just the likelihood, weighted by whatever the prior is. And in many cases, we can compute this posterior actually computed on evaluated on grids and make images of it anywhere. So a sentence I'm going to say over and over again over this lecture course, is we can do this, because we have computers. We don't have to use everything that's in the statistics textbooks, because we have completed. This is actually a really recent phenomenon, that I think many people especially in that community have not realized yet. And so it's good to take the machine learning class. Okay, this is the end of today's lecture, I invite you to please click on or open up your phones and provide some feedback by scanning this QR code. You can also go on EBS, and find the feedback form for lecture three there where you can type in stuff if you prefer to use a laptop, but please do it. You may have noticed that the number of feedback we got from the first lecture to the second has already dropped very significantly. I always have to remind you that the only way for you to tell me if something goes wrong is via this process. Don't wait until the official evaluation of the electric force comes up sometime in December. Because by then it's way too late for me to change anything about the course, if you'd like me to change anything, please use this functionality. Find a note for your calendars. I already told you in the first week that we have a running research seminar that is virtual online, everyone's invited to attend. Last week, there was a cool lecture by Fred Kinsler from UBC in Vancouver. Some of you were there, just a few of you this week as some of you know, the one by Tony Patagonian from Helsinki University. not technical, but if you'd like to get into how theory talks for machine learning work or how methods, research and machine learning works, it's a good to expose yourself to some advanced cutting edge research. He's a postdoc in at Aalto University and a theorist of current machines. So if you'd like to hear about current machine learning algorithms. All right. Thank you very much. Have a good week and I'll say goodbye to everyone in the call as well. Thanks 

