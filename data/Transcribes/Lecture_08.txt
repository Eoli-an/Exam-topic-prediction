Okay, so quick check for the people online. Could you briefly say something in the chat? If you can hear me? Yes. Okay, great. Thank you very much, then we can get started. So let me see, like, of course, the final thing that I need to check. So apologies to everyone in the call, I'm a little bit late because I had to do the 3g tests outside. Let me see if we've got right focus. Alright. So I hope you've all scanned in for this room. If you're here in the room, actually, I will not start this lecture with feedback. Because you know, the last two weeks was, the lectures were taken over by Professor Macker. So I won't feel like sharing feedback for him. Also, I think information like he still has to learn how to show QR codes. So we only got feedback from about three people. And for one lecture only one. So there isn't much to show anyway. Instead, I want to show you where we are now in the course of this lecture course. So we started the first few lectures talking about how to get data, how to collect it, what can go wrong, if you collect data? How do you make sure that it's informative about the thing you actually want to see, and that you don't accidentally bias yourself? Then I did three lectures on what you do, once you have collected the data and you want to estimate one particular quantity, you build, you build an estimator, you estimate its confidence region, so how uncertain you are about the correct value of this unknown quantity. And maybe if you have a hypothesis that you want to provide some support for when you can take a null hypothesis and have it rejected by the data, or the last two lectures, Professor MCIT told you about, like, we moved on to doing analysis on quantities that are more interesting than a single real number. And he spoke about regression, or maybe supervised machine learning as a more general kind of topic on and of course, it is in a very, like basic, very basic form with least squares regression, logistic regression with Bayesian interpretations, because that's the point of this course. Right? The more advanced stuff you'll get to hear about next term, if you take the statistical machine learning and probabilistic machine learning lectures. So today, I would like to talk about maybe something you do, as that we could have done earlier as well. But it's going to be a little bit too advanced, maybe so that we could have done it three weeks ago, on maybe a preliminary first step you might do when you get data. So no matter what you do in the end, with your career as someone working with data with you working for a company, in research in science for institutions, sooner or later, I guess, I mean, that prototypical situation will be that someone gives you a dataset and then they asked you to find out something about it. Right. And if you're taking too many machine learning classes, you may think that every single data set comes in a clean form pre prepared for you, you know, everything about it. It's a benchmark, I don't know, contains handwritten digits or something. But typically, when you load data sets for the first time, well, actually you don't see anything, right, you just load it, and then is there and then maybe the first thing you might do is make an image, right of what the data actually looks like, look at it at the table. If it's a table with, with, you know, descriptive rows, like a proper pandas table, you can just look at the table. But quite often, you'll just get a CSV file, right? It just just contains numbers. And then you might decide to look at your data set. And it might look like this. So this is a typical data set, I'm not going to tell you what this is. But the prototypical form of any data set, of course, just like any database, is a table. It consists of rows, whereas the pointer there, and so typically, the number of rows is called in. And every row contains features that are typically called p and a p. And sometimes people talk about small n, big P or small, a big N, small p, depending on whether this matrix is rectangular, skinny, or rectangular fat. But so that's maybe more of a side observation. But the bigger problem, obviously, is, how are you supposed to make sense of this? So the typical question, whoever you work with, might ask you is can you help me with this, you know, like, find out something about this data? So what do you do? If you encounter this? Let's collect some ideas. Also in the chat, please. Yes. Aha, so the answer here in the in the lecture hall was will look for correlations in the features. And you can actually see them right. As, as annoying as this dataset looks, there's obviously a lot going on here. As you can see these vertical y columns, right? I mean, they are columns of this matrix as well. But there's kind of clear structure, like some sort of periodicity maybe, right? Maybe this is, so at some point, I had, I had a Once cool advice, the master thesis at a car maker and edited. So it looked like this was a welding robot. And you could see kind of the periodicity of the sensors. still collecting. There was also an idea there. Was that similar? Okay, so how do you like what does that mean, look for correlations? What's the tool you'll be using? competitions like rank based once. We have won like candidates. So what would those things tell you what your computer? Can I just tell you that things are correlated with each other? So what would you use those for? What's the end goal of this initial analysis? Ah, very good. Yeah, you may have seen the title of the lecture, right. So for those people in the call, the idea was we were trying to do use correlations to somehow reduce the dimensionality of the data set. So there are as you correctly say, there's a lot of different methods that kind of rotate around this idea of computing correlations across the data set, or reducing redundant information. You've already collected a few. If you specifically talk about dimensionality reduction, what's the first method that comes to mind for you? You can also type in the chat if you'd like. To save PCA. Wonderful. So while he says I'm for it was also in the in the chat PCA, principal component analysis. So here is a little game that I like to play in this lecture. Every single year. I say, so actually, okay, I'll tell you about it first. No. So what I'll do is I'll revive PCA, because every single year, I say, what do we do? And people say VCA. And I'll get back knock and ask, Who of you have heard of have heard of PCA? Before? Almost, oh, actually, not everyone. So who dares to say that? They haven't heard of PCA principal component analysis before? Okay, very good. So not only for these three people, but actually, I'm also pretty sure that everyone else if I asked you to do it here now life is gonna be a little bit. Okay. So, but I'll do it very quickly, because there's two more things I want to do today. So, the idea behind PCA is that is actually one way to motivate dimensionality reduction is what could call generative dimensionality reduction in the SEC, the following sense. And then we briefly talked about this generative bit. So the idea of generative dimensionality reduction, which is actually a more like general concept than just PCA is that we're trying to look for an encoding. So a way to map from the high dimensional space to a low dimensional space, and a corresponding decoding that you could think of as some kind of generalized inverse of the encoding, such that the, we get an encoded representation, so low dimensional thing. And that's supposed to be a good approximation to the whole data set in the sense that if you map it back, so if you reconstruct from this embedding, or this encoding, then some measure of quality between the original data set and pushing that data set through this encoder decoder chain is good. So let's this high dimensional data set looks like this. We want to turn this into a matrix of size n by I don't know two, typically two, because then you can look at it and then map it back through a map that takes a two dimensional thing and maps it back to a p dimensional thing. And hopefully, the reconstructed data set looks a lot like the original data set. Why would you do that? So let me first point that out, because that's actually the most important takeaway. So PCA is the version of this, I can already tell you where both of these maps the encoder and the decoder are linear, and the loss is quadratic, and then we get PCA. So why would you do that? Well, there are many different reasons why you might want to reduce dimensionality. And they are all a little bit iffy, as we'll actually find out today. So that's going to be the main takeaway of the whole lecture, I can already tell you at the end, but realize that this dimensionality reduction business, it's a little bit complicated. And yes, I recommend you try it if you get a data set that looks so annoying, like this one, but don't expect it to do magic for you. Okay, why would you want to do it, maybe you want to save memory, maybe this is just too big. I mean, of course, this is not too big that I can like show that on screen. But you may have a data set where p is I don't know a million or 10 million or something and you want to reduce it down. Another reason you might want to do this is that you somehow want to look at the data you want to see structure, there's something going on, they want to see in there. Because it might be like no biological data, really complicated stuff is very difficult to interpret. You want to project it down to two dimensional space, and then you can see what's going on? Or yeah, you want to somehow find structure, which is maybe actually for visualization, right? Okay, so notice that what we're not using here so far, is any form of y, any output any label, I we're assuming that we're doing this in an unsupervised fashion, that's just data that someone gave to you. And they didn't tell you anything further about the data. So here is the derivation for PCA. And because almost everyone raised their hands, I will do it on one slide. This is one of three math slides that will come over the course of this lecture. Here's number one. PCA is the following idea, we're going to do what I said on the previous slide with with linear maps, and a quadratic loss. So what does the linear map mean? That means we take our data set that's here, after stupidity, called an n by d, you will also call it n by p. And we will project it onto a basis of a vector space, we'll call that basis you it's an orthonormal basis or it's right, not all the individual basis vectors are orthogonal to each other and have no one. This is, you know, your first semester undergraduate linear algebra class. So if you want to represent a vector in terms of some other basis, then you project onto that basis, you could think of this vector as being expanded in some basis with some coefficients that you can compute by projecting the individual vector onto each of the basis. That's another way of writing it like, oh, there's a transpose missing here. But basically, that's what we're trying to do. And now we don't want to represent I mean, if the basis is is a complete actual basis of the vector space, then of course, doing this doesn't really help us at all. It's just another way of writing dataset. But what we are going to do is we're going to say, I'm only going to use n of those D basis vectors, where m is a small number, think two, and four in those dimensions will actually represent the data set in this way. You can actually more generally assume we'll just put some coefficients here, and then who knows what those coefficients are. And of course, you can already guess what they are. They're over here, right? But we could we could see that in a moment. And then in the remaining dimensions, we're just gonna do a shift of the entire dataset. So notice that there is no n under this be it there is just we take the whole dataset, and we shifted in that space, but there is no individual coordinate anymore for each data point. So that's our encoding. And that the encoding actually went so fast here that you may not have noticed this line here. That's the encoding, you just project on basis vector product. And this is essentially a decoding, right. So you can now write the entire if you look at the right hand side, and think of it as a representation of x, and that's a decoding where the A's are computed in the encoding fashion. Now, we just have to decide how to choose a and b. And by the way, also, you I haven't told you yet what the use are just that they are an orthonormal basis. And for that, we need to define a loss function. So on my previous slide, I said, we have to set somehow find an encoding and decoding such that the loss is small, but we have to say what the losses, so the loss we're going to decide is a quadratic loss without representation, error and reconstruction error. And so now we just plug stuff in, and the rest is linear algebra or analysis or calculus, I don't know. So this is the last function we're going to use, we were concerned with the sum of the squared differences between the two individual datum, and V cons encoded and then decoded, they don't, we plug in from above for that is take a derivative with respect to a and b, actually, you get to do that, on your example, homework this week, you like very straightforward, you can imagine, if just look at this expression, you notice the two coming down as an inner derivative with respect to a and b, notice that this is a linear function. So things will be easy, you just rearrange. And you'll find that the optimal choice for a given that you have you, of course, is exactly the encoding that you would have thought of a projection. And the optimal choice for B is slightly more interesting. It's a projection of the data set mean, onto the remaining dimensions. And if we do this this way, then we can plug in our choice for A and B and get a no actually not yet, we can just look at this expression here and realize that we can rewrite it as this form or more compactly, we can write this loss, this quadratic loss as a sum over the dimensions of the data set that we have not encoded. So the sum goes from m plus one to D. That's the bit that we are not correctly setting, right. So you can imagine that in the sum, if you set a to this encoding, then these, you can write x n in this form, right as a big sum from iPhone one to D, and kind of do your homework for you now, right. And if you do the sum, you can split up that sum in one term that goes from one to M and one from N plus one to D. And then you'll notice that the first n terms just cancel out. And we're left with those remainders, this bit that we don't ENCODE. And now we can just stare at this expression and realize that this s here in the middle, that's basically a sample covariance. It's the this like the normalized sum over square distances between the data set mean, and the individual data. So if you want to minimize this expression, we can now finally think about what the correct choice of the basis is that we want to project into, you can so you could do this properly. And formally, if you like, by induction, you start out saying if I just choose one, you to get to if m is one, what would the optimal choice B to minimize this expression, you can maybe convince yourself that the right way to do that is to find the eigenvectors of this s and find the eigenvector that corresponds to the largest Eigen value and use that as your first encoding. And if you keep doing that, you can do an induction proof and find for yourself, but it's also kind of intuitive, that the optimal choice for you for the basis to reduce the this sample variance as much as possible is to choose the used to be the eigenvectors of as ordered by the size of the eigenvalues. Okay, so that's PCA. That's a complicated piece of math. In a moment, I'll get back to it and show you what that looks like in code. Because conveniently, the, if the Python implementation of PCA is shorter than the math, we need to write it down. So one key nice observation of this is that you could also think of this, like so there's two different ways of thinking about what we're doing here one is that we minimize the sorry we maximize the covariance of the data set in the encoding or we minimize the quadratic reconstruction error of the decoding basically. And if we managed to make x bar zero, so if this is just X X transpose then then we can think of this operation analogously. And this is hopefully something you may have done either in the master machine learning lecture or in your undergraduate calculus lectures. As a Finding this the singular value decomposition of the dataset, the central data set, and finding the right singular vectors and just projecting onto the first few of those. Okay, so we'll get to that in a moment. Now let's go to our dataset. So let me go back, here's the data set that I showed you before. Let's do PCA on that and see if you can find some cool structure in it. So I did that. And it looks like this. So these are the first two principal components of this data set. It's that good. So I said at the beginning, you might want to do this process for various different reasons. Right? One might be to find structure and do a visual representation. I am sure, I hope you'll agree with me that this is not a particularly interesting visual presentation. It just Yeah, it looks actually it's like a Gaussian blob. I mean, notice that the X and Y axes are badly scaled. So I mean, really, if you make this a quadratic plot, then it's just a circle. But it's this original data sets, right? Actually, if you look at the original thing, you can kind of see there's so much structure, right? It should be somewhere there, but PCA cannot find it. Another reason I said you might want to use PCA is or dimensionality reduction is to save memories to encode the data somehow. And actually PCA does this really well for this data set. So here, I'm showing you the average reconstruction error. So average means there is a sum. And the reconstruction error is this quadratic loss that we spoke about before. And I'm showing you this as a function of how many features we use to encode the data. So if we allow ourselves to use about 100 columns of that matrix, to represent the dataset, then we have a reconstruction error of you know, if you eyeball it, something like one like 10%, right? So that's pretty good. So 90% of the data set can be represented in the first 100 of these principal components, which is really nice, given the data set has 1764 dimensions. But to find structure, it's maybe not so great to do this. So it does anyone, by the way, have a better idea of how to find structure in this data set. So if you look at hierarchical clustering algorithm, a hierarchical clustering algorithm for people in the code, that could be an interesting, Vic, try that out few like, one thing Oh, yes. So you can a Fourier transformation, someone says, Oh, yes, I said, that's an interesting idea. And actually, a Fourier transformation of this data set will look, I think, interesting. So try that out for yourself. Any more ideas? So one thing you may notice if you stare at this for a while, is that there is a regular pattern that repeats, and you could check what the periodicity of that pattern actually is. And then you'll notice that it's a divisor of this 1760, whatever eight dimensions that this data set has teased me, someone says, Great, we'll do teach me in 10 minutes. And so if you find that there's this periodicity, that is also a divisor of the number of columns that this data set has, does that give you a hint? It means that you can take any of these rows of this matrix, and we arrange them into a rectangle. And you could make an image out of that rectangle. And you'll see that the data set looks like this. This is a famous one of these benchmarks that you may have seen before. It's called labelled faces in the wild. It's a collection of human faces. And so I deliberately done it this way to only show you now what the data actually is, because this is very common. I mean, it's it's, I think it's a, it's a useful way of realizing how much structure you could even have in a data set, and that you just normally don't have the right feature detector for it. Now, it just so happens that we all of us here in this room, we have sitting at the back of our skulls, the perfect pattern recognizer for this particular data set, which is faces, right. So our eyes are very, very good at detecting this structure. But of course, there are many other datasets out there that have probably just as much structure it's just that our brains have not been To see them this way. So apart from audio, and images, maybe symbols that are images, our brain is pretty bad at picking out patterns. And as you could see, PCA is maybe not the right way to find those. So if you do PCA on this dataset, and actually, you'll get to do that in your exercise number three, this week as the opening little finger exercise, then this data set looks like this. So what you see here at the top are the principal components. So these are the first 11 principal components of this data set, ordered from largest eigenvalue to smaller Eigen values. You can of course, if you do this, for your exercise, don't look at all of them, there's 1704, and so on, right 68 of these principal components, and the first few 100 of them all look like faces, they all look like aspects of faces, the very first one kind of, you know, encode your, you know, prototypical round thing with eyes, basically. And by the way, let me point out that the color, of course, doesn't mean anything, because the direction of the eigenvector doesn't matter, you could invert it, and then it's bright outside the dark in the middle. And then there are lighting principal components, right from the left from above, from below, maybe there's one from the right somewhere as well, the data set tends to maybe not be lit from the right so much, then there are, you know, facial features, orientations of faces, maybe there's something very forceful of beginning to see special kinds of emotions or types of faces people do when they take pictures of being taken off them, and so on. And in the rows below, you see two particular images. So these are the ones here on the right, there are two two images that are picked randomly from the data set. And they are both reconstructed in these spaces. So if you project those two faces onto the first Eigen vase, then they're called Eigen faces, of course, right, then you get these two representations. So this image is more dark than bright, which is why you get a dark face first, and this image is more bright than dark. So you get a bright face first. And now we keep expanding. And you can see that sort of from maybe from here on outwards, you can kind of see that there are different people, maybe after I don't know, for that thing for yourself, when you can start realizing who this person might be, if you've seen them before, it's relatively quickly that you get a good representation of individual faces, even with just 10 principal components. So even though this representation look really useless, it is not useless, it actually is pretty good at compressing. So things like for example, such regular datasets, it's just really not good to look at as a two dimensional plot. Okay, now, someone said Disney in the in the chat, and I want to get to that. But to do that, well, I first want to introduce another dataset. So this was able faces in the wild, spoiler alert, you'll do it in the act, you'll use that data set in the in your homework, and I'll actually return to it next week. But for now, I want to take a data set that is maybe a little bit more realistic, of the kind of tasks you may actually face in, in professional life. Why? Because images and audio recordings, or maybe the two kinds of data set or audio or text, right? Are the two kinds of data types, that we now have very, very good algorithms for in the form of deep learning. But for more generic data sets that just come without so much structure, and the sort of approaches are still much more lacking, because they are not. So there's no there's not as not as much kind of foundation models for them. So here is another actually quite famous data set. It was published in 1992. And then quickly became a part of the University of California, Irvine, benchmark data set for machine learning a UCI benchmark data set. It's called the Wisconsin breast cancer data set it originally, I'll tell you, and this is actually something okay, that this is a story that you may have. You may hear many, many times Moreover, but you can't hear it often enough. What what actually should happen ideally, in a professional setting, if you get to work with data, there's the following someone arrives with a data set someone you work with your colleague, right? Come on, you know, with a thumb drive, or they send you a link on Slack, here's my data set, can you please go and do something with it, then your first reaction should be okay. Please tell me more about this data set. Why do you want to do that? Well, because if you just saw otherwise, you have no idea what to do. Right? If you just open up a CSV file and stare at it, it is like silly to assume that you as the data expert should be able to extract information from it. The one person who knows how to get what that information should be is the person who gave the data to you, as also a formal reason to do that, as you know to do inference. You need a prior And the likelihood, you need to define those, most of them contain information about how the data was generated. And so you need to get that prior from somewhere, you can't have it in your head because you don't know anything about it. So you should ask someone, and then what they might tell you is the following thing. So there's this paper that was published in 1992. It was written by three people, St. Walburg. mangasaryan, what they did is they took biopsies of patients who arrived to had to get diagnosis for breast cancer, they didn't all have breast cancer. And so now you have a small tissue sample, right, that arrives on the microscope, they took a grayscale image of that sample, just one such sample, this is from the original paper. And then some poor schmuck, probably the first author had to sit down with an electronic device and outline every single cell that they could see in that image. And he did that. So you can see that there is in this image, there is 1-234-567-8910 1112 1314 cells. And then they computed a bunch of features of those cells. So they didn't store the fact that there were 14 cells in that image, some images had 10 cells, some had 23, some had two, but they computed features of those outlines. And I'll tell you more about those features in a moment. And what we actually get here is this dataset where they collected those features. So this is what the data set looks like. Yeah, as you can see, it looks kind of similar to the stuff I just showed you before. But now we know it doesn't contain any faces. This dataset contains, I think just over 600, or just under 600 patients, and 30 features numbered from zero to 29. Now, what I'm showing you here is actually I can also go over and show this to you as a, you know, NumPy notebook, I actually loaded this, you can find it on EBS as well. This is the data set, if I've called it oh eight WT BC, on the on EBS so that you can find it associated with this lecture. And you can read it with this one line in pandas and then just make a an image. So this is the image I just showed you. So what you see here is on the right as a big matrix, the log base 10 logarithm of the values of those features. So they actually have a very high range, we have ranging from you know, 10 to the minus three to 10 to the three about six orders of magnitude up and down. And they're all non negative, clearly. And on the left, this data set actually has a label. So we know that some of these were classified as cancerous as maligned and the other ones not as benign. And so I'm showing those labels on the side as well, for each individual patient. Okay, so now, let's do PCA again, just so that we're back on track, there's a question in the chat. What are the features here? Could you maybe give an example of a feature that just like, that's part of the story? Give me three more minutes, we'll get to that. So imagine someone just told you the story that I've just told you so far, right? This might be a typical conversations you have on the phone or on Zoom or on slack with your coworker, right is the data set? You know, go ahead, do something. So your first knee jerk reaction is let's do PCA. So actually showed you the math before here is pca in Python. That's it? Fine. So what I do is I take the data set x, I subtract the mean to center it. That's a fancy line. Complicated, okay? Once we've done that, I compute the SVD, the singular value decomposition of the data set. So after this, we now know that x can be written as u times s times v, permission, V transpose, right? Where s is a diagonal rectangular matrix. So that's a matrix that is, like, taller than it is wide. And it has on its diagonal numbers that are all non negative, and everything else is zero. And on the U is a large orthonormal matrix and V is also a large orthonormal matrix. And actually, the algorithm returns V transpose, which is why it's called the V h, because it's be Hermitian. That's it. That's PCA Well, actually, to make a plot, you have to project onto the first two principal components. And I do that separately for the two classes, because of course, we might want to see whether the algorithm actually find some structure. So I do that once for the malign the way we have a black piece up here, samples, project onto all of them, and then do this also for the benign ones. And so why are the labels they're binary, so I can just use those as logical labels and make a plot. And, actually, so the trick is, I'm just plotting the first dimension against the second dimension, right? That's the low dimensional embedding for you. And here you go. So on the left, you see the actual principal components. So they are all in one matrix now. And on the right, you see the data set projected on to do with those first two principal components. And the black dots are I should have put a label here, maybe I have this label in my slides. There we go. The black dots are the benign ones. And the red dots are the malign ones, the ones that are cancerous. And you can see that there's a lot more variation in the cancerous data, which is kind of expected maybe like you would expect that, you know, tumor cells are kind of more chaotic. And oh, by the way, did you notice that this plot looks different from this plot? Can someone guess why did I make a do have a bug in my code in one of these two slides? Yes, the axes are flipped. So do this goes from plus 1000 to minus 4000. Here it goes from minus 1000 to plus 4000. Set a bug 20 to fix it. Yes, exactly. So for those in the in the call as well. And notice that I'm computing single vectors, so I just need an orthonormal basis. If you take a vector and take minus that vector, it's still orthonormal to all the other vectors. So you can't really fix a sign for the choice for the choice of faces. And sometimes, depending on which of the algorithms you call, whether you call sai pies, Dinaric SVD, or one from some other package, you get the signs in one way or the other. But it's the same plot, but it's just flipped over. Okay. So this is PCA, interesting structure, right? That looks like there's something interesting going on. So now you go back and you call back your colleague is I find some structure. Right? Now we can do classification, you can do this in two dimensions. Actually, you can just take take the first two, first two columns of this may have this matrix in particular that you could just do it by I just look at it's pretty straightforward, right? You can see kind of see, right? So actually, by the way, what are those features that you that you gave me? Like, what is this dataset, these 30 dimensions? What are they? It's like, okay, here's the here's the story. So there's actually a file that comes with this, if you download from the UCI data set? Yeah, quick question. Don't Don't ruin my punch line, though. Ah, these are actually it's better to understand that if they show you the updated version. So it's this, these are the entries, I can show you what the actual plot is, of the V of the principal components. And so each column is one principal component, one vector, one basis vector to project onto. Okay, and now, so now, finally, Akali comes around, because it is actually I found on the hard drive, there's another file, if you go to the UCI data set collection, we actually find those explanatory files. And nobody ever reads them, right? Because you just want to write a research paper, whatever these data sets are not important, right? They're just benchmark data set. So what actually are they? So they are, the actual data set has 32 dimensions. The first one is just a number that counts which patient this is. The second one is the diagnosis, the label that I've already showed you, it's maligned or benign. And then they actually just measure 10 things. It isn't measured 30 things, it just measured 10 things, the radius of the cell, the texture of the cell, the perimeter, the area, the smoothness, the compactness, the concavity, the number of concave points, the symmetry, and the fractal dimension, the coastline approximation, whatever that is. So why are there 30 features? Where are they stored, the mean value of each of these features, the standard deviation of those features across the image, because remember that these images contain several cells, right? For each patient, you have maybe 14 cells. So you have an average value for those 14 cells, a standard deviation for those 14 sales. And then you have a worst case value the furthest deviation from the mean, actually didn't they didn't take the one that is furthest deviate, because it's probably too crazy. They took the mean of the three largest deviations from the mean. And then store those, and you can actually see this, right, you can see that the first 10 And then the next 10. And then the final 10 Kind of looks similar. There's always these four greenish stripes and then six darker stripes. Huh? This tell us anything about PCA. So this is now your job again, right? Your colleague has told you anything about the data set and you will get PCA, is this a problem? Is this wrong? Like you go back home, you try to sleep on it, you collect those fines, fines, PCA is fine. PC is always fine. Actually, we think about it. So what are those, actually? But even are those things? So there's a radius. Hmm? So the radius is maybe measured in centimeters wide, or maybe some variant of centimeters. I don't know how they measure it. Maybe it's micrometers or something. The texture? Well, that's grayscale value. So it's a number between zero and 255. The perimeter, probably also a length, right? The area Oh, that's an area. So it's centimeter squared. Okay. Smoothness, I don't even know Right? Local variation in radius length might be a variance that is a square of length might be a standard deviation, no one, no one knows. Actually, I tried to read the original paper, they don't say it's totally up. There is a I mean, the paper actually goes into length to describe what they did. But if we wait on this bit, the compactness, whoo, okay, it's something that has to live between one and infinity. And then all the other stuff I really don't know. So this is called a tabular data set. That's the word that sometimes is used in the machine learning community for this. So data set for every single column contains a different quantity measured in different units of measure. So even the first 10 dimensions of this data set have different units of measure, they have lengths, like binary numbers from zero to 255. Areas, weirdly bounded scale objects from one to infinity, and stuff you don't really know. And then they are repeated three times, once you see a mean, once you see a standard deviation, and once you see a maximum value. Well, okay, so that might mean that we have to somehow get them on a different scale. Right? We have to put them on the right. Annoyingly, there isn't really a right scale, to compare these things to each other. Because I mean, how do you compare a grayscale value to a length? You can't be. But if you decide to rescale, if you decide to, let's say rescale? Like the second entry, by dividing it by 255, so that, you know, it's a number between zero and one that maybe seems a bit better than what you're effectively doing is you're taking the data set x, and you're multiplying it from the right with a diagonal matrix so that every column of the matrix gets rescaled. Right? So if I do that, what's that going to do to PCA? So remember that we're doing a singular value decomposition of a matrix. What is the singular value decomposition of a product of two matrices? Everyone's like, I don't know. Yeah, you don't know. Of course, you don't know, because there is no closed form expressions that tell you what the new single regular composition is. It's just anything. So here's the original PCA I showed you. And if I rescale, the data set, a PCA looks like this. So I just rescale, this first time, second dimension by 255. And the fifth and the eighth ones, just just as an example by tenant 200. You can try that yourself as well, if you like. And it looks different, right? So previously, you might have stared at this and go oh, yeah, does this interesting, you notice this cut off a purely straight line rule. And then here this like this, this long tail? Maybe we need a name for dough. So like, if you're lucky enough to get biomedical experts, you might go like this, maybe we have different types of cells, maybe I can name them after myself. And you know, and no, it's just noise. And it's particularly annoying, because this is PCA. And it's a linear transformation, right? So you can't be more linear than linear if all I've done here is done other linear transformation. So okay, so there's a problem with PCA, which is an please keep that in mind. We'll get back to it in a moment, which is that if you're if you linearly rescale individual dimensions of a data set, then the principal components change. And maybe this isn't the problem for data sets like this labeled faces in the wild one that I showed you, because every single column of that matrix is a pixel grayscale value. So there isn't really a reason to scale some pixels by some numbers and others by others, because they're all on the same scale. But for a tabular data set like this, this is a genuine problem. And it's a pretty fundamental one, for which, I would argue we don't have a clean solution and not even sure there is a clean solution at all. Okay, so here's a quick summary. Principal component analysis is a very straightforward algorithm that everyone here has seen before almost everyone here has seen before or heard about, it corresponds to centering the data and then doing a single Have any decomposition on it or alternatively, computing the covariance matrix of the data and then finding the Eigen vectors of that covariance matrix and choosing the first few for representation. PCA though is a meaningless algorithm, if the data is tabular, if the individual columns of the the data set have different dimensions, but it does its job it, it compresses data into a low dimensional representation such that this compression maximally reduces the variance of linearly compressed and decompressed data. By the way, just as a side note, if you want to sort of put that in a mental shelf for for lectures that come up in the future, I showed you that you can think of PCA as this linear encoding and decoding. So you take your data, you map it through a linear encoder that gets you a low dimensional representation, and then map it back out through a linear decoder, that gives you a weak reconstruction. And then, because everything is linear, we can do this computation in closed form, in essentially one line of Python, if you like. And there is a generalization of this that many of you may have seen before, where you just assume that both of these maps, the encoder and the decoder are non linear maps, then this is called an auto encoder. So, this nonlinear map might, for example, be a deep neural network are any nonlinear function you can think of that you can fit and optimize somehow. And what then people also sometimes do is that they change the loss function. So this is a way of changing the encoding and decoding to make it nonlinear, then it's called an auto encoder. And then people also fiddle with the loss functions or not just make it square reconstruction error. But sometimes people also compute log losses, something like a KL divergence between the the original data set and the encoded and decoded reconstructed data set. And then this thing is called the variational auto encoder, you may have heard about it before, because these are is one class of algorithms that can make these generative models of human faces. And you know, they're not GaNS, but they're not of the alternative to GaNS, just so that you have a connection to them in your head. Okay, so that was the summary. Basically, here is already what I said before, largely, right, so generative dimensionality reduction amounts to projecting data into low dimensional latent representation, such that the reconstruction error is small. And that's why it's called generative dimensionality reduction, because you can use it to generate new images, you can now go into the low dimensional space, the embedding space, pick a point in that embedding space, and project that out through the decoder. And you will get something that is supposed to look like the original data. If you do this with is labeled faces in the wild dataset, I encourage you to try that you will actually get something that looks vaguely like a human face. Nowhere near like your modern, you know, style Gann, whatever cool generated images, because it's just a tiny little data set. And it's just PCA. But it's actually not bad. So you can generate faces with that. And in fact, some of the like most advanced algorithms for modeling of human bodies, for example, that my callback here in keeping this working on also based on PCA, just a really souped up version of it, not the simplest thing that I did in like two lines of Python. But there is another way a type of dimensionality reduction that someone has already pointed out in the in the in the chat that is not of a generative nature, and yes. wouldn't be wrong to train a classifier for the person just to do what second cluster scan by five. So on scan version of the data, which we did the PCA so that the wrong the training was it's not wrong. So the question for those in the call is would it be wrong to train a classifier on this? No, it's not wrong. You can train a classifier on this, you could even train maybe a linear classifier on it. We'll get back to classifiers in a few minutes. I mean, so classifiers are never really wrong, right? They can just be better or worse. Maybe that's not the best way to choose a classifier. But actually, let me get to how to choose the best classifier in the moment. So for those who are bored already, you can think about how you would choose the best linear classifier for this data set. Right. And you've heard about it before, of course, right. Okay. So, there is another way of doing dimensionality reduction that was specifically invented for visualization purposes. Not for not for compression, not for Yeah, not not not to save memory. Why? Because it doesn't allow the generating data is not a generative process, it just does the down projection bit and tries to do the down projection bits such that you get an image that is pleasing to look at. And that algorithm is called C stochastic neighbor embedding. And it was invented by this wonderful chap, Sam robots. He was actually invented, I think, a postdoc working with Geoff Hinton, and then he became an assistant professor at Columbia, or NYU in New York, and unfortunately died very, very young. I remember I got to interact with him a little bit when I did my PhD. And, and it was very sad when he died. He had this wonderful, actually pretty cool idea. He used to eat, he used to give wonderful talks, I got to see him give a talk once and it's like, was really nice. So I'm not going to be able to present it as well as he did. But so it's, it's a very smart idea that is genuinely just motivated by making a nice visualization. So here's the idea, we have our high dimensional data set x. Remember, it's, you know, size n by d, or m by p. And what we're going to do is, we're going to say, we are going to represent every single datum of this data set in a low dimensional space, two dimensional space, maybe we'll call that datum, why. So every x now has a y, and they are our representation of the two dimensional space. But we're not going to build a map that takes x and maps it onto y. Instead, we're going to first think about the distribution of x in its high dimensional space. In our case, it's this 30 dimensional space. And then we're going to think of the distribution of Y in this two dimensional space. And we're just going to make those two distributions as similar to each other was possible. Now the question you might have is, well, how do you make these two things similar, if one is 30 dimensional, the other is two dimensional. Ah, so you, that's where the name comes from, you think about features of the distribution in the high low dimensional space that has nothing to do with dimensionality or maybe not nothing, but they are not direct representations of the number of dimensions. And then you can optimize those two things to each other. And one way to do that is to say what it means to be a neighbor of a data of a datum in the 30 dimensional space, and what it means to be a neighbor in the two dimensional space. And then optimize those neighbor properties such that they become similar, you can imagine that that maybe provides a graph, but graphical but graph way of thinking about the data, you could think of a graph of neighbors, and then make that graph in the high dimensional space as close to the graph in the low dimensional space as possible. So here is the the algorithm within the way it's now done. So we first define a property to be a neighbor in the high dimensional space. So for that, we take our high dimensional X. And for every pair of data sets, I n data points i and j, at least in principle, for every pair, you can imagine that that's expensive to do, but there are simplifications of it. For every pair, we compute their squared Euclidean distance in the high dimensional space, or any other form of square distance that you might think of standardize it by some parameter sigma. That's just you know how links scale of when things are called close, and then compute a probability to be a neighbor for these two points. So the probability is computed in some kind of word, you can think of this in many different ways. Can one motivation that some robots provided is imagine that there is the probability for two points to become neighbors is Gaussian distributed, where the probability to be a neighbor is a Gaussian of the distance between those two points in the high dimensional space relative to all the other neighbors, right. So for every single pair, so for every for every eye, you could look about look for one particular neighbor J, and then compare to all the other neighbors that are sorry for me J could think about all the other neighbors I think about all the know all the all the case that are not j here we go, all the neighbors and standardized but that gives you a number between zero and one probability. And that's defines an affinity matrix between the data. Now that affinity so far is as symmetric. So we make it symmetric. Why? Because then the computation is easier. So we just take the that matrix, oh, we, whoa, and symmetrize it so we, like you know, take transpose it and add the two together. And then we also set a diagonal to zero because a point cannot be a neighbor of itself. Otherwise, there will be ones on the diagonal, and that's kind of annoying for the computation. And, yeah, so we have to set this perplexity this sigma parameter to something, we just set it to something. Okay, we'll get to that in a moment. And then so this is So then so same robots originally said we'll do the same thing in the dimensional space in the in the two dimensional space, and that the people could publish it was in Europe. So this nice paper, and then people kind of forgot about it. And Sam wasn't about anymore to advertise it and make noise about it. So Geoff Hinton in 2008. Actually not that was meant some of us are still doing right. Okay, so maybe that was some some I don't know the full story behind this. But as a PhD student, of Geoff Hinton, who picked up the algorithm again, and said, Oh, we could do a minor variation, instead of using a Gaussian distribution in all dimensional space, we'll use a t distribution. Some of you may know what that is. It's a scale mixture of Gaussian distributions. It's just it's just another bell shaped curve that has heavier tails. Why is that good? Yes, because then points can be further away from each other and still be considered neighbors. So it's a bit loosening up the representation. And, yeah, then we divide, we compute a similarity measure between these two graph distributions, p and q, which one do we use? Well, first one comes to mind is pale divergence that we've encountered before, and just optimize it. And then you'll find a representation. So you can optimize it by computing a gradient of the KL divergence, and then following it with your favorite optimizer, gradient descent, BFGS, Adam, whatever you like. And I'm telling you all of that, because it's your homework to implement this algorithm. And I can tell you, there's a cool toolbox for it, that does it for you. So you don't get to use that in your exercises, you have to implement it yourself. But here for us, we can do that. Because part of the takeaway from this course, is also to play with, like this interaction between using toolboxes, where you can just do something very quickly and actually doing it yourself or you begin to understand what's going on. So if you use this toolbox, and you see if I can do this myself, then you can run this machine is fighting it very quickly. And you get this plot for our breast cancer data set. Now Nice, huh? Okay, so someone looks way cooler, right than PCA before. Is it better? So let's first check whether the argument actually default it was supposed to do. So remember that the goal of this algorithm is to take the high dimensional distribution, compute affinities between those data points. So there are probabilities to be neighbors, that willingness to be neighbor, and affinity, and then find a low dimensional representation in 2d, such that the affinities are conserved. So the way that I do this is and I make a complicated plot that maybe you have to look at the code for this later. It also takes a while. So PTC actually runs very fast, but making this plot is going to be a while. So what I do is I make a complicated double for loop, very nasty thing in Python, because I couldn't be bothered to write it more efficiently. And four in the high dimensional space, compute the affinity. So this is this A matrix first. By the way, this is what this matrix looks like. This is the affinity matrix in the high dimensional space between those 600 points that are in the data set. So a white number means that two points are very fine to each other, they want to be neighbors. And a black point means that they totally are separate from each other. And then I make lines between every possible pair of points on the right. So that there are quadratically, many lines in that plot. And I set the alpha value of those lines to the affinity in the high dimensional space. So every single point in this plot is connected to every single other point in this plot. Do you see that? No, no, you don't know you don't. Why? Because only the points that are really close to each other in the high dimensional space actually get a green line. Right, because the points that are in a high dimensional space far away from each other, you won't be able to see the line because it's alpha value is zero or pretty close to zero. So, what you see is that this argument has found the representation in the low dimensional space such that you only see local green lines right there only local neighbors are connected to each other and that means that the algorithm has done its job, it has found the two dimensional representation of this 30 dimensional data set such that only points that are actually in the high dimensional space neighbors are now neighbors to each other. Or the other way around. All the points that are neighbors in the high dimensional space are also neighbors dimensional space? Yes. What's the reason for the shape? Someone is asking? Let's try that out. So what I did is I run the same code again, but now in an interactive fashion, so we run through this is his knee. And now, what I've said this algorithm has at least three different parameters we have to choose. So remember that PCA had no parameters, right? PCA is just you center, you do the SVD, done GSD, even though it's very fast, just all these parameters. The first one is this length scale of what does it mean for things to be neighbors, that's called the perplexity in that community to do bit annoying term. But that's just what it's called. The other one is the, the way in which we measure similarity. So remember that I told you just measure in the high dimensional space, the distance of points toward each other in the Euclidean distance. But of course, there are many norms for vectors, you could choose in a high dimensional space, actually, in any dimensional space. People often argue that in a high dimensional space, it's actually not so much where the points are Euclidean, close to each other, it's more about whether they lie in the same octant of this high dimensional space. In a two dimensional space, that doesn't seem very intuitive. But in a high dimensional space, that's actually much more important. Like, do they point in the same direction, then they should be similar? Or do they point in different directions, and then they're dissimilar. And then there's an annoying third component of this algorithm, which is that you have to randomly initialize it. Because, of course, there are many different ways of taking a 30 Dimensional thing and mapping it onto a two dimensional space. So you have to initialize the algorithm somehow. And you do that with a random seed, I've set it to 42. But you could set it to a different set, for example, for 41. And then the algorithm runs and this is what the embedding is. So this embedding is just as good as the previous one. Hmm, huh. 40. Oh, okay, that's also just as good. So why is it just as good. So remember that the only thing the algorithm is trying to do is to get points that are neighbors close to each other. So global structure really doesn't matter whether this, this sort of long snake here, whether it twists this way, or the other way, or whatever, doesn't really matter, right? Because these two points are still like a point here. And there is not going to be a neighbor, no matter whether we twist this a little bit further or not. So if you look at TC plots, you should never look at the global structure, whether something is on the top left or the bottom, right, or whether it's U shaped, or snaky shape, doesn't matter. It's completely irrelevant. I said, we can also change this way of measuring distance, we could measure the cosine distance or whether things are in the same region of the input space. And if you do it this way, then we get this representation of the data. Again, with random initialization, I'll go back to the Euclidean thing. And we could also maybe most importantly, change the perplexity so that's actually the most annoying parameter of the SNI. If this number that you have to set and this one, so these first two things arguably don't really change the, I mean, once you know how to look at these data, that is these plots, you may realize that they are actually like, you know, in sense of local structure, they're actually quite similar to each other. So this snake kind of whines and bends around, but you always get like a large cluster of points that are clearly separated. And then there are a few that are like within the original data. Right, this one, this one again. So here again, there's these few points down here, a large separator thing, and then there's this kind of point where they overlap. But this perplexity parameter actually changes things a lot. So if you set this so remember that this is the link scale of the Gaussian affinity. So if you make that number very small, then that means that basically no point is a neighbor of any other they become more and more separated from each other. And so eventually, they kind of cluster into ever, ever smaller subgroups, they only get to see very, very close nearest neighbors, as points that are like connected to each other, and they completely disassociate from each other. And as we go up, you can try that for yourself as well. You get to you start to see kind of clusters emerge. And now you could maybe see in this, if this is the correct choice of complexity, then maybe there are three clusters. Actually, there are these relatively clearly benign, relatively clearly malign, and then there is this interesting group in the middle that overlaps somehow. But if we take the perplexity higher, then maybe it's more that they're just two arms, right? So there's this clearly, and then these two groups are maybe the same. Maybe they kind of overlap with each other. So how do you how do you set those parameters? Nobody knows. So we have one of the world's experts on TCP You're in Tuebingen Dimitri Kobach. He works in the medical faculty, he has implemented actually parts of this algorithm himself. He's very good at this. And I've asked him, he said prolixity kind of tune it a bit, right? And this cosine Euclidean, you have to think about really like the interpretation of similarity. When do you think that there's something to do with the physical interpretation of the domain of the dimensions. And just to make matters finally worse, remember what I told you about PCA? If you change one of the dimensions nonlinearly, you get a different representation. Of course, the same is true for TCS. In fact, this is true for pretty much any dimensionality reduction technique. If you take a nonlinear transformation of the data, the output changes. So for tabular data, there is never a correct way of doing. So this sounds like I'll get to you in a moment. This sounds like I'm dissing Disney, and maybe I am to some degree, because I think that there is an there is a in especially in the machine learning community, there is a bit of a tendency to think of the Chinese newest toys as the the coolest way to do things by just because it's more complicated, because you need SGD or Adam to train it. And because it has hyper parameters, you get to June, somehow it must be the cooler to. And that's actually not necessarily true. So you'll notice this yourself in your exercises, maybe this week, they're sort of designed to frustrate you a little bit with Disney. If you have to implement it yourself, you'll realize that PCA is like one line of Python. And very much nothing can go wrong if you decide to use it, because it's just that one line. And it's super stable linear algebra has no stochasticity. It's even it even deals with badly conditioned data. Everything is like I mean, SVD is like the most rock solid linear algebra algorithm out there. And Teasley has all these parameters that you have to choose and get right and everything. However, of course, in like certain communities, like for example, in bioinformatics, these algorithms are actually very popular, because they do allow you to do some interesting things, right, they allow you to play with the data, to tune those parameters around and stare at them and get to see structure. But you have to be very careful to understand what kind of structure you should see there, you have to understand that the global structure doesn't matter. There is no llama in this picture, right? You also have to understand that oops, sorry, verb. Or maybe actually, this is a good way to end right? You also have to understand that these clusters, you see that they are a function of how you set the parameter sigma. So if you make the perplexity small, you get clusters, you'll always get clusters, no matter whether they are clusters or not in your data. And you have to figure out whether those clusters are actually Villano. And then, one thing I want to point out is that every time you see TC plots anywhere, social media papers, they always are colorful, will always see classes in there. And I'll notice that TC does not use those classes, right? It's the entire point of the algorithm is that it's not using labels. And so people color the data then to say, look, I've seen that this algorithm has found structure. But it's maybe and I've done this now myself as well, maybe dangerous to do things this way to always look at the data already with the informative structure on it, because it's very tempting then to look at this graph is a special type of cell up here that always ends up up there, or, you know, there's this there's this thing that this is long tail that we've now seen all of these plots that comes down. And, and I mean, maybe this is actually useful. So, okay, long story short, what should that actually lead to? It should lead you to look, go back to your data, and then try to figure out what's going on. So you could, for example, say, oh, there's this cluster of clearly maligned cells, and a cluster of clearly benign cells. Let me look at a few of those, what do they look like? How to what, what does represent them, and then African, put them aside. And now look at this intermediate region with these two types, these overlapping labels, what is about what happens with those cells? And then maybe I can use that as a way to build a better tool, right as an information to start further analysis. But you should not use this algorithm to train a classifier, basically. Oh, speaking of which, what do you use to train a classifier? By now, maybe you've got the gist of this. Actually, we're trying to do very simple things. So don't say five layer cons net with no patch norm layers in between and trained with this new madam algorithm that I found because it's cool. What's your very, very first classifier if you have labels, binary labels logistic regression on good choice, logistic regression with which kind of which kind of features PCA features interesting idea. So, maybe take that with you and think for yourself about what would happen if you do this with PCA features, and why they might be good or bad. So the two PCs already motivated with the choice of loss function before we decided what the last function is, which in your case is least squares for the not sorry, not least squares, but cross entropy. So, for softmax outputs, right, or like logistic outputs, but what is the feature of how do we represent the function that map's from the input to the output layer of this network? What's the simplest function you can think of? What's the simplest, the simplest transformation of a data set, you could possibly think of? A linear one as a single vector to project onto. And then we have a one dimensional space, we want to do a cut through that. And you could call that linear logistic regression is actually this algorithm has been around for longer the basic basic version of it, and I'm pretty sure almost every one of you was taken an undergraduate classes encountered it. lda, linear discriminant analysis. So let's see what you could do that with our data set. Let me tell you to start. Okay, so can you play the game again? Who has heard of LDA? Before? Who's heard of the other LDA? By the way, there's two different LDS. Right, there's linear discriminant analysis, and then there's latent Dirichlet allocation? We're not going to talk about that that one. So there's linear discriminant. As is, and interestingly, it was less than PCA seems like PCA is really your bread and butter algorithm. And LDA is like a little bit, but actually LDA is older. So LDA is the answer to the following question. I've now taken our data set again. And I've just sorted it. Actually, I've done this in Python as well. So where's my mouth? Oh, ah. So my machine is really struggling. So I've just made a plot of this data set. And I've just sorted it by labels. So before it was unsorted, now it's sorted. Now, what you could do is you could actually stare at this data set like this, right? And sort of saying, Okay, you can kind of see that there is a line here going, okay, so Well, okay, you can imagine where my mouse is, right? So here is the separation between cancerous and non cancerous. And if you look to the right across the data set, maybe you can visually see a line, right? So which of these dimensions is maybe the best one to pick? Just as a mental game, this is not lda, right? Just as a mental game, you could look at this, I think this is actually something you might do yourself. It's a tempting, tempting thing to look at this and say, I don't know, maybe, maybe dimension six, maybe 23. Right, because you can see a relatively clear line there between the two classes. But what the LDA provides us an answer is to say, of course, you could take any linear combination of those 230 dimensions, and maybe that will give you a better separation between those two classes. And that's exactly what the algorithm is trying to do. Let's find the optimal linear projection onto a one dimensional space, such that these two classes are as far away from each other as possible. Now, annoyingly, if you really, if you pay attention to those sentences, you may realize that classes far away from each other is a sentence that needs a little bit more detail to be explained. And we'll do that in you know, two slides or three slides. The first one is a historical one. I'm sure all of you have heard that story before, but maybe I can add a bit of a twist to it for you. So LDA is a algorithm that was invented by this guy, Ronald Fisher, one of the most controversial statisticians out there have already mentioned him a few times. It's easy to pick on him because he had a very complicated character he worked at did I tell you the story before before about Rothamsted Research Station in the UK, so Okay, so this is this is maybe not like the, like, textbook version, but a a nasty way of telling the story of statistics, you may have noticed that in Germany, there's no statistics. You go like there's actually two universities in Germany have a type of statistics department. And everyone else just has mathematical stochastics. Because statistics isn't really a field, at least not in Germany, or in most of the world. In the UK. It is in the UK. Statistics is its own thing. It's separate from math, and physics, and even computer science, and it was just kind of a bit of a thing emerged as its own community, and it's due to this guy. So he decided that he wanted to have his own Field. Why, because some people say, and this is really just hearsay, he couldn't get into a math department and Cambridge, mathematicians didn't want him to want him there because he wasn't a clean physicist didn't want him either the proper theoretical physicists, and he worked on crop science. So getting more out of the soil. And then you also got interested in eugenics, which, of course, in Germany, we have a much more complicated history with, but there was also a strong eugenics community in the in the UK. So eugenics is the, quote unquote, science of separating the species from each other, and making them flourish to you No Man's delight, literally. And of course, it has, in particular strong undertones of racial segregation, and tuning the human genome making the right people have babies. And Fisher was very much interested in this. So for his day job, though, he mostly worked on agricultural questions. So he made lots of tables, and developed mathematical tools for analyzing these. And this is actually like the prototypical paper about this. So in this paper, he talks about classification, between initially just two different kinds of flower, these two Iris flowers and Germany called chimera, India. And there are two different ones. One is called iris setosa, you can see it on the left. And the other one is called iris versicolor. On the right, and those were, forget about the third column for a moment, those were both collected, I mean, the images were not collected. But these numbers below they're from the original paper, they were collected by our colleagues of fishers in the like on an on a peninsula that is on the eastern shores of Canada, just north of the US Canadian border, north of Maine. And in the same field, this guy went around, forgot his name. And he collected measurements of about 50, each of each of these types of flowers, and what he measured for different things. The So, okay, so these are flowers. I'm not a biologist, or a botanist, certainly not. But I can tell you that I know that flowers have these two different kinds of leafs, they're called petals and sepals. The petals are the shorter ones in the middle. These are the in German, they're called. I think they're called gluten blood. And the sepals are the longer ones that go out. And in Germany called case blood. And you can see that they're different, right, clearly. And they have different lengths and different widths. So this guy measured lengths and widths for both of these types of leafs, and you can see them below, right. So that gives us a data set that has two classes, so far Setosa, and rosy color. By the way, the colors don't matter, because these plants actually have very varying colors in in the wild. That's also why it's called versicolor. Very easily color. And now our goal is using just those eight dimensions, sorry, four dimensions. How do we separate these two classes, these two races of flowers from each other. So this plot is not actually an official original paper, but it's become a common thing to do now. So you could take those four dimensions and plot them against each other in a kind of a matrix, if you like. And so you see on the diagonal, of course, you only see three lines, because it's the same dimension plotted against itself. And you can also plot them against each other in other pairs. And you can see that in some cases, those classes, some combinations, those classes are very well separated from each other. So my pointer has stopped working. But you can imagine that the actually maybe I can use a mouse, in this case, and or up here. So these are different plots, because they're not the other diagonal, maybe these two as a pretty clear separation between Setosa and versicolor. So you could use that as a classifier, if you like. This is actually not what Fisher was trying to do. But that's what the algorithm is being used for now. And now that that that at question could be which combination of those four numbers could they use to maximally separate those two classes from each other. And here's how Fischer egg formalizes this notion. So remember that those classes are distributions. They're clouds of points. And so we have to describe them, at least in sort of a two dimensional way with two different parameters. One is rare, they are their location, and the other one is a Y. They are their covariance. And so what Fisher says is that we're thinking of our data set in terms of two sufficient statistics. A reason to think about them is that they might be sort of the random draws from from IID random variables. So if we sum them up, we could think of roughly Gaussian distributions, we get a sample mean and a sample covariance, and we get those differently for every class. So we take the labeled data set, now we very much use the labels take only the first group. In our case, it's the benign classes, right? sum them all up to get the mean, come to the class covariance. And then we go to the other class, the maligned ones, find the mean and the covariance. And then we define a loss function. For the so okay, we're going to choose a projection w is going to be a vector. And that vector, we get to apply to the to the data set, and afterwards is a one dimensional data set, they are not 30 dimensions anymore, or for like an official thing, but only one dimension. And in that dimension, we now have a distance between the mean and a width of both of these distributions in a one dimensional space, right, once you project down, and what what Fisher says we should do is we should try to make the means as far apart from each other as possible when measured on a scale of the width of these distributions. So that means on the one hand, we want to make the classes far apart from each other. And on the other hand, we want to make the classes like very compact, so that they are like because, like being being far apart from each other for distributions that are very wide, it's not good, you want to have two clearly separated blobs, right? They go away from each other. So we want to minimize this ratio between what's called in interclass mean and Intraclass covariances. Now, there's a little bit of math. So here's our function. This, these are scalars, right? Because they're projected onto w, but we can write them with W. Now here's one line of math, we want to maximize this expression, which is a function of this vector w, I've just plugged back in what that is. So there's two, this is the bit where it's slightly complicated. So I'll use two minutes for it. See, see my can get my my normal projector again. No, no, it's just impossible. Okay, so this expression at the top this interclass distance, that's the difference between these two mean squared, you can write this using the original class means as like this are the outer product of these objects. And you can think of this inner thing as a matrix itself, if you like. And the interclass covariances. Again, we can write with a matrix that is a sum over the two individual classes and their outer products. This is not just the the joint covariance, because they have different means right? One class has mean one than the other bias classes mean to. And so now this is a function of w, and it contains matrices. So we can take a derivative with respect to w, you can do that for yourself as well later. And you'll notice that this is a ratio, so you'll get you know quotient rule, first, you get the derivative of the numerator divided by the denominator minus the numerator divided by the derivative of the denominator, why should the denominator square times the inner derivative of the denominator, that's this expression. So at the top, a two comes down, one, w goes at the matrix SP remains, and the denominator stays the same. And for the other, we get the squares the denominator, the numerator, multiplied by the inner derivative of the denominator. And now we want to set that to zero. So we can forget about the two. And now here's a classic argument that goes look at this expression for a bit. And you realize that we have shear that this is just a constant, right? And here we have w times. Sorry, if you have Yeah, we have W transpose times this vector times this vector. So this expression is a constant w transpose times the vector that's just a scalar. Or not a constant, it's a scalar, times the difference between the means minus another scalar. That depends on w, that's true, times this matrix times the vector times the vector. And now remember, if you only want the projection, so the length of the vector doesn't actually matter. So that means we can actually treat those first two empty scalars as constants, because they'll just change if you V scale W, but their direction will not change the direction of the vector we get. If this is zero is clearly we have to be something like w can be written as this difference between two vectors times the inverse of s. And that's it. That's the linear discriminant. So I'll leave it to you in the in your exercise to figure out how to use that as a classifier. But we can do that now a little bit in Python just as an end thing before we before we are done. So here let me repeat this story again, that for these kinds of tasks, you can always use a toolbox that that will typically save your design time will just work because it's a toolbox. But the downside of a toolbox is you don't understand what's going on. Or alternatively, you can implement it yourself, then you have to spend half an hour thinking about what you're trying to do, maybe look at the slides again, but then you know exactly what you're doing. And if it doesn't work, you can back fix it. So let me first use the toolbox. The toolbox is, in this case, scikit learn because this is such a basic algorithm that you can use it for it. So I'm in importing this thing called linear discriminant analysis, I take our data set the one with the this like not here, our Iris, Versicolor and Setosa. But cancerous cells or non cancer cells, with the labels, the binary labels, and fit it. So that's how you call this toolbox. By the way, figuring out what to do here takes 15 minutes anyway, right. So just getting the toolbox to work took me about as long as implementing the whole thing. And then you want to project the datasets or the one dimensional thing. And that is called Transform, insipid learn what those y of x. And now ideally, I would like to know what this projection actually is, because I want to look at it, it turns out, I could learn doesn't give it to you just don't. It's a private member of the class, it's hidden. So I thought, Okay, I'm going to be smart. And what I'll do is I just take this, I have to have this ability to transform the data set, so I can transform anything. So I'll take the unit matrix, and transform it. And that will give me a vector that is actually the transform that is applied to the data set. Right. So it's like reverse engineering psychic loan to tease out what it's actually doing. So I'm taking a unit matrix, and applying the transform to it. And then I make a plot. And what you see here is, let's first look at the right, these are these are the ideally separated best linear discriminant of these two groups malign and benign data points, and second already standardizes, or in this projection that it does it standardizes the data so that the discriminant is actually zero. So the optimal point to cut this at zero, and you can see that the numbers are kind of meaningful. And you can see that this is actually a pretty good classifier, right? Given I mean, imagine you were trying to trying to build a medical tool, a product, maybe not so bad, right. And remember, that even comes with a kind of, it's not really Bayesian, right, there's no proper uncertainty. But you can probably imagine that if you look at this for a while, you can come up with a way of getting a confidence estimate from this classifier, and actually part of your homework. So I'm also plotting on the left the No, I shouldn't be doing this, this this projection that the algorithm actually applies. So this is what the discriminator looks like. And you can see that sort of it's 14 and 19 and five, are the interesting dimensions. 14 Uh huh. And 19 Hmm. of five, wow. Okay, so look, it's not, it's not the kind of thing you could just see with your eye, it's actually like, it's the combination of them, it's very difficult to get an intuition for this. That's why linear algebra is cool. You can also implement it yourself not using scikit learn. So I separate the data into two classes malign and benign, done that before I compute the class means because we need to separate those two from each other. And then I compute the entire class, no, intra class covariances for the first and the second group and sum them up, and then compute, well, pedestrian language, I compute the inverse of SW and apply it to the distance between the means there's not actually what I do, I just call it linear solver, because it's more efficient to do but. And that's our projector, and I standardized the projector to length one, so that it's a length one thing and make the same plot again. And you can see that you get a very similar kind of plot. It's just not centered and standardized. So but our, you know, discriminator probably lies somewhere here. And the plots look exactly the same. If I go up and down, kind of see that they look the same. You can also check that for yourself, if you look at the code, and here is our discriminator. And it looks the Oh, it's not quite the same. But the plot on the right is the same, right? Ah, so I'll leave that as a little bit of thinking homework for you. Why those are not the same. And I mean, here's the hint. Remember that I did some hacking to get the projector out of sight learn. And when you do that, it's always dangerous to say something happening inside that one has to keep track of. So this plot is actually the more reliable one because I've written it myself and I know what I'm doing. Okay, so this was the end, I can maybe conclude with a bit that Fisher also did this for his iris. And then there is another Iris actually Virginica. It's a variant of the residual or one that happens to be in the southern part of the US. And so you can see that they are not so well separated. So at this point, sometimes people tell her like, cautionary tale about eugenics and how it's dangerous to separate these two classes into separate species based on this kind of data just because you want to classify it into a different class. Unfortunately, that story doesn't quite work. Because if you read the original as as nice as it would be, by to this Fisher for it, but actually, if you read the original paper, he goes into great length about figuring out what the role of virginica actually is. So it turns out that if you're a botanist, you can do of course, other things than just measure petal and sepal, width and length. And you'll find that actually, these are three different species, because they have completely different chromosome sets. One of they have different numbers of chromosomes, I think the Setosa, one has like 30, and the virginica, one has 13. And every single one is a hexaploid. So it's actually probably a mixture of two different types. And then what Fisher points out is that the ratio of the size of the genomes of these two potential source flowers happens to be pretty much proportional to the distance they have in Lea. And that is for him evidence that versicolor might actually be an offspring of virginica and Setosa, that has happened from cross pollination. So that's actually a pretty smart kind of story. And it's used to this day as an argument for the two species being being related to each other. So it's not quite so straightforward. But maybe it still kind of works as a mental kind of motivation to think about what to do with LDA. So here's this is the end of the lecture, dimensionality reduction, which is all we did today. And that's it will not come back to it. Next week, we'll do something else next week, is a tool, a pretty sharp tool, because there's quite a few of them around to turn high dimensional complicated datasets into low dimensional ones, and that the first time someone gives you a data set, this is what you're supposed to do. Think about the data set, figure out where it comes from, how much you know about it, and then choose an algorithm that may allow you to look at the data in low dimensions. As you've seen, this doesn't always work. And you have to know what the algorithm does to be able to use it. And that's generally a good idea. So before you use a tool, make sure that you know what you're doing. Please give feedback. Last week, and the week before, as I said, at the beginning, we only got like one, or in the second class, second week, I think four or five different pieces of feedback, which is very difficult to do statistics on. So please scan this QR code. Now if you're at home, or here and provide some feedback. There's also currently ongoing evaluation of the lecturer from the department. And I hope that everyone here has gotten an invitation to take part in this, please take part in that as well. Otherwise, I get annoyed emails that you're not participating in, then I have to send you an email. If everyone has scan the QR code, I can also finally tell you that there's I didn't get to invite you to our seminar the last two weeks, I have to do it. Now this week, we have a cool talk by Santa pester. She's actually a PhD student from both tubing and NYU. She's sort of traveled the world back and forth doing her PhD a lot. And she'll be talking about lifelong robot learning. If you're interested in kind of less former talk than what we had in previous weeks. I remember I noticed that some people might have been scared off by too much theory. So this one is a little bit closer to the action quite literally. Okay. You can also of course find more about this on Toxa. Two, I'll go back to the QR code so that you can actually scan it if you like, if I can. Okay, and thank you very much it doesn't seem like there are questions in the chat. So I will close the call here as well. Thanks a lot. 

