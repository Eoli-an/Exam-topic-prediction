So good morning, I think you online should now be able to hear me if you can hear me then please say something in the chat or I don't know, raise your hand or something. Great. Thank you very much. Just need to start something right all right. Good, then, let's get started. Thank you all very much for being here being back for being joining the call this morning. As always, I'd like to I'm hoping that everyone who is in the room has already scanned in and registered their attendance, everyone in the call doesn't do so. And now let me just figure out how I could start the presenter. Okay. So, as always, let's quickly get some feedback out of the way. So last week's lecture was maybe like my textbook example of how good for a lecture should look like everyone's very happy about this lecture. Of course, it's also a bit of a crowd pleaser. So it wasn't so hard to get good scores on this particular because he also liked the speed. The feedback actually take off the mask, the order is much better. They mostly want to address two points of feedback that I got one it says actually on the site, the top left, someone wrote, I was a little bit too over optimistic regarding our ability to limit the increase in usage. And actually, I agree so and maybe in the spur of the moment that wasn't particularly clear enough about this. I'd actually thought about this problem before I wanted to make a slide and then didn't in the end. So just to be very clear, I do think that we need external regulation on how much energy is actually used by the system by us. So how do we do that? There are various different political processes that have been proposed to achieve this, right, like a carbon tax or cap and trade schemes. I'm not an economist, so I'm not going to argue for one or the other as a solution, but it's clear that there has to be some kind of market force or maybe regulatory force to ensure that people don't use too much electricity. The point still stands, though, that it's actually possible to achieve continued growth with more or less constant electricity use. Okay. And the other point that isn't actually on this slide is actually between the lines and people told me that it's actually kind of over happy to hear that, that to be given job advice that you can basically, like, raise a few thoughts on how you might have to choose your career. But that doesn't, of course, necessarily only apply to sustainability as a topic of societal concern. Actually, we'll get back to that a little bit over the course of the like the last lecture. One of the strengths, one of the the exciting aspects of AI and computer science and digital technology at the moment is that it can basically help with all of societal of society's problems, sometimes more, sometimes less. And, yeah, so I'm not saying you should work exclusively in sustainability, there are many other interesting topics to look at. Okay, so with that, let's come to the actual topic of today's lecture. I've already lost focus on this. No, go. So last week, I also told you to start working on I told you how to start working on your own project. And by last Friday, at something teams submitted a pre registration and abstract basically, of what you wanted to do, I've only looked at a few of them, these will be exciting, I'm very much looking forward to see the results of those. And so now over the next few, what, three weeks, I think you're going to be working actually on those projects. As you do that, hopefully, you'll encounter a few of the concrete challenges of actually working with data with data that you curated yourself that you select yourself. And as you do that, hopefully in order ready just in time, I'll try and use the lecture to talk about some of the sort of soft, not so quite concrete issues you may encounter as you work with data. And so advanced warning, much of what you do today, and actually also the two lectures afterwards, isn't particularly structured, today's particularly bad. In this regard, the next two lectures are been a little bit more formal. Mostly because this is these are problems that a don't really fit into any particular scaffold, and be, they're still so early in the development of our field, that there aren't really clear solutions that everyone agrees on. So what we'll talk about today is how to document and structure your work, and what kind of problems you may encounter. As you work with data. There'll be two different parts of the lecture, the first, roughly half the talk about how you structure a research project, like the one you're currently working on. And then the second half, I'll talk about issues that arise in industry, commerce, when you're working with data commercially, when you're trying to build an organization that revolves around data. So let's start with the scientific problem. So if he this if you work computer scientists, but you know physicists, or chemists or biologists that at some point in your education, you learn to begin a project, how to write a level. So that books so I studied physics, as an undergraduate may have mentioned before, and I, again, it's totally normal to actually get lipids as well as the things when you arrive in a Latin your project, or your master's thesis, or even a PhD was that ceremonially your PhD advisor or your magical hand you your laptop, sort of bought by the Secretary of the group special, six pieces of paper found, and you're expected to keep track of everything you do, with? Actually, typically not a pencil, but like an indelible piece of ink, like a fountain pen or a Bible. Why? Because you're supposed to be able to reproduce what you've done. When you reach your scientific conclusion, because as you're trying to solve your research question, you're maybe trying various things. And so one thing that will actually happen is that you will get something wrong, right? And then you will want to for yourself, have a recollection of what you've tried before and why you decided to do things differently, because they didn't work. A second issue is that you may realize later on that you did something wrong early on that you would actually correct by looking at the numbers again. And the third issue is that someone may come along and criticize your work. And then you'll have to prove to them that what your experiments you did, were actually, you know, valid, trustworthy. And so this is an image that I got from the website of the Mustang society at the, at the mpg, they're actually like training courses for natural scientists on how to keep a lab book, and what to put in there and know where to put, you know, even images of the of the lab set up and all the things you don't even typically don't even think about that you sort of have to try and think of the stuff you don't think of to write it down. Like, I don't know, what was the temperature in the lab that morning? Things like this? Perhaps So someone says maybe I'm using the wrong microphone. Let's check. Yeah, I'm probably do. Now now, maybe the quality of the of the voice might be better. Is it mature? Maybe I'll go like Oh, a little bit further away. If you can still hear me then. Now I've got the right microphone. Okay, good. So it's Connor. Hmm, okay. Sorry, be check. I can do something about this. Ah, let's try like this. Yeah, this might make my TV better. Okay. So. Yeah. So why is so I mentioned that one problem might be that people actually may come and criticize you for your work later on. This is maybe the most threatening scenario of them all. And it does actually happen. So a few years ago, there was a very famous case that made it into, you know, even national media. When Britta Nessler she's a well, physicist, material scientist at the MIT Kasco, she was awarded one of the maybe the most prestigious German science prize, the lightning award that goes out to about three to five people every year, funded by the the FDA. And when she was awarded the prize, someone complained to the tfg and claimed that her work was fraudulent. And then they actually stopped the process like this, she wasn't invited animatronics. This is usually a big ceremony, where, well, before Corona, right, everyone gets to go. And then all the awards, the award, these get their prize, one after the other, they give a good cool speech. And she wasn't invited to that, while the DFG was checking through her lap books. And in the end, actually, she was able to prove that her wasn't fraudulent by digging out lab books from like, up to 1999. So that was in 2017. So 18 years of work, basically, she had somewhere on the desk, it was actually able to prove that she did the rights to that her work was solid. Now imagine what that does to your career. If you're like at the, at the peak of your scientific career, you're getting like the most coveted award, and then someone comes and claims that it's wrong, what kind of mental load that must be, and then actually being able to prove that the claims are wrong at a very impressive achievement. I mean, in a way that even elevates her work even more. Now, would you be able to do that? I mean, you're not in the business long enough yet to think about what it means to go back like 14 years. No, yeah, no 18 years of your work. But think back to your like maybe bachelor thesis research project you may have done at some point, maybe a seminar talk you gave about something, would you be able to go back and reproduce your results even know where you've stored them? So you may think this is an extreme case, this only happens to people who are you know, at the very top of the game, very famous actually happens all the time. So here's an email I got in 2014 and I was a postdoc from my diploma advisor. My master's thesis advisor eventually tank is super famous. One of the most prominent scientists in biomedical physics in Germany, maybe in the world, he was for a while was in the running for a Nobel Prize. As tank director in Heidelberg, where I did my master's thesis. We wrote a paper together in 2000. And we voted in 2006. I think it was published in 2007. I was a master student was on a very old laptop, and then after that, so I life happened, right, so I moved to a different country, but the new machine, one laptop died, I didn't have a backup of some parts of the harddrive. That was a different time by we didn't have like Cloud backups, and it was just a little bit more difficult. And I came back from my PhD I moved to tubing and I was opposed. He actually needed this person, because he's one of my mentors. It's the kind of person I asked to write references for me. And then he sent me an email like, this is a very typical interview thing. You know, by the way, one line, huge amount of work. And I'm like, Can I do this. So I actually sent him an email and said, I'm very sorry, I've lost this data, because I've lost the hard drive that it was on, it's gone. Like, I just can't get get it back. And so I go, I don't know, get a coffee, come back half an hour later, I have another email from him. It's an auto very, we've got backups. And so he CC is IT admin in that email. And when I came back, I already had a second email from the IT admin with a link to a file to a zip file that contained my entire diploma thesis. And I downloaded it, and lo and behold, I was able to reproduce that figure took me actually an afternoon. So I'm very proud that I managed to do that. But it's a teachable moment, that no matter how early in your career, you are working on something, it may people may come back long later. So this was, you know, seven years later, to ask for details of your work. So do you think you can reproduce a figure you made for your bachelor thesis? If not, then we have to think about how we could do that. And if you're now doing your research, this term project the next three weeks, one of the goals, which is why we ask you to submit a Git repo, this is much more sort of 2022 way of doing things is to get you to think a little bit about how you structure your code and the process. So that you are actually able to come back and understand what you did, and maybe even prove to other people that what you did actually works. So you have one big advantage, which is that you are computer scientists, and the stuff you work on tends to be written in code. And code is by nature much more reproducible than experiments in a lab. So how could you structure your code and your work your documentation of it so that it's reproducible? Now, as I said, at the beginning of the lecture, there is not going to be one answer. Actually, what I'm going to show you now are a few examples that are going to be a little bit orthogonal to each other, from my own work and work that's being done in my research group. As examples of how to structure code, and other documentation, I'm not going to claim that that's the optimal way of doing things. And I actually would like to invite you to think along on how you would have done this or whether you're currently doing it differently, what you like about the way we do certain things and what you dislike, and obviously, I'm not going to so by the way, so someone asked in the chat, if you asked me to question my results, no, actually, they wanted to reuse the results of the experiment for something else. They wanted to run some other analysis on it. And yeah, good. So yeah, let me just show you some examples. And then this is obviously not the kind of thing I'm going to ask you in the exam, by the way, right? I'm not going to come along and say, oh, you know, what's the right name for this folder in your the stupid question to ask, but instead, so I'd really like you to think about how you are going to write your analysis of your term paper for the next few weeks. Okay, so here's what I used to do. This is an example of a pretty old project, actually, um, 2014. Right, after pretty much after No, in before I got that email. So, and it's a little bit censored. So I've, I've changed the names of a few files. So one thing I used to like and do is so okay, maybe I should say, first time, the one like a main challenge with research code, is that it's not production code. And it's fundamentally qualitatively different from how you would structure code for a product. If you're building as a piece of software, an actual you know, a library a product for for a company or for research, then you know, what you're going to do you start with your with your Is it possible to find my master's thesis online? I'm not I don't think my master's thesis is online. But the papers of course, you can find right. Just don't think in Hindi 2007? No, Hennigan think so. If you're writing a product, right, you have a first effective meeting with all the stakeholders, maybe you're going to do with you have a scrum master. Someone talks about what the what the design goals of the project are, you make a big outline, you make a big plan, you know exactly what kind of functionality you need. And then you start to get ready for when you have like issues to like, take off one after another nice. But in research, this is not how things go. Right? You start off with a question, as you all now do with your datasets like I think it should be possible to do this thing. Let's try that. And then you try and it doesn't work like you because we because you thought that things were in a certain way you thought that something was possible that turns out is a little bit, it's not impossible, but it's a bit different than what from what you thought is you have to adapt, you have to change, sometimes you have to change your code in a pretty fundamental way. And that means the typical processes of good software engineering don't always apply. They apply to some degree, but they don't fully apply to research. So one thing I like to do for more open ended foundational research, is to have a number of files that like visibly document, the process of research, just like the pages of a lab book would. So I'll code structure for my code would look something like this. There'll be folders, one, four. Again, I like these three letter acronyms. It's a bit silly, maybe, but it kind of makes for a nice visual, visual, pleasing view of the of the folder called Source and data and experiments and documentation, where the obvious, obviously, what's in documentation is the papers that get published in the end in experiments are things that make figures in the paper at the end, and also experiments that actually leads to the results in the end. And in source is the code that actually does the work the actual, you know, the the algorithm, the result, the product in the end. And data is typically a symbolic link to some datasets, if you actually need data, right, depending on what you do. So these experiments, they tend to have these kind of numbered forms. So there's an experiment number one, experiment number two, and so on. And what I do there is that actually, do I have this on a slide? Yes. So a first experiment might look like this, it has a doc string at the top that says, what we're going to do is that I have to I have this question that I'd like to solve, right, there's this thing that I think it should be possible to do things in a certain way, let me try this out. And then I try it. These days, this might be also a Jupyter Notebook, not just a Python script knows, right, if it's probably a bit more flexible in terms of markdown text to talk about what you're trying to do literal programming. And then you try something out, you get some results, and then you write underneath a result set. Okay, now, I understood the following thing, period. And ideally, it's the amount of work of like, one day or so. And then you come back the next day, and you try the next thing, maybe it's a correction of the previous thing, and maybe it's the next step. Importantly, if you figure out that something you tried was wrong in the first experiment, or the nth experiment, then you don't just delete it, or, you know, overwrite it in a Git repo. But instead, you actually create a new file, experiment number two, we said our there was this problem in the way that I tried to do things, this doesn't work, I'll do it differently. So you could think that you could push that into a Git repo. But anyone who's ever tried to revert a Git repo to a particular state that worked, knows how painful this is. So if you if you know that you are going to evolve your view on why you do certain things, it is not a good idea to to, to use the functionality of a Git repo for that, it's actually better to just plainly keep the text around to look at. So that when three weeks from now, and this actually happens more than you think, you come back to think why did I Why didn't you Why didn't you do things this way? Again, like we seem to decide, like three weeks ago that we set the learning rate some way or we use this architecture rather than the other, or we had this idea of how to tune this one parameter. And then never, we never actually did it. Why was it that way? You could go back and say, here's the corresponding text, you tried this, this was the result. And that's why we decided to do things in a certain way. Are those results still valid? Or do we in the context of now actually think about them differently? No, okay, then let's keep doing things this way, or No, actually not have you realized it's actually quite the other way around, we can discard this experiment and do it again. If you're doing data analysis purely for visualization, the next three weeks, maybe this won't be so important. But if you're doing a PhD, over three years, that's certainly going to come even in a project that takes three, three months over your your master thesis, you're going to get to a point where you ask yourself why you took decisions in a certain way. Another thing that I like about this structure is that also my minor minor thing, I tend to use this with my students that if I contribute some code, and I use like I start to count from the top something like far away in the numbering scheme, so you kind of know, basically, which author did what you could also use, you know, author initials to do that somehow to, to separate who did what, of course, you can come up with all sorts of structure for that. And then there's a piece of code of course that makes figures for your paper. And those those are among the most important pieces of code to keep documented so that people can come back later and replicate Use your results. So you want to have those in a very clear structure that you can find again. So I tend to use and use to call them, you know, fixed figure underscore something. So you could come back and say, Okay, today's this, this, this figure in the papers, it does this down here is the paper folder, which contains a bunch of fingers that may be so Okay, back in the day, these are actually tech files. I'll tell you next week that by now, I've changed my mind and should actually be PDFs, but for a very specific reason that I will come to next week. And so they will go into, you know, like page one of the paper page three of the paper page six of the paper, you want to know how you made that. But there's a corresponding piece of code that is named exactly the same name as the output file, so that it's very easy to go back and said, add this image. But this figure was made with that code. Let's go there. This is exactly what we did. So the question is, ah, so ritual for gaming is a concept that I think goes back to Donald Knuth, someone might want to correct me on that. That is pretty much this, like, it's kind of, well, it's pretty much actually what I what that some of my Jupyter notebooks that are used over the course of this term, are maybe a bad example of how to do this, but they are kind of the idea. So you have to write code, with documentation in it with comments in it so that you could basically read it like a piece of text, right, so that the lines between written words that are human readable, and mathematical computation in a computer should be as blurry as possible, just like when you're writing a Latech document with math, the equations that are in line should be part of the sentence to coach it, ideally be part of a sentence as well. So that's the underlying design idea behind Jupiter that you could, you could possibly have these interspersed Markdown and code cells, such that you could read it like an essay, but sort of, you could essentially write a paper entirely as a Jupyter Notebook, with a title and an abstract. And then the code for making everything that's in the paper could be part of the of the document. This is the the ideal this idea behind things like Jupiter, they arguably didn't catch on to the degree that one might initially have thoughts, right. So these days, we still read PDF papers rather than this kind of stuff. But it's still a nice idea. So this is though, like 2014, things have changed a little bit since then. And now of course, a, at least in my group. And I think this is kind of the standard now in research groups, more and more using Git repos to actually not more and more exclusively using Git repos to document developments of code. And so what I'd like to show you are a few examples of actual Git repos from my group, and most of them public. So you could just look at them as well, which are, in my opinion, an ideal way of documenting your research, because they come along with the paper, people can click on the link in the PDF paper or go to the Git repo. And there they get everything they need to know. And actually, since we had those established, those kind of Git repos, with the process of people asking about our work has changed a lot as we get a lot more questions, which is good, because it means people find it much easier to access it, the questions have become much better, because people can figure out the easy bits themselves. And the process of managing those questions has become much more enjoyable, because you can work through, you know, issues, even pull requests on GitHub. Okay, so I want to show you one example, that is actually not a public repo, you can see it's private. And this is a, an actual live Git repo. So you can see edits there, like from yesterday, because by my PhD student, Jonathan Vega, is I asked him whether I'm allowed to show this is fine with it. And he's currently in in New York working with colleagues there that you can see down here and working on a paper that is actually on archive already. It's a resubmission. So I feel not so bad about showing it to you, it's not really secret. And what the main thing I want to show you here is that this repo is a nice way of wrapping, maybe you like to you could wonder about if I'm supposed to publish my code, what which part of it do I publish? The like the embarrassing bits were started out at the beginning and got things wrong, or only the end result. There are some colleagues who actually like to publish like everything. They even have an open keep repo while they're working. But it's these are getting less and less these days. So luckily, they didn't know where I used to work with a while ago. When in our PhDs like seven years ago, he always had he made a habit out of just having public reports for everything he was working on. Even stuff that was wasn't published yet. Back then machine learning was a very different field. Now the competition is a bit tougher. And I think people don't really like working in this open way anymore. Because you don't really know who's watching, right. So what this report does quite well, I think, is to create a structure such that you can keep the private stuff private, and the public stuff public, and not have them separated in a in a like hard mechanical way. So what you can see here is that this, this repo contains a, a, this is the private part, the private part contains the paper and actually even reviews because in the previous round, right, so the last time we submitted this to a conference, we got some we got some feedback from the reviewer, and we sort of write down what our responses to those are, we have discussions about it, it's the kind of stuff that you don't necessarily want to be oops, sorry, want to have public that was one level too high up. And then you know, there's also like talks that don't have given about this work somewhere. And so the only things that you want to be connected to this work so that you can come back and everything is in one place. But no, maybe not the stuff you want to publish to everyone. And then there is a sub folder that contains the actual work, the code, the fingers created being created for the for the for the paper, and you can like you can see that this is already basically a little nest that Git repo inside of the surrounding one, which already has a doc string, and it's already pretty much finished. So when the paper is accepted, it just has to flip this and make it public. And then this part can be the documentation that's out on the on demand. And hopefully, actually, I haven't checked, it contains, you know, the plots that are being used for in the paper, maybe this isn't a great example of this yet public paper isn't published yet. It's just in preparation. So let me show you another one. This is an example of work from by there is a typo. By an honeybush, not a PhD student. This is a published paper. So you can see it's a public public good report on some arcane piece of algorithmic development for probabilistic ote solvers, if you want to know more about those, that's gonna be a lecture in winter term on numerical algorithms for machine learning. This is maybe in my opinion, an example of the kind of minimum viable publishable result, like what the length ideally should go to two, if you want to document your research to the outside world, so it says this is good report does not contain the code for how he arrived at the paper, but it contains all the code you need to reproduce all the figures in the paper. So you can go in here, and there are, you know, individual experiments that are also in the paper, which is so this is Julia. And you can sort of open those and random and like, you know, clone the clone the Git repo and reproduce all the figures that are actually in the paper yourself. And if you have a question about how the experiments were done, well, it's pretty much answered by this code. Because no, it contains everything you need to reproduce the figures. And then the figures in here actually, just just the actual files, right, just the output. And as you can see, there is no Latech in here, right? So there's no code for how to you know, he compiled the paper that's all separated in a private variable. And actually, maybe also nice to point out is that this, like alongside this paper, Nathanael also published an actual, like a little product, like a library that does this stuff, like implements these algorithms efficiently. And it's a separate report. Because here, the goal is a different thing. Right, the goal is to have a library that other people can also work on also can also edit that have that has like a little community process. Basically, it's part of the defector JL kind of ecosystem. And here, of course, he doesn't include the figures that are in the paper, he just puts in little little animated GIF to show that these algorithms are doing something fun. And then there is documentation of how to use the code not not how to reproduce the results of the paper for that there's just a link to the paper somewhere. And then you can like, check those. What else? Okay, so I'm going to show you two more examples of kind of the progress from simple research code, arguably, too. Like a little product more or less. So goal of the goal of research shouldn't just be to write papers, in my opinion, should be to produce artifacts that can actually help people somehow either analysis that produces insights that can be used in other ways. But if you're doing methods research, like my group does, then the message should be the output. So in the end, the paper is just an intermediate documentation. And then finally, you want to actually write some piece of code that people can use There, there is, like, once you have, you're talking about a product, there is a clearer kind of design process for software where you can use the kind of ideas from software development. And to some degree, at least the second half of this lecture, I'll tell you something more about the challenges that arise from this. So here's an example of a, of a paper that was recently published. It's it was in neurips, last year by Fran Schneider and Felix Daniel, who is this a little bit silly that this is dark mode, I should have changed this before the lecture. And this is a it's a piece of software that is basically a debugger for deep neural networks provides all sorts of visualizations for during the training process of a deep neural network that are richer as an interface, then, you know, your standard tensor board plots, so your standard weights and biases, thoughts. So you can see an animated GIF Here are four distinct does. And it's so hopefully I've opened up the right. Yeah, I think this is the right repo. So this is this is the repo for the actual product. And you can see it sort of made up like a typical deep repo, right with readme and like installation procedures and documentation. And then they tell you what you can do with this kind of stuff. And what to cite, it's not the report that you could use to reproduce the older results from the paper for that is actually a separate report that is cited in the paper, it looks a lot like the other one. But here, you can go and say that there were 12 plots in the paper, how can never produce those, it's a separate kind of goal of one or the other. And in particular, of course, you could imagine that for people like this, people might file pull requests here, often the authors actually do that themselves. If you have several authors, at some point, it becomes meaningful to work with pull requests, rather than just pushing to main, especially once you have releases. So this is actually you know, it has a 1.0 release. It's the kind of stuff that you want people to use with an interface that is, like demonstrated and documented and has examples. And so at that point, that like the software development cycle becomes a little bit less researchy. And a bit more structured with clear goals. And so if they wanted to change the interface of this, they would have to have a version 2.0. And, you know, prepare that properly, and so on. And I hope that pretty much all of you have had a software design lecture in your undergraduate courses. So I'm not going to bore you with you know, Scrum, and agile and so on. And like an extreme example of this is, maybe I'll just show you this to save some time. And then how am I allowed to do that? Why am I not allowed to do that? So that me Oh, nice, huh? We just open admit directly, something wrong with that stupid. So here's this, this is like the most expensive piece of software that my research group has written. It's an entire community effort, not just by my group, but other people as well. It's a large collection of probabilistic numerical algorithms. And here you can see that's actually a product right at this point, it's like it has a landing page. It has community interfaces, you can like it has a proper documentation, there's like an API reference, like the proper thing, you need to actually want to produce code in the end. And so is this like a smooth transition from, you know, this kind of work into. So just writing papers into actually getting a result out. So your goals in research are, of course, not just to document everything and make sure nobody can come back and criticize you, but also to have impact in the world in the end. So at some point, you have to switch from, this is how I made my fingers into, you know, here's a piece of code that everyone can actually use. And the documentation for that is a little bit different. Okay, so yeah, that was a little bit high level, maybe a bit weak. Let me let me summarize a little bit and then wait for questions from Yan can already see them? Like I'm asking for questions. So maybe if you have some, then bring them up after this. So while you're doing research, you have to understand that your the process of research is going to organically evolve, you're going to change your views of how you write your code, and actually what your algorithm is trying to do. So during that phase, you want to document everything for yourself and for posterity in a way that you can still understand when you come back 10 years, 14 years from now, that of course includes comments, but it also includes structuring the code in a way that you're actually able, even without comments to come back and understand how certain things evolved and arrived. Git is not always the best way to do that. Because going through a Git repo, even though it's like a distributed ledger, and kind of, you know, well structured, is tends to be hard to actually manually search through. It's typically easier if you want to understand the process. to just look at a bunch of files, and those files tend to be pretty small anyway, it's just like, you know, a kilobyte worth of text. So it's not really a problem. One thing I didn't say so far yet, we'll get to in the second half of the lecture is how to store your data. You, by the way, of course, now probably encountered this issue, depending on which data sets you use the very different formats that makes sense. One design rule that people try, I tend to mention is that it's a good idea to use as open and simple data formats as possible and as advanced and efficient formats as necessary. So if you have a very big data set, then or very, then maybe it might be good to store it as a database, just because there's so much structure or not even access the whole thing, but just rely on it being on the web, like, you know, if you want to do something with Twitter, is there's no way for you to store it. On the other hand, if you have a data set that has you know, 200 lines of 200 rows, then and you know, I don't know, 20 columns, then a CSV file is actually a pretty smart idea, because you can just look at it, and it's so much easier to find problems. Also, it's not going to get outdated, right? It's not like 10 years from now, whichever database format you use is outdated and deprecated, and doesn't work anymore. Try to structure your code such that you can understand for yourself what goes where. And ideally, you have a process that that's actually not in the slide I should have put there, that naturally allows you to publish the bits that you want to have published in the end to the to the whole world and give your postal code for that on GitHub or on your personal get whatever get lab or get instance. Of course, it doesn't have to be any questions on this? Does anyone think it's like some of the ideas I propose were bad? Or we would like to do differently? In the past? I've had sometimes people suggest actually quite good changes so. Oh, yes. This is a very good point for everyone in the call. The question is, how do you separate the Jupyter Notebooks, which are easy to read, but very messy as a piece of code? From the source code that actually does the heavy lifting? And you're totally right. So they said, there's a sort of effective way to get cited every now and then the people so much right with with Git query that, I think, a shockingly large amount of code on GitHub in Jupyter notebooks and for like, 60% or so, and isn't actually executable, because the cells are out of order. Right? Because Jupiter, right, you can do that you can you can, you know, evaluate cells out of out of the temporal order that they're in. And sometimes that's necessary to make the code work. If that's the case, then it's very bad code. Right? So So A, of course, you have to make your Jupyter Notebooks actually relatively clean as well. But I think your question already kind of hints at the answer, right? There's this qualitatively different kinds of code, there is the kind of code that actually does the computation. And that goes into source right into into the, the, like, into Python scripts. So maybe No, maybe it's not even Python, right, maybe some low level language script that actually does the computation. And, you know, visualization, data analysis, pre analysis, just understanding what's going on that can go into a Jupyter Notebook. Now, sometimes there is a bit of a crossover, right? So you depending on what your project is, if you have a mathematical idea that you'd like to try out, then I would start with it, I personally would actually start with a Jupyter Notebook. If it's a bit pedestrian, if he's like, some, no, you're not a good computer scientist, if you're writing Jupyter notebooks that actually can really help understand things like to do little toy experiments, make a little plot, you understand that things work in a certain way. And then at some point, you feel like, okay, now I understand what I'm actually trying to do. And then I'll sit back down and write an actual piece of code that implements the algorithm. And before that, there were various instances of the same code, maybe in the Jupyter notebook that evolved over time. And now they've actually moved somewhere where they are, like, relatively stable. And there is never like the ideal point for that. Sometimes you're doing it too early, and then you have to keep fixing it later on. And that's actually work, it helps because it keeps track of how you change things. And sometimes you do it too late. And there's like 20 copies of the same piece of code in Jupyter notebooks, and at some point, you realize that's not a good thing to do anymore. So let me out. Okay. 20 is bad. But sometimes I've had, you know, eight copies of the same algorithm over time. It's not ideal, but also, those are the kinds of issues that you really can deal with. If you have to think about them eight years ago, eight years from now, it's going to be a little bit annoying then but you'll just go Like I came back then I did this like over and over and over again. But at least it's next to the code that makes the figure right. So I know that it's going to work. Separately, separate notebooks, for example, for every experiment, which tries its best as Watson experiment, but change your friends, the majority of the code remains the same. That robot over, tried to go to different files. Like to try to write down from inside. So, the question for people in the call list, let me see if I can paraphrase it is. At which point, is it? Like, what's the what's the mental step that triggers a new Jupyter? Notebook? Basically, right? So when, when it's an experiment? At which, at which point, is it better to create a new Jupyter Notebook? And respond? Is it better to just go through that cell again, and fix that one line that has an obvious pocket? And again, I think, yes, there is no clear, clear answer. But there is maybe a mental process, guided by what I showed you in the last few slides, basically, like guided by the goals of this process, like what do I what am I trying to achieve? I'm trying to document what I'm doing. I'm trying to make sure that when people come back later, they'll be able to follow what I did. And so that makes maybe helps to mentally in the concrete situation of writing the code, understand that sometimes there are issues that you can, you can just fix now, right? So the trivial thing is, you know, like, I don't know, you're making a plot and you're changing the changing the labels of the axis before any before you put the plot anywhere, right? Before any publication. Why would you? Why would you create a new this stupid, right? Let's look at the extreme case, or there's an obvious back, like the discord doesn't even run valid, it's gonna throw an error, of course, I'm going to fix it in the cell, right? It might also be that, you know, as you write it, you realize there's some smart way of doing it very efficiently to do just do right, right then and there, because you're excited about it yourself. But if you've done a step that has that you feel is influencing your thoughts for the future. So machine learning, one thing that often happens is that you do if you do deep learning research, that you have certain training procedures, and you take these, these weak decisions to do things in a certain way, use that one optimizer because it worked pretty well, once or, you know, you change the learning rate, because there was this one experiment where this really worked, then it might be a good idea to store somehow, why you do why you did that. And depending on what you do, it might be better to just create a new Jupiter file. Alright, so for example, one thing that really triggered me to make a new ship, it's like a while ago, I worked on a long time ago on linear solvers for linear systems. And I realized that there is a way that classic algorithms to conjugate gradient classic algorithm for linear algebra tends to have very nasty instability issues that people don't tend not to talk about too much in the literature, because it's like a very applied thing. And the mathematicians don't like these applied questions. And so I realized that my code kept failing, not because I did something wrong, but because the actual algorithm from the 50s just has this property. And then I made a whole thing for myself, like a little document just to just document how it goes wrong. Keep that somewhere and then write like, from now on, I'm going to do things in a certain way because of this. And that turned out to be very useful, actually. So it's actually documenting a problem, and that I've now sometimes sent this stuff to people. So like, this is this doesn't work this way. Yeah, so I think it should trigger to add a new piece of code, if you actually think it's worth keeping this around. For whatever reason worth because you want to document that you tried it, because you think you want to include it somewhere at some point, or because you think you yourself might question later on why you took a decision in a certain way. And you want to have it documented somewhere. Okay, so. Okay, those are that's the bit that I wanted to say about research code, where the qualitative thing just the second is that you you don't yet know what you're doing. That's what defines research. Basically, if you fully understand what you're trying to do, then you're moving into kind of a production mode, and then the documentation of changes. Okay, so now there were two questions. Let's first so, the question really, people in the call is about Open Science. Well, actually, the word open science is means a lot of things to different people. It might it might just means that the papers are available for download without paying a charge for it. And basically all research in all the certainly all the surgeon in AI and machine learning or good research is of that kind. So that kind of gold open, open access and have another way, another thing that people mean with open science is that they make the data available as much as possible. For methodological research in AI and machine learning, that's largely the case because people often use benchmarks that are available anyway. Research that actually hinges on the data. So you know, when people do research where the data is, the hole is the the asset, pose a much bigger challenge in this regard. Also, data that has privacy issues are a big challenge in this regard, they actually have an example of this later. So actually, it's very, maybe maybe your question is a very good segue into the next section, because there's a whole sub segment on how to deal with the data. And I guess maybe a compact answer to your question is, with code, I would recommend to be as open as possible from the time on that is public. So once it's published, at least as a preprint. With data, there are often considerations that you have to keep in mind. And the kind of data that my group tends to work with is easy in this regard. And then anything that has to do with people and their lives, and their privacy, or data that makes money for someone tends to be a problem. Ah, you wrote that in the Okay, that's you ha ha, ha, so how do you store? How do you store data data, not code when it's very large. So there are some, okay, so what actually doesn't what actually means large. So there's one, one way in which things can be large is that they are to that you're not a good idea to have as an as a explicit entry in your Git repo. So if you have a huge CSV file that keeps evolving, you obviously don't want that to be indicated for actually a PDF of your paper, you don't have to get it for either, because Git can do atomic edits to it. That so so one way to deal with this, first of all, stuff that can be produced from the code like PDFs of your paper, you just don't need to put anywhere, you could have them just procedurally generated, by the way, actually, if you want to look at look at that, that's maybe a fun thing to look at this, I can't show you, because it's private. But so there are actually GitHub actions that create PDFs for you automatically. So you can if you just Google for GitHub action, build Latech, you'll find them to just just like to add to your to your Git repo. So you can, this is an example of a paper where the PDF is actually here, you can just click on it, but it's not part of the Git repo. It's just created by GitHub, actually, when you you know, whenever you do a push as an automatic trigger action, actually. So there are some parts of your of your code that can be procedurally generated, and then you don't need to store them at all, then there are things like NFS, large file storage on GitHub, that are specifically designed for objects that are too large to put in a Git repo, but you want to have them somewhat publicly documented anyway, if they're a little bit larger, you could just have an n, they don't evolve as quickly, you can put them on your own cloud in some way on your own cloud or on your own cloud, and have them publicly accessible. That's already less ideal because those links tend to rot over time, right? You have them in your PDF of your paper. And then three years later, people can't find it anymore, because it's now outdated. I've recently had an email from someone about a paper that we published six, seven years ago, asking for a data set that we back then used as a benchmark from someone else, and we couldn't find it anymore online. And actually, it was annoyingly was not part of our back and forth with code back then. That was it was a Git repo, but it wasn't particularly well maintained, different time. And that's actually a problem. And then thankfully, one of the authors happened to have it lying around on a hard drive. And so it could be recreated. So, of course, there's kind of a trade off, right, as your data set gets larger at some point, you don't want to store it anymore, but then any way of making it accessible will also become less reliable. And if it's very important, then maybe you need hold structures for it. So at the extreme end, I think it's like Kieran leaving in a few minutes at the University Hospital is part of a national data infrastructure research data infrastructure. nazionale Forschung start an international tour for health care data that builds a very complicated distributed ledger so called Data train, where people can ask research questions, medical research questions about patient data in German university hospitals. And then there's that elegant process of collecting the necessary query. over a collection of databases, such that you can trace back who read which entry when at which point in time, for privacy reasons. Super elaborate. And you may have if you've been to the home clinic up on the way this direction, right? You may have seen a bunch of containers outside, like two storey 10 containers wide that's full of people working on this project. super complicated, right? So for your, for your individual projects for the next three weeks. It depends a lot on your data set. And I think as long as you think about how you want to do it, that's already good. So some of you will use datasets that are so large that you can provide them to us, then we'll be interested to see how you try and make this process transparent. So we'll link to somewhere through code that will collect it. Yeah. Okay. So with that, I want to get to the second part of the lecture, which is the fluffy part. So I've already put a fluffy figure image there about so you know, you're also here to learn how to be an AI engineer. And if you want to go out into the real world and make money afterwards with data, and one of the I hear I keep hearing from people who've graduated from my group and going on went on to work in industry that actually working with the data is the hardest part of the work afterwards. And that's the bit that they are least prepared for if they're coming from university, because it's so tough in practice. It's one of these instances where the people in the real world, quote, unquote, claim that the university is not preparing people properly for the kind of work that they might experience later on. But it's also a good example of why University tends not to be very good at that, because it involves processes that I, for example, are very rarely very rarely involved in, because they are happening in closed corporate shops somewhere. So it's the kind of it's also the kind of content is very unstructured that doesn't really have clear defined standards or procedures or best practices. And so what I'm going to tell you this is my big disclaimer at the beginning is very fluffy kind of business school stuff, with little stock images on the side, too. But I do think it's useful to think for you about the kind of challenges you would actually encounter, if you now get to graduate with your master's, you go out and work for whichever company you like. And your goal is your task is to build a product that involves data, and code, of course, what kind of challenges that you're going to encounter, because it might be useful to have to set the back of your mind as you encounter them. Partly because then you won't look as unprepared, maybe because you're going to be better at avoiding problems from early on. And actually, many of these problems are still just emerging, the industry doesn't know how to solve them yet either. So it might be good to kind of follow along. So I already set the most important part, the thing that actually sets these problems apart is that they involve data. I said at the beginning of the of the course, lecture number one, and said that a learning machine is a computer program that uses data to refine a model. So it involves three things, a model, which we talked about afterwards, a little bit in which you will learn about actually most of your lectures in an ML master involves a computer program. So code that sits on a hard drive or in some Git repo somewhere code that is written by a human software engineer. And that is the kind of thing that you know, computer science has evolved processes for that you can learn about software engineering, and then it involves data. And the data is the nasty new contribution to the process that complicates software engineering, when it has a central role. Because data is largely outside of the control of or led largely but it's sometimes more out of the control of the person writing the code than the code is. And that makes things harder. So what I'm going to tell you is stuff that comes from a paper published by three people forgot the first names of the first two people this is ni Lawrence as the senior author. It's on archive, I'm not sure it's actually been published since and I want to tell you the story of that paper, because it it helps understand where we are as a community. So this was I says on alias is not actually an alias I wanted to upload it forgot to upload it. And but you can find it on archive right. Knee Lawrence used to be a an academic, if we studied, did a PhD worked as a as a professor at University of Sheffield. And then he did what a lot of AI people did. He got a job, a corporate job because it seemed more exciting. So he went and worked for Amazon in the UK for for a while. A few years. He opened up a lab in Cambridge. And then I think three years ago, he went back into academia is now a professor in Cambridge of machine learning. And so I follow this is really the people I thought he was by PhD committee we know each other quite well. And he's a wonderful man. And I, so he went through this kind of experience process of first doing academic research, then going into industry, realizing, you know, all sorts of new challenges that he had to address trying to address the manga, getting annoyed with them and deciding to go back into academia. So I think he has a good position to be in to talk about the challenges people actually face in industry when working with with data. So this is a it's also a very Business School II type text that could be somewhere in like a magazine industry magazine rather than a journal. And it outlines four high level categories that actually design a process for how to deal with data and lots of challenges listed along the way. And those four steps are pretty obvious, as most business school stuff is, once you've written it down, like so how do you write a product that involves data? Well, you first have to get data, they call that data management. Then you have to, once you have the data somewhere sitting safely on your hard drive or in your database, you or in your whatever the server that provides the data to you, you then want to do some big machine learning on it big old machine learning, like you know, train a model, build a model that does something cool, then you want to make sure it actually does what it's supposed to do. That's called model verification. And then you want to build a product, you deploy it out into the real world and make money with it, ideally. And all of those have challenges. Even if you just write down those four steps, mentally, you can probably already guess what a few of them are going to be. And they are pretty, you know, maybe straightforward. But the moment you start thinking about them, you realize that they don't have always easy answers. So let's start with data management, and have one slide for each of these higher categories. First thing is to collect data. So one key challenge with data is that for most businesses, at least ones that aren't newly emerging as data centric businesses, the data is not the defining object around which the infrastructure is built. Instead, companies up until quite recently, were often designed by some sort of one of the design schools was for software engineering, was to build micro services like the Unix idea. So you have individual servers, or machines that do certain things for you. Whichever they might be, right. And they are deployed and elsewhere, all over the all over the world, maybe they're even globally diverse. And they do well, various different things. And they all have their little data that they either manage or collect, typically, as a side effect. So often data actually gets collected as a log file somewhere. Now, some at some point, management decides that they think this data is so valuable, you should be doing something cool with it. Now your main challenge as an engineer, May 1 be actually to get to it. One of my previous students who now works in industry tells me that she spends 80% of her working time as a machine learning engineer, just getting the data and preparing it. And then the last 20 minutes, 20% minutes actually doing machine learning. Then once so first, you have to figure out what the data is you have to talk call, a lot of people understand what they do figure out that there is data there, get a process for them to send it to you. And then you have to parse it. And then often data comes in, in nasty formats. It might be stored, you know, in nasty formats, like log files, it might also be stored in as Excel files. Excel is super popular in industry, much more than many of you might think if you once you leave university, it will be just everywhere. It was shocking the large files as well, the two super complicated stuff that you would really think should be a script. But no, it's some visual basic in an Excel file. Pick CSV files, scan PDFs, SQL databases and all over the place. I want to tell you a story. I was once in a in a preliminary call with someone from the University Hospital to do for a project for a project that we have eyeballing. I think I can tell you it was it was with the the university's women's hospital for a project on detecting complicated births. So when a when an expectant mother comes into the hospital, they do this taco Graham like this measurement of muscle contractions on the in the belly and heart rate of the fetus. And they wanted to predict whether the birth would become complicated later on from features of those this data. And she was super excited about this was one of the chief doctors of that of the hospital. She was very excited about it because she had collected a lot of data she said, and from 1000s of patients over the last few years in the tubing and University Hospital. And so that sounds really exciting. And these are time series I think we have methods for this sounds like a cool project. So how much do you have what kind of data is this like and then it transpired and it had to ask several times To understand that the data consisted of scans of the printed out ink that comes from the machine. So there was the as as, as the mother sit there waiting to give birth, I went through this twice with my wife as well, they actually hooked up to this machine. And there's very slowly a piece of paper comes crawling out. And it's like this wide, it looks like a like a bill from from a cash receipt, right. And they just collected all of those, they put them in binders, and they put them through a scanner, and now they're available as PDF files. That's a very typical situation. Actually, I've had several conversations like this with people in industry in science. And I think this is wonderful, all this data, but just imagine how much time it takes. Right? So I realized that if you would do this project, you would have to first have to get a PhD student exclusively to get the data into a machine readable format. And that's a really boring project that nobody would like to do. So we shelved it, we didn't do it ever. And it's actually a super exciting project that could, you know, save lives. But it's just impossible to do. Because Think for yourself, right? How much work it would be to get a line out of a blurry scanned PDF. This happens all the time. In industrial settings, you're calling someone up, they say I have the data, we have all this stuff, it's all stored. And actually, the process of getting it into manageable, like numerical format, you can actually work with takes forever, or it might be might actually make a project completely infeasible. You also need to have labels at some point. So once you've if you actually want to train a machine learning algorithm, you want to know what you're trying to do. And so often data in practice is not labeled. That's why there's a large part of research on semi supervised machine learning, finding structure in datasets without getting labels, and then just getting a few instances labeled by whoever is the expert, and getting them right. Also, the people who make the labels are not right, they are often wrong as well. One typical example of this is medical doctors, they all completely disagree. And evaluations is a typical thing, when people build medical software is that you have one doctor labeling the data. And then if you have a second point, it turns out they have differing opinions. And that's actually this group that I mentioned before, but Winfrey thanks scope where I did my master's thesis, we I wasn't involved in that actually, I did something else in that project. But during the time I was there, they were building a large piece of software to create training data for a neural network that would label volumetric electron microscopic images as intracellular, or extracellular regions, on a resolution of a few nanometers. And so they had experts, once neurologists actually just one to get some reference data. And then a bunch of TVs, students sitting in front of wakeup tablets with electro electronic pens, labeling the outlines of cells, volumetrics, that's huge. That's like 100 TVs in the end doing that forever. And they actually actually design a very careful process. I'm not joking, it's actually over 100 people over the years, it wasn't like immediately, they had to buy lots of hardware as well and get money for it. But now it's a wonderful project. So the cover of science a while ago, they actually had to carefully design a process for how to overlap the training regions. So that every expert will know every human labeling volume would have the same volume would also be labeled by another human or at least a certain part of it, so that they could compare their their, their quality to each other and get an average result with uncertainty estimate that can be used to validate whether the neural network that is trained from it actually works in yet. So you need to have these kinds of processes. Sometimes you can even create strain data, right? So if you've been to the lectures on autonomous driving by Andrew SkyGo, you may know that you know, autonomous driving, that's not a big, big deal about generating training data in a simulated fashion for self driving cars to create weird situations. Basically, like a like a pilot trains for emergencies and an airplane you have to kind of train autonomous cars by showing them situations that might arise. And then at the end, well not maybe not at the end, but at the entire time, you have to check whether the what you think about the data actually makes sense. I'm not going to say much about that, because we had lectures about it. As we spent some time over the past few weeks and months, talking about how you turn a data set that is like a CSV file that may look really ugly at first on screen into something you can analyze and understand and how, you know, you understand what kind of structure it might have. Some things I didn't maybe mentioned so prominently is that they are also often kind of bugs in data, right? They might be omissions of things they might be cut offs in labels. I used to have an example of this. This Mona Lisa co2 curve, it used to have a, the rule is that if the if the sensor doesn't work that day, it stores a number of 19 and minus 999 parts per million co2. So it stores a numerical value, it doesn't store none. And if you use a standard pandas procedure to remove all the Nan's and those non stone, those things that actually announced they don't get removed, if you're not compute averages, they're all skewed by those minus 999th, that gets added into your running average. And if you might not notice that if you don't make good plots of the data, if you don't look at it, maybe you have y axis cut offs at zero, because you know that parts per million should be positive. And so you never get to see those data points that are missing. This kind of issue is, you know, it solved by looking at your data really carefully. Then you have to train your deck, now you have data, now you want to train a machine learning algorithm for it. And this is where, okay, I'm just going to open up the whole thing. This is where your all of your machine learning expertise actually comes to fruition. So building a good machine learning solution to a new problem is where the expert actually shines. And I'm only going to show you this slides. But of course, basically your two year master here, if you're doing a machine learning Master is all about this. So one of the key takeaways is something I've already mentioned several times in in the in the lectures is that a simple algorithm isn't necessarily a bad one. One thing that you may have an idea that you may get from some from some lecture courses is that the best solution is always the most complicated one the most recent transformer architecture, with like, you know, 20 different layers and some complicated nonlinearities and Monte Carlo dropout and batch norm for training. But actually, it's often pays off to start off with a simple model. In this paper, they have an example of I forgot which company it is, I think it's from Airbnb, where they basically did kind of this silly story that you could imagine that they started out trying to train a deep neural network, they could never quite get it to work to predict something, whatever. And then at the end, they had to like they slowly slowly decreased and make make the make the model ever, ever simpler until they actually had something that worked. That turned out to be some really original paper. But I think it's something like a two layered, very narrow, deep nets that basically had like 120 degrees of freedom. And then they could get things to work and slowly started increasing them. Again, we had some examples of this over the course of this lecture as well, this lecture course of how PCA might be easier to use than T Snee or linear discriminant analysis might be a better approach at first than logistic regression. Because it's so easy to understand and so easy to debug. So if you encounter this problem in the wild that you have a manager that who thinks you're you're silly, because you're trying to do simple algorithms, first, it's a good idea to push back. Again, a little story from from experience, I once was on a on one of the evaluators for a master thesis at a large German car company industrial master thesis, where that the goal was for some production process with a welding robot, they want to predict whether the wealth was good or bad. And I got a thesis to read. And it was all, you know, convolutional neural networks with 20 layers. And when I asked in the presentation, where the industry advisor was there as well, I said, Why didn't you try something really simple? Did you did you try logistic regression Did you can't this can't be so hard. And then the advisor stepped in for the company, and actually said and and and and we had a master thesis about this a year ago. And they've already done this. And he actually worked quite well, we just wanted to see what the deep learning could could do. And so this student wasn't allowed to do to try this. I told him not to do it, because they've already done it. But he never got to compare to the original algorithm, you know. So if that's actually what happens, if you have a manager who says no, I want to speak fancy deep learning solution, then maybe I don't know, in your in your way, when he's not looking, try and try and use a simple algorithm first, and maybe keep it around at least as a benchmark, right. And then if you can show that your complicated algorithm works better, wonderful. But if it doesn't, then use the simple thing, because it's easy to implement. It's easy to test. It's cheap, it runs on easy hardware, you can run it on the edge, and so on and so on. And then, you know, there are all these problems that come with large scale machine learning how to hyper parameter tune and all this nasty stuff that this course is not for, but that you have to think about as you proceed in your education. Now comes interesting stuff that actually happens after you've trained your deep neural network, or run your PCA, or whatever you whatever you're trying to do, which is that you have to actually convince yourself that you're doing the right thing. And here academic research again, maybe sets bad bad precedents. So we're used to showing some learning curves Whereas some, you know, maybe not even trading loss, but test loss test accuracy is plotted over time. And then it's like one line is above the other, you can publish your paper, right. But in industry, the KPIs, the key performance indicators tend to be quite different. And you tend to be whether you're actually able to do what the consumer of the product wants, maybe it's the end consumer, and you want to know whether it's a product or not, maybe it's just your colleagues who want to do something with it, and they actually need it for a slightly different functionality than the one you've been training on. And it might be hard to, you know, like, actually address those. And then sometimes you have to change the output of the algorithm or think about the interface that it provides. So that it actually allows people like actually gets people excited. And this is not just for data centric code, of course, it's the case for basically all software development. But with, with data, maybe the challenges are a little bit exacerbated by the fact that there is some part of your, of your pipeline that you can't change directly. And it's also very hard to see which of the two things I actually wanted to say about this, it can be very hard to formally verify that your algorithm works. Why again, because there is data involved. If you have going if you went to a software engineering class, you've learned about how to build a unit tests. And unit tests are, you know, always typically defined in a non stochastic fashion. And like very concrete, clear interfaces, like if I put this in, then the code has to produce this. And it's very hard to do if your code involves a data set that you can choose yourself. And that may even change over time. Or if your code does something like AI is supposed to do that is very complicated. It's very soft, that reacts that, like reacts in an intelligent way to complicated inputs. Because then it might even be hard to list the kind of behavior you want. So the famous example is self driving cars, right? How can you be sure that a car will always do the right thing? If it's self driving, so for planes, actually, engineers have come up with various ways of doing this. These are called, you know, reliable software engineering, formal formal verification of software. They're basically two different ways of addressing formal verifications of software classically used in, you know, to build auto pilots, for example, one is complete enumeration more or less, so you make sure that you basically you think or consider every possible path the code could take. And there are various ways of doing that, right. One might be that you constrain the programming language to only allow certain things, maybe you have design principles that only like ensure that certain principles are used, maybe you literally enumerate every possible state, the algorithm could be in and check that they are all safe. Or you do some mathematical analysis. So for example, for autopilot autopilots control systems controlling and control engineering systems, a typical thing that control engineers store is that they linearize, the system that you can construct a linear controller, or they even assume the system is linear. And then you can do linear algebra, use fancy tricks from dynamical systems theory to show that this linear dynamical system can never go beyond certain safe domains. So what this requires fundamentally, all of these approaches is some constraint of how powerful the algorithm is you're allowing yourself to do, I mean, you will know this, because you've taken probably some theory theory of computer science course at some point. And you know, that if you make your system sufficiently powerful, you can't show anything about it anymore, right halting problem, and so on. So AI sits at a very awkward point in this, in that we want systems that are very powerful, and that can actually do complicated things like driving a car. And we therefore have to make them quite flexible, like using deep neural networks, rather than linear classifiers. And then fundamentally, those mathematical tools fly out the window. You can't enumerate all the states that deep neural network can be in. And if you do a linear analysis of the neural network that actually can help. And to some degree, we do it. I mean, I showed you this Laplace library on future slides, which is maybe a version of this, but there are corner cases where you move sufficiently far from the data, and then things become very difficult to predict. So getting processes to check that are actually pretty complicated. Do I mention real world data is this actually on here? Maybe comes on a later slide. Okay, I'll show it chosen on the next slide. So because it's kind of overlaps into what happens in practice, right? So once you have actually written your code, and usually if you've pushed it somewhere out maybe this Let me see. Okay, so there are even I mean, they are even former ways of showing that neural nets can have very, very nasty properties. One paper that you may have seen before may see in the later lectures in probabilistic machine learning maybe, is something that my colleague Professor hind it a while ago, at CVPR was to show that if you take a riilu deep neural network classification network with a soft max output, if you move sufficiently far away from the training data in the input regime, right, a deep network is a map from input to output with some weights, and is trained on some inputs and label outputs. If you now take any of the training inputs, and any other of the training inputs, now you have a line between them, and then you move along that far away from the data set. If you are sufficiently far away from the data, then a classic riilu point estimation, a net with a with a softmax, output will, is guaranteed to become arbitrarily confident on one class. So it will predict any random class with 100% confidence if you just sufficiently far away from the data. So if you encounter as if your self driving car that you've built on such a network encounters a situation in the wild, that looks sufficiently different from the training data you've provided to it, it's going to predict to do something with 100% confidence, even though it should really not predict anything, because it's arbitrarily far away from the training data. That's where a lot of this adversarial robustness kind of questions come from. So in practice, and this is where we get into model deployment, this is a veal very, you may have about a product that you may deploy in the wild that interacts with data. Because it might see data or someone might maliciously show it data where it just doesn't work anymore. And then you need to have procedures for detecting this. And I think I actually have it on this slide. So let me go forward. So this is the ultimate thing. What do you do, then once you've actually like finished your your product. In software engineering, again, there are established procedures for how to get software out into the wild. There's continuous integration, continuous deployment, that like procedures for how to add functionality to a piece of code that make sure that things keep working, and that they are efficiently passing into code. And you know, that the whole design process is kind of safe. So with data, those processes become much harder, because the data set may evolve in a way that is difficult to formally track. If you're changing code, then you could say, you know, you can do a pull request and the key people, you can say, I'm changing those two lines of code. And then you can look at them and someone can say, Are these going to be difficult or not. And you could have an automated process that says this code will I mean, you've seen those before, right? This code will affect the output of this and this, you can run a few test suites, and then you get some red lines, some green lines, okay, you have to go in and fix this before, before you're allowed to push this into the code. But if your code relies on data, then the data set may change in ways that you've anticipated, because you've active, you're actively trying to update your your model. But that might still have an effect on your machine learning algorithm that you don't fully expect. Or it may change in ways that you haven't anticipated. Because someone just changes the data set in some way that you haven't thought about. So another example. So I've tried to come up with examples from my own experience, as you can tell, they're not perfect, but I have a few in. And maybe I should have mentioned this example, previously, because it's kind of cross cutting across these issues. One of the things I've learned recently that one of the key data management problems in public administration in Germany, is that's currently creates a lot of busts in the super exciting spheres of bureaucracy in Germany is what's called the biggest act, what is your room? So the renewal of databases that you that are used by the public sector, so there are lots of registers in Germany, I think there's like 24 registers, one of them, you may know is the car futsal cluster. So it's a list of all the cars with their license plates. And I guess it's stored in Flensburg. I don't know. Maybe along with the register of people holding holding driver's licenses. I find that fun. I don't actually know anything about that about that register. I think it's a fun mental exercise to think about why updating such databases is a problem. So how many cars are there in Germany? Let's say 40 million, right? Maybe it's a little bit less a little bit more doesn't matter. Right? How much do you store about each car well, You know, maybe it's 1000 characters, I don't know. So the whole thing, the whole database is on the order of a gigabyte in size. In principle, right, in our heads, if you think about what kind of amount of data disease, it's the kind of stuff, you could actually stored as a CSV file, if you wanted to write, you could, you could load it into RAM. If you if you use that, if you could download it somewhere as a text file, it would be a gigabyte, you can download it over the internet, put it in your on your hard drive, it's on the size, where it's just about so large, that you may notice that you want to dump it afterwards. You don't want to keep it for around forever. But it's not like an unimaginably large, complicated object, right? It's something you could load with pandas. And you don't even need an SQL database for it. And like, work on it, it's really not that much. But if you think about it, what so why do we have such a complicated administrative interface for it, but there's like an entire entity in charge of maintaining it. And they have some procedures, and it's very complicated in their forums to put things in and out and change stuff. Why is that necessary? Well, because there are multiple consumers of that data out there. Maybe there are, you know, police agencies all over Germany who want to have access to that database. Maybe there are, you know, companies that need access for it for some regulatory reason, maybe, you know, maybe the car makers need access to it, I don't know, right. And you want to change anything about that database, then everyone, whoever has access to it needs to know about it. So this is an issue with general datasets that are not at the core of the design process of how a company works, or a public institution. Actually, if you want to have some maybe actually, this car photography is an example of a data centric object where you first make the database, and then you tell people, the people how to interact with it. But in companies, often, the data sets come from somewhere, and they just are produced as a side effect. And if you use them to train a machine learning model on it, you have to make sure that no one gets to change that data in a way that breaks your system. This is sometimes called the problem of an unknown consumer. So you have someone who trains a deep neural network on some log file of some server somewhere and then the admin of that server decides to change something and everything breaks down the line in a way that you've never expected. This is also why many companies or some companies now spend a lot of money on building this what is called a data lake. As opposed to a data center. So having separated entities, one where people get to experiment with data and look up what kind of data sources there are. And then the data, let's call the data lake, right, a big collection of data, that is just someone can look at, and then having a formal process of moving stuff from the lake into the center, where it gets stored in an immutable way that you can't change anymore. So imagine some of those changes can actually be very subtle. Now, of course, right? If the Catholics or Buddhism decides to add one more column to their CSV file, you can imagine that they are gonna have a process for that, right? If you decided to store as a note the color of the car, and previously they haven't started, of course, someone will think about how to make a process for them. But imagine if the Lambert's empty when Tubingen decides to change the way like because some some new person gets hired, the old one retires, someone new gets hired, and the way they enter one string changes, they used to make a comma between tubing and something else, right, and they don't make the comma anymore. Or they they tend they used to write halt, comma Vuitton bag, and it forgot about the bag. And then suddenly, there's three different hops in Germany that all look the same in the database. These are these kinds of issues that just arise in a way that are very difficult to check for. Imagine what kind of doing a test you have to write to detect these kinds of issues. If you want to know more about this, this is the point where I can advertise for someone, then I'll be done. I should maybe say this, there's actually I'm not sure he's teaching electronics next year. Have you heard of Professor cuz he is here an honorary professor here at the at our CS department doing, he also happens to be the CTO of the Schufa. You can imagine that he may have big problems with databases all the time. chufa, of course, is a complicated company that has a complicated public perception. And he's teaching as part of his honorary professorship, his teaching lecture courses here, at least I think, once a year, maybe even at the term on often on how to work with relational databases with probabilistic inference from complicated databases, fair decision making on from from data, I encourage you to actually go to one of those lectures if you want to hear more about those kinds of issues. That doesn't mean that you have to drink all the Kool Aid kool aid, but you can see I think, once again, she's very happy to answer probing questions, and may actually be good to, you know, learn some interesting insights from someone who has to deal with these legal ramifications or So, basically in their daily lives, okay, I'll leave out the final slide because you can look at it yourself. But you can obviously, just to point out, you also have to think about ethical issues. While you're working with data I've had, we had an entire lecture on this, I'm not going to talk about it, you also have to think about security, they have fun new ways on in which machine learning models create new security issues. Just to point out a few, you may have seen many of them on social media, because they are kind of tame, they make nice social media fodder. So one example is that people can steal data through AI interfaces, in ways that are not previously thought of. So one example is this, this, this paper came out last two years ago, where they managed to extract training data from I think that was GPT, two or so. So a large natural language model, where you basically you give it the beginning, have an address of someone, and then it returns the full private address, with email, with email, and everything and the phone number of someone just to complete the text. Yeah, you can imagine that there are all sorts of issues like that, if you just if you give someone access to a database, then they might do all sorts of nasty stuff with it. And actually, there's a corresponding other issue the other way around that because of this, people can become very cautious in giving away their data or giving access to their data. And that creates kind of loss of opportunity to do interesting things. I would like to actually argued in front of the members of the Bundestag, ones that our careful over focus on privacy rights in medicine, is harmful to the rights of individual personal bodily integrity, because there's so much knowledge stored in patient databases across Germany, that could solve so many healthcare problems. But we just don't allow anyone to look at the data because of these privacy issues. So this is a problem that we that that is going to be very important over the coming decades, because legal regulation is now becoming more and more prominent, and the take, finding these careful offsets between security and safety and privacy, but also actually doing stuff is going to be very interesting to watch or to do. And, okay, finally, it's important to actually build management structures for this. If you're entering a company at some later point in your career, you may encounter people that are called data analysts, or that have data expert roles people who are employed. Some companies, I think this is a very smart idea, and I'll do is to translate between, like the boffins and the managers between the people who know the tech and the people who take the decisions, so that data can actually become a central part of the decision making process. Okay, so some high level guidelines for how to like how to deal with this are summarized here on here, this just actually taken more or less directly from this paper that I based this this part of the lecture on. One idea is to build a data centric architecture. So to start the design process with data that only works. If you build it from scratch, it's very hard to do in a running system. For USD engineer, it's to start simple, build simple machine learning solutions. First, you may have to do that anyway. Because you don't have much time if you spend 80% of your time getting the data for the company to hire good people like you to do the to do the work, and have people who are specifically working to build these connections between the code and the people working with it. Okay, with that, I'm at the end, I'm not going to summarize again, because it's basically similar to what I just said, as always, here is the link to the feedback, please scan and provide feedback. This is obviously a very different lecture from last week. So I'm curious to see your feedback. One thing I wanted to point out is at the bottom of the slides, next time, I'm going to be teaching the aforementioned theory of computer science, the people try the German lecture for the Bachelor courses. So I'm looking for tutors for that for that class. I already have a few, but I have just over half as many as I need. So if you're looking for a heavy job for next term, and you want to relive your undergraduate years of thinking about you know, tuning machines and finite automata and complexity classes, then and you speak fluent German because the course is in German, then please get in touch. You can just send me an email. Thank you very much All right. Thanks everyone online as well. Goodbye. 

