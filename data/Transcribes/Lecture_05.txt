Hello everyone in the call. Let me check my microphone. Anyone can hear me let me know thank you very much. So it'll be a bit silent in here for a bit until I can get Started. Okay, so let's get started. Let me see if this works. huh? Uh huh. So, today there is an About 25, all of you in the in the room and there are 50 in the call, I guess it's still going to be a little bit more. So sorry, we only slept slightly delayed because of 3g testing outside. The I just heard that the lecture that happens before us is going to move into purely Virtual mode. So for next week on, there's going to be more time for us to come in, we'll be here half an hour before the lecture starts. And you can come in and use the room, in case you're wondering, as the numbers are rising, My policy is going to be that I'm not going to proactively turn this into a virtual lecture, I'm going to keep being here, until someone tells me I'm not allowed to do this anymore. And then you may have to be virtual, I hope that at the moment, the political will environment and back of Minister Teresa Bauer is to keep lecture halls open. And and I mean, regardless, there's always the virtual options. So for those of you who don't want to come into the lecture hall, you're always invited to just join the call, I hope everyone in the call can hear me didn't really check again, after switching on the mic. So maybe give me some positive feedback by telling me in the chat that you can give me this, thank you very much. Okay, then I'll put the chat here. And hopefully, I'll see what people are typing. So today, first of all, of course, as always, we start with feedback. And as always, my pointer is not working. Hmm, there we go. So last week's lecture was again, like, at least by my standards, this is relatively good scores, overall, evaluated by positively, although there are a few people now an increasing number of people who thought it was a little bit too hard. So I'll try and consider that. The good news is that this week's lecture, at least for my matches I'm doing is probably the last one that is relatively focused on mathematical issues, then the next two weeks will be lectures by Professor Macker. On regression, we'll see how he does that I don't actually know his slides yet. And then after that, the lecture will begin to become more and more practical. So if you're very that this is too much math. From January onwards, things will be different actually, from even before Christmas, is some detailed, detailed feedback. So how do I do this in most easily so to her? Okay, so people like the examples, and the code will do more. One more example, one more code today. Some people said that they're struggling with the math, basically, actually, there were several of these of these kinds of answers. And so all I can say is that there's a we do need some math to do quantitative reasoning. And if there are specific concepts, if there's, if I use a word that you don't understand, maybe just tell me you're in the room, just raise your hand. Or if you're in the chat, just type it out. I know that this is hard to do, it feels a bit awkward. But it's, it's not a problem, right? I can just typically, I won't be able to go through long, long details, I can just, you know, wave my hands about it. Say this is the object we were thinking about. If you want to know the big details, you should consider attending the math or machine learning class by appointment. And, yeah, so someone said I should use examples with numbers, I thought I was using numbers. I'll do that today again, and then we'll see, well, maybe that's still too abstract, we'll see. The other thing I should tell you is that this is the time of the year. This happens every year about this time, a few weeks into the lecture hall into the lecture course, when the department does the external evaluation of the lecture. So the one that isn't run by me by collecting things through ETS, but by an external website, and for that, I have to send a list of emails to the people running this evaluation. And I need want to collect that list of emails. Now, what I've done in the past is that I take everyone who signed up on Alias, take their email address and send it to them. The problem with this is that we have 250 people signed up on Alias. And they are only as you can see about 90 people participating in the lecture. So that means more than half of the email addresses I sent to these people are never going to get a reply. And then I get an email saying, Oh, you have such a bad response rate is only 30% 40% of your of your participants so far, if done the poll, and then I have to keep telling them that Yeah, but that's because that's the people who are actually in the lecture hall. And everyone else just signed up initially, and now didn't show up anymore. So please, if you want to participate in the evaluation, hold your phone to this URL, or you can just go to ETS, there is a there is a group called evaluation participation where you can just you enter that group and all that means is that I get your email address and I'll send it to the evaluation people by the end of this week. Okay, so that's the admin done. Now, what we want to do today, so this is part three of a three lectures sequence on inference estimation, confidence, and now testing. So what do you do with your data, once you have collected it? Were the first two lectures were about collecting data, either randomly to have no bias, or maximally guided by information to learn as much as possible. And then we talk about how to do inference. And the answer to that is always ration inference Very good. Now, if you can't do Bayesian inference, or if that your task requires you to make a statement, that will guess of what the correct value is, then you compute some estimators. For example, through maximum likelihood, we saw that maximum likelihood has reasonably good properties, it's asymptotically. Consistent, it's asymptotically, distributed like a Gaussian random variable. But it can sometimes still be bad. You saw some examples on your on your homework, particularly if the likelihood has an odd shape if it's not differentiable, if it's very astrometric. And then we spoke about confidence. So how to add error bars to your prediction to your estimate. We do that by basically looking at the likelihood and finding analytic ways of talking about it that aren't just here's a picture. And for example, you could compute the second derivative of the likelihood or the log likelihood at the mode at the point where you construct your estimate, and use that shape this curvature, how spread out the distribution is to construct arrestance. What I want to do today is a final piece of technology that statisticians use a lot, which is a bit different from what we've done so far. So I would argue that estimation, and confidence are pretty much universal concepts. If you do inference, if you are doing anything with data that doesn't involve literally observing the thing you're looking for, then you have to do some inference and some confidence estimation. And there's a final third bit called testing, which is often presented as just as important, but actually, arguably, is used for a very, very specific, maybe niche application, which is to answer binary questions about whether a hypothesis that should be true, if there is no signal, how likely the observation is that is. So that's the kind of answer to questions like, Do vaccines work? Or are humans affecting the climate? And you can see that it might you've seen these questions a lot in the know all the media, social and public. And you kind of already know that the answer to this question is a bit silly when the answer is yes, in both cases, but yes, doesn't cut it. It's not enough, right? It's more complicated. And how much does it work? And how are we affecting the climate? That those are the actual interesting questions, but testing isn't about those testing is just about excluding the base answer that there is nothing to talk about. And we'll see today a little bit more detail what I mean by that actually, do I have an example now? Yeah, so maybe this is a fun way, I don't really know what the right order is have to do things. And here is an example that I just cooked up like two days ago or on Friday, that is really just supposed to show very, very weakly what we're going to be talking about. So imagine that you're measuring some quantity, I don't know the average temperature per in like 20 different cities in the world. Over overtime, you have some data from 100 years ago, and you have some data from the last five years. And you're the base assumption, if let's let's consider the question, Are humans affecting the climate, or actually, we should be a bit a bit more careful, maybe phrase it as has the climate changed in the past 100 years. And so far, this none of this, the plots you see here in terms of data actually have anything to do with this, right? This is not based on real data, I'm just trying to construct a situation where you can think about what those numbers might be. Now, imagine that your hypothesis is in the like, 100 years ago, that was the average temperature was something that we subtract so that the center is at zero, right? And it had some spread plus minus whatever, let's call it three degrees, right? And now we're observing 100 years later, some actual measurement. So you do one more experiment, and you get a number out. Not so then this you the hypothesis, this is called the null hypothesis. The hypothesis if nothing has changed, is that the data we now get to see our draw from the same distribution? So let's say this is your observation, read minus four. Well, actually, if you talk about climate, it's probably more like plus four, but whatever, right? Let's say that's the number of you've observed. And now the question was Try to answer is Is this okay? Like, is it possible to explain this observation with this base hypothesis? Because if it is, then maybe we shouldn't be concerned, right? Maybe there is nothing to worry about. Because there's lots of things we could worry about. Why worry about something that might well just be a random chance? And you know that there are some people who argue about this, like for climate change. So the problem here that now that arises is, how do we do that? Right? So what we would like to say something like, well, this is an unlikely observation, well, maybe it is likely maybe it's unlikely, I don't know. Right? So the first thing, you might you might think, as a as a possible, like way to solve this problem is to say, let's look at this blue curve, the probability density function of this, this explanation for the data and check whether it has a low value, right, if that number here, if the curve here has a very low value, then what we observe is very unlikely to happen. And therefore maybe we should be concerned, it's not well explained by the data. So this would, for example, happen. I mean, this is arguably here not a low value, well, maybe it is, I don't know, it's just about a low value. But like, what we're probably looking for, is something like like this, right? If observed something that is exceedingly unlikely, under the original explanation. And then maybe we need another explanation, because this is not a good explanation of what's happened. Now, the problem was just looking at this blue curve is, there's other ways of making that number very small, it could, for example, be that our hypothesis, the original explanation for data, it's just everything is so extremely broad, right, we really just don't know anything, we could explain everything with this with this hypothesis. And then that number is going to be lower as well, right? Because it, it's a probability density function. So it distributes truth over the entire range. And so if you make the range very wide, the number will be very low. But under this kind of observation, right, we are very much at the center of the distribution, very well explained in some sense, because we are totally in the region that we would expect. So this is a problem that arises in nothing continuous spaces, actually. But whenever the space is large enough, whenever the hypothesis is sufficiently general, or the space that we have to predict, and it's very, very broad, very large, high dimensional, then we'll get this kind of situation. So to fix this a one way, or the way that statisticians usually do this, is that you don't consider just the value of this density at this point. But you compute the probability for an entire region of the event space. So for example, you integrate all the way to the left, like to minus infinity, and say, the total mass of something of something happening, like the event we saw, or something even more extreme, something further to the left, that is a number. In this case, that number is roughly 10%, right, nine times 10 to the minus two. So in this case, the probability of observing what we saw and something more extreme is 10% 10% is probably okay, right? It's like one in 10. It's not particularly surprising. But if the distribution is narrower, then now it's 2%. And now it's, you know, tiny, three times 10 to the minus five. So that's 0.003%. And it would be very surprising to see something like this. Actually, it would be surprising to see something like this, if you only did one experiment. Of course, if we did 10, to the five experiments, we would expect to see three events like this, right? This business is exactly what we're going to be talking about for the rest of today. And I'll ignore the orange bit for a moment, we'll get to that in a second. So this is the idea of testing. In terms of pictures, and now there will be four slides that are pretty math heavy. And I'll try and like tell you what the formal definition of a test is. There'll be one slide in particular, advanced warning, that will be a bit tricky. We'll go through that slowly. And then we'll do a big example to show how this actually works in practice. So the I just kind of explained this already, I'll just read it out again, so that we can clear up this the language a little bit. The problem we're trying to solve is that if you if you make an observation, you would like to test for yourself whether the base explanation, the boring one explains this data? Well, actually, ideally, what you really want to find is the correct explanation for the data, right? That's a much more challenging problem, because there are often infinitely many ways to explain an observation. If the climate has changed over the past 100 years, why is that? Well, there's many possible explanations, and it takes much, much more work than to just look at one Time series, it's our this is this, this is like what I've seen 100 years later, it's different from 100 years ago to figure out what actually the explanation is. So to make our lives easy, a simple thing we might do is to just consider whether a base hypothesis, nothing has changed, everything is as expected, can be ruled out under the data. So the way to do this is not to look at predictive probabilities for an individual observation. Why? Because if there are many possible explanations, then any individual observation is going to be unlikely. So we instead have to construct a region along it that we consider surprising and see how far out in that surprising region we actually are. And constructing such a such a procedure was called a test. So a test for the, for the hypothesis, we don't really believe in any way, but which would be the base explanation for for the observation. So why is this even a problem? Let me let me tell you, so in previous two lectures, my statement was always the right thing to do is just Bayesian inference. So actually, there is a Bayesian inference answer to this problem as well. It's just that and annoyingly while in the last two lectures, the Bayesian answer was actually tractable, if you put a little bit of your mind to it this time, unfortunately, it isn't. And, well, I'll tell you why that is a problem in the moment. So here is how you would test a hypothesis correctly in the Bayesian sense, you compute what's called the evidence. So that's a marginal over the observation, you've you've collected the data under the hypothesis. So what that means is the hypothesis has is actually a distribution over those over some unknown variables. Like, I don't know the distribution of, of temperature across the planet, or you're talking about vaccines, it's like some binomial distribution over outcomes, given that there is some probability to be infected. And we integrate out all the variables we don't know, and get a marginal distribution over the observation x, given some hypothesis, H zero. This, by the way, is the term that we've already used a few times so far, in the denominator of Bayes theorem, the evidence for the hypothesis. Why do you do Bayesian inference on theta you do prior for theta times likelihood for x given theta divided by this object? So often, we can actually compute this, or at least you can approximate it. But now what we have here, if you look, if you look at this object, if you just think about what it is, right, it's a probability for the data given the hypothesis. So that's a likelihood. It's another it's a type two likelihood, a marginal likelihood, a meter likelihood, or a hierarchical hyper likelihood for the hypothesis. So it's a distribution for the data given the hypothesis. So we could use Bayes theorem again and do Bayesian inference on this hypothesis by multiplying the prior Okay, priors on what's the prior for hypotheses? Well, we've learned so far, that priors are usually not actually the problem that's to do Bayesian inference will have to divide by the evidence. And that requires us to write down all possible hypotheses, we might want to consider every possible explanation for changes in the climate on the planet, every possible explanation for observations in some randomized control trial. And that's hard. It's hard because it's not as simple parameter parameterised space where you can just go through, and actually, it's kind of it's also hard because it opens up kind of Pandora's box for discussions with the reviewer or with the public. Like, oh, but you didn't consider this one very extreme hypothesis that might you know, like Bill Gates says, Some worldwide conspiracy, this is not your hypothesis base, why did you not consider this, and actually, I consider this quite likely, because I have a high prior value for it. And therefore you should divide by a much larger number, and then the uncertainty is much, much higher. So this is the point where, you know, arguments start that political discussion starts. So social discussions, and so instead, a simpler thing to do. Maybe also a more dangerous thing is to say, Okay, let's not get into the argument yet of what the actual explanation is, let's not compute the posterior. But instead, let's just consider this one hypothesis, H zero. And let's only compute that number. And if that number is way too small, then at least we know that this zero is not the right explanation. So that's what we're trying to do. So let's just consider the law policies the null is the technical term for the hypothesis for everything is this as expected, and see if you can reject it, if you can make it if you can argue that it is extremely unlikely to observe what we have observed without testing an alternative, an alternate hypothesis. And to do this right, we have to not just evaluate the probability for the observation under the hypothesis, because that's just a number. And it can be arbitrarily low if the space of possible observations is very large. So instead, we introduced a set of events under consideration to include the event we observed. And all more extreme events, the tail of the distribution in the direction of the stuff we've seen. We still work it still works. So there are many problems with this. And before we even go into it into like a discussion of how to like how to actually do this, I want to already point out some of them. Because the main problem with tests is that people do them wrong, or that they misinterpret the result. One problem is that you have to talk about what more extreme actually means. So if you go back to our example, you could say, Where's my mouse? Yeah. So you could argue that let me go back to the setting, you could say, Yeah, okay, so this observation of minus four, it's kind of surprising, because it's quite far out in detail. So it's only, you know, 2% chance of seeing something as extreme or this or more extreme. But actually, you know, if you would have shown me something at plus four, that would have been equally surprising, right? That would be the other side of the distribution. So actually, we shouldn't consider only the stuff on the left side extreme, we should consider the stuff on the right side extreme as well. This is called a two tailed test. So if you do that, we have to add up the mass under both sides of the distribution. In this case, it's easy, it's a Gaussian distribution. So there's just you know, the same number on the other side, so we just multiply this value by two. And now the chance of seeing something as extreme as the stuff you've certainly seen is 4.5%, which is, you know, twice as much. But 4.5 is, now it's one in 20, experiments, 20, whatever, 20 odd, maybe it's less surprising now. So then your homework this week, we'll do an example, a week with a distribution that isn't a Gaussian. So it's a symmetric. And you'll find that it's often then much harder to talk about what is actually considered a surprising event. Like that, just like, you know, people doing secondary analysis, always assume that the likelihood is known. People who do tests always assume that it's kind of clear what surprising means. But actually, often, this is not so straightforward. And unfortunately, that isn't even the big problem of the biggest problems of tests. The bigger one is that, what happens if a test tells you that the null hypothesis is unlikely? So this is actually a good, maybe, okay, a good first point for me to ask this question. And then I'll ask it a few more times in the lecture, because this is the one thing everyone always gets wrong. So if we are in this kind of situation, if the test tells us that what we have observed is extremely unlikely, under this simple explanation for the data. What does that mean? You could also type into the chat if you like. If yes, so Oh, someone? Okay, that's a really a question. So I'll just use the Lucia answer from here. So I'll repeat the answer also, for her for the people in the call. If this happens, then that means that the null hypothesis is most likely wrong. Right, that this explanation for the data is most likely the wrong one. Now, let's be careful about that sentence, right? There's two things in that sentence, that arbitrary key, the first one is most likely wrong, it could still be right, it could just be really unlucky, it could just be an extremely unlikely event you observe, right? And you can never rule that out. Because there is some probability mass on the left hand side, it's just a really tiny one. And the other one, which is the biggest problem is, it just means that this explanation for the data is wrong. It doesn't mean anything about another explanation. And the way that this is typically done, so the the point where tests show up in science is, you know, you're trying you're a scientist, you're in a lab, you have this cool hypothesis about how some mechanism in the real world works. So you do your experiment, outcomes a number now what you do is you construct a test where you say, oh, is under the old hypothesis, right of like, under the base explanation of how everything would work. This would be the distribution, this distribution is clearly wrong, right? Because the p value is tiny, small, and therefore my hypothesis is right. I know the last bit that is not allowed, right. It doesn't tell you anything about potential other explanation. To do that. You have to do a semblance of Bayesian reasoning. You have to at least consider two hypotheses. Write your other one and the original one. And at first, first check whether your hypothesis is at least a bit better at explaining the data. But even that is not going to rule out. There might be many, many other explanations, maybe some of them, some of them being much simpler than your favorite hypothesis that actually also explained the data. And this is typically what happens that whenever people talk about tests, there is a either explicit or implicit assumption that therefore their explanation is right. So for example, imagine that all the data for climate change would look like this. But if it's just you know, the last 100 years something has changed, then the fact that something has changed is not an IT does not imply that it's human caused. Of course, there's many, many other sources of data that do provide ample evidence that climate change is actually a human caused caused by humans. But if it were just that this would not be a valid argument. Now, there was a question. Okay, good. So I don't have to read. Good. And I won't read your question anymore, because you just wrote it. I think you think I've answered your question. Good. So let me go back to the slides. What this is a first summary. So testing is a philosophically fraught concept. Because you're you're trying to argue for a festival, you'd like to do inference, but that requires you to write down everything you won't want to consider as a hypothesis. And because it feels a bit dangerous to do that, and you don't want to open yourself up to criticism, you don't want to do that. So all you do is write out one hypothesis, which is arguably worse than to consider all of them, and just show that that one hypothesis is wrong. But you never believed in that first hypothesis in the first place. That's why you did your experiment. So now all you know is that this first hypothesis that you didn't believe in, is actually it's actually okay not to believe in it. Now, what you're not allowed to do is to take that data, this evidence and use it as an argument in favor of your hypothesis. I'll ask that kind of question several times again. So now here comes the math, the one slide that actually contains the formal definition of this object that everyone always uses called a p value. And this is going to be a confusing slide. So I'll go through it slowly. And then we'll do an example. Which is hopefully much clearer. So by the way, hands up who's heard of the term p value? Everyone? Very good. Cool. So now we are going to get to a definition for P value. And as a question, don't you have to formulate a hypothesis before you do the experiment? i Okay, so that's makes things even harder? Yes, of course, you should consider what your hypothesis space is before you do the experiment. But let's be honest, that's also not actually true in practice. So, okay, in some cases, it is. So there are some some, some sub domains of science in, for example, in I think, in Psych, Psych, psychology, psychiatry, this is becoming a social sciences, this is becoming more and more of a thing, where people this is called pre registration. So people send a letter to, you know, a journal, or a conference before they do the experiment and say, We are planning to do the following experiment. Here's what we expect to see as outcome, they need to be experiment, and they already get permission to publish the results before they fund the experiment. It's a really cool concept. Unfortunately, it's not actually followed in the by far the majority of science, it may be true also in you know, super high stakes, fundamental physics. If you're building the Large Hadron Collider, you kind of know what your experiment is going to be. And you can do the analysis before you have the data. But in, you know, your average computer science paper, that's very far from the truth. Actually, it's more like the exact opposite, right? You do the experiment, then you decide what you want to do ask in the first place. Okay, so here is let's go very slowly through this definition. So we are considering one data set, there is one data set one observation of a random variable drawn IID from some distribution, which we assume to just have, that's going to form our hypothesis, typically the null hypothesis. And now we're going to define a thing called the rejection region. So this is the kind of observation that is going to convince us that this hypothesis is not acceptable. We're going to reject the hypothesis, statistical test for this for the null hypothesis. And the null hypothesis is the assumption that the parameter theta lies in some region, or is drawn from some distribution is a decision rule that rejects the hypothesis if we get to see a data set that lies in the rejection region, and it does not reject it's called retaining the hypothesis. If it's in the it's outside of the rejection region, this final sentence or the second sentence is actually important. It's a little bit like with Turing machines, if you don't, if you do not accept, that doesn't mean you reject, right or if you do not reject, that doesn't mean you accept. If you if you if you do not reject the hypothesis, nothing has happened. It doesn't provide any data whatsoever. You Don't know whether the original hypothesis is not true, you just know that it's not yet unlikely. So this usually involves some kind of function called the test statistics, we've already seen some examples of those we'll get to see more today is the critical value. And this is really just a weak way of saying there's going to be some region such that if the data fulfills some inequality, then we'll call it rejected. Now, there are two annoying objects to be defined, that are actually maybe not super important, but they're used in literature. And they're sometimes used in papers as well. So I'll define them as something called the power function, the power function is the probability to reject for the probability under the null hypothesis for the data to lie in the rejection region. And the size of the test is if there is an entire region theta zero to think about to consider for the hypothesis, then it's the largest possible value that the power could have. So when I say that there is a danger that when you hear the word power, that you think it's a good thing, right? Power is something something good, right. So you want the test to have a lot of power. But actually, that's exactly the wrong way wrong. And I'll tell you now, why. So suppose that we have for every possible value of alpha. So for every region, in this probability space, a size alpha test with rejection region corresponding our alpha, then the p value, the thing we actually care about, is the smallest level at which we decide to reject the hypothesis. So what that means is, I have I have a barrage of potential tests available, and they all have a different power, or different size. So they reject a certain amount of the data. Now, what I do is I find this smallest possible one, the one that rejects the smallest amount of data, but still rejects this dataset, then I can return a number and say, the probability. Sorry, this smallest possible test that still rejects this hypothesis has the following size or the following power, and I call that the P value. Now, if that is very small, that means that even a very weak test one with very low power, could reject this hypothesis. And therefore, we should really reject this hypothesis, because basically, anyone who has any kind of power will reject this that this data set, so therefore, we should, we should reject it. While if the power is very high, then that means you have to you if you really want to reject this, this this hypothesis, you have to use a very drastic measure, you have to reject a large part of the data, you have to use a lot of power. So that's bad. But the simpler intuition that you've just already used, which is compute mass at the tail of a distribution is actually the much more intuitive thing. If it's just very unlikely to observe the data that we've observed, well, then, maybe we should just reject this hypothesis. Okay, so that was some annoying math. Now, I guess I mean, immediate questions to that. Otherwise, I'm going to go into an example. And extended examples. Yes. Yes. So the question for people in the chat is, this definition is about the null hypothesis. What about the other hypothesis? What about the power for the hypothesis? And so here again, at least for the purposes of this lecture, there is no other hypothesis. So to do a test, you just test one hypothesis. And of course, you make that hypothesis, the one you don't believe in. And ideally, it's supposed to be the one that everyone arguably believes in at the moment. So the the ideal situation is, we're doing science, everyone has agreed on the standard model of the world. And now we observe one data set that cannot be explained well under this model about the world, right? And then we know that your original model about the world is wrong, we have to adapt it. And then that's where like, the machine of science jumps into action. And we start you know, doing analysis and forming new hypotheses and then we can consider which one actually would explain the data. This is also you know, how a lot of physics actually works. You do some experiment that something that totally violates the so far known laws, laws of the universe, and then you go and see that okay, what can we do? Like what what do we have to change in the math to make it explainable? Unfortunately, in many parts of kind of, you know, more everyday cottage industry science, that's not actually how this works. So, he is the, the example. And I will once again use the biontech Pfizer phase three trial. Why? Because well, it's on everyone's mind anyway. And it's actually a very convenient situation because it is so extremely clear cut. So if you go on Twitter, and check any post by, you know, one of the controversial usual suspects, like, present Austin or so saying anything about vaccines. And you click on the comments, which is always a mistake, you see, you invariably find someone further down there that right variation of all, but can you show me any proper scientific proof that this date that this vaccine actually works? That where is the proof, there is no proof that the vaccine actually works? Well, here is how you make a formal proof that the vaccine actually works. That's what we're going to do today. So remember, nobody spoke about this last week, that so there was this phase three trial. And if you want to read about the protocol, you can click on that link. It's very complicated. Someone has a microphone on this, oh, Kuntala, I think you're gonna have to switch off your microphone. So and you remember from last week that we actually can boil down this extremely complicated trial, with 44,000 people in at least three different countries in the world with a lot of complicated policy, right. So if you click on this study protocol link here, you'll get to 175 page document, I think, on how exactly they selected people and in the end in oil all boils down to 22,000 22,008, and 862. So it's two large numbers at a number of people that were in the two different arms of the test of the of the of the study. So 22,000 people received the placebo, or in the control group, and 22,000 people receive the actual treatment, the actual vaccine, and of those two groups, which were randomly selected. So there is really no causal link between who you are and whether you are in one arm or the other will assume that this is actually true, you can read the study protocol, it's quite convincing that this is actually drew, then eight of those people who received the vaccine actually caught the disease in the end. And 162 of those who did not receive the vaccine, called caught the infection in the end. Now, so last week, we spoke about how to use these numbers to estimate the vaccine efficacy and how to construct confidence regions for what the what the vaccine efficacy is. And we found something like, you know, 95%, plus or minus 5%, roughly speaking. Now, there is a more like a more fundamental question maybe just won't be discussed today, which is, does this actually work? Before we even talk about what the efficacy is, is just does it seem like this vaccine does something that it actually helps people get less infected? And the answer to that is a test. So our null hypothesis is going to be the vaccine does not work. So that means the probability to be infected, if you have been treated is exactly the same as the probability to be infected, if you what if you were in the control group. So those two numbers we've in these estimated so far, the probability to be infected if you're vaccinated, and the probability to be infected, if you're not vaccinated, they are the same under the null hypothesis. So there's actually just one number that has to explain all the data. And you can already guess that having seen the numbers eight and 162, it's going to be unlikely that this will happen at the same time. The question is just how unlikely actually, is it? Should we be surprised by this? Or could it be just a statistical fluke? Might you see these distributions even with 22,000 people, because this is such a low probability to actually catch the disease in the first place, maybe there's this distribution of possible outcomes. And what you see is just two unlikely combinations of events, right? Maybe these two groups just happen to be in the tails, but in the expected part of the tails. But in the opposite direction, it was just unlucky, there were quite a few people who caught it in between in the control group, and quite a few, quite quite a low number of people who caught it in the in the treatment group. So for that, we have to compute a predictive probability for the observation. under the null hypothesis, we have to compute a p of x given H zero, and then have to have some argument about tails, right about what what you consider, like which direction be considered a surprising one, compute the mass in the tail, or maybe in both tails, and then see whether that number is small. And we'll call that number p, the p value. And if that number is very small, we'll say oh, it seems really, really unlikely that this would happen by random chance. The annoying thing here, the main complication is that there is this unknown quantity f, the unknown, from the unknown probability to be infected, which we can't directly measure. We mean we have data that of course provides evidence of what it is, but we don't know for sure. So one way to do to test this would be to say, Let's Move F around for all possible values between zero and one and compute a p value for every point possible such choice of F, and then find the largest possible P value, that's a super conservative thing. And that corresponds to the definition I did in the end. And not in the end that two slides ago that what we're going to do is a slightly more Bayesian version of this, where we integrate out the unknown variable. So we consider actually every possible explanation in one goal. Right. So instead of instead of considering them all and then finding the most extreme version, we're considering the p value for every possible choice of F, and then sum of all of those together and kind of average prediction. Here's how you do this for this particular setup. And it's a nice closed form thing, even though it requires a little bit of math. And it's called a beta binomial test. So at this point, maybe I should point out, there's a lot of different statistical tests out there. If you open up sai pi dot stats, you can find lots of tests, or if you open up a physics textbook, you can find a long list of tests. Or if you're using as many social scientists who are there's something happening in the chat, let me just read this in a moment. If you if you do something that many social scientists do, which is we usually use a software to do your statistical analysis, like SPSS, for example, then often you can load your data set, and then there's a pulldown menu somewhere at the top right for tests. And then there's many different ones that have different names, like they're called Pearson's T, or Fisher's exact, or beta, binomial, or whatever. And you can just click through to all of them, and each of them gives a p value, and then you just pick whichever one is the smallest P value to make it significant. That's really dangerous, right? Because there are different ways that you the test arises from a hypothesis, a generative model for the data. And if you pick a really stupid generative model for the data, you can make anything look really unlikely. So if you want to think about this, I'm not going to do this today in the lecture. But another way of doing this, the addressing this particular situation. So what we are getting here actually is called a contingency table. So we have four numbers, right? Two different groups that have two different outcomes, right? So either your test, either you're treated or not treated, either you have the disease or you don't have it, those kinds of tables. For those, there are different tests, and one of them is called Fisher's exact test, which sounds really cool, right? Because it's by Fisher, Fisher was a big guy, and it's exact. So it must be the right thing, right? Because it's exactly other things are probably approximate. So read up on Fisher's exact test, if you like. And you will find if you may be able to convince yourself that it's not the right test for this situation. And if you want to know why, and like, here's a hint, it's because when we do the test, we don't know yet how many people overall are going to be infected. If you would know this, then Fisher's exact test would be the right thing. And surprising, or maybe a nice thing is that in this case, which is exactly what actually gives a larger P value, so it's more cautious. But actually, in the end, they're all going to be tiny anyway, but it's the wrong test. Right. So yeah, keep in mind that if you want to do tests, you have to be careful to pick the right one to actually explain why you use it. Okay, so someone asked, would it also be valid to ask Does the vaccine work? And unfortunately, for tests, we're not allowed to do that. So because what does that actually mean? What does it mean for the vaccine to work, we have to be much more careful, much more cautious to explain exactly what the alternative hypothesis is. So if we end up rejecting the null hypothesis, that's the probability to be infected in both groups is this is the same if you reject that, that hypothesis. And that only means that that hypothesis is quite likely wrong. But there might be other explanations. So for example, it could be that there was a problem with the trial. Last week on the exercise, we did an example with the Astra Zeneca trial, where there was actually a problem with a subtle introduction of bias, where one group of people were given a certain dose regime, but they were in one country. And then another group of people in another country was given a different dose regime. And that changes the distributions of the outcome, right? Something similar could have happened here. Maybe randomization wasn't good. Maybe there was some subtle bias that the doctor has always gave like the young, unhealthy people the vaccine, that is a possible other explanation for this data that is not covered by this test. So to make sure that this is not the case, you don't have to go and open up the study protocol, or you have to believe the people who did the test or the IMA the court, the court, the agency that does all of this is supposed to do that for you. Right, then you maybe if you trust them, then hopefully you can trust these outcomes. So okay, how do we do this? So last time, we already did inference on an unknown probability. So what we did is we assigned a beta prior a beta distribution or as a prior over the probability f. So last week, we had two different probabilities, F treatment and F control. Now on the null hypothesis, there's only one probability f. But we're going to use the same mechanism, a beta distribution. So just to remind you what a beta distribution is, it's this. It's a way of writing a probability over a binary outcome over a Bernoulli experiment as the x in a form that is, oh, this x shouldn't be there. Hmm, I F, can I just quickly correct this? If I can find the mouse pointer, I can correct it somewhere. Now I have to quickly on the screen, there we go. Okay, so this is a way of writing probabilities, that is what's called conjugate two observations. So the posterior is going to be conveniently of the same form, just with different values in the exponent here. So if we start out like this, this is actually a uniform prior, it's just a very convoluted way of writing a uniform prior flat distribution from zero to one, then if you observe a bunch of events, like in the control trial, 162 positive cases, so people who were infected, and of course, n control what minus MC, people who were not infected, then the posterior over the unknown variable F, after doing just the control trial is and this is so yeah, is a is a better distribution with change parameters. So maybe I could have added a line here that says actually here, here we go. Right? Ah, so maybe this is a typo as well. This should be MC here. Okay, I'll just keep this over here. And then you can imagine that there's an MC M control, but a probability for No, this is not right. But it is right. Okay. With F Yeah. Okay, this is right. So if you multiply Ah, okay, so if there's a c here, that is right, so if you multiply this prior with this likelihood, we get the posterior over the unknown variable F, given just the control arm and distribute MC again, which is a beta distribution, again, with updated parameters. And last last week, we saw that distribution, it's this kind of very spiky, almost Gaussian shaped distribution, bell shaped distribution at the very corner of the simplex. So lastly, we did that, and I realized what I just do here is a little bit confusing. So let me just do that. Again. The this is our prior, and it's of the form f to some power times one minus f to some power, this is the likelihood for the observation in the control arm, it's also of the form some constant times F to some power times one minus f to some power. And so the posterior is a product of those two, normalized the normalization, we'll get rid of this annoying binomial coefficient at the beginning, and we'll have to be normalized with another normalization constant, B, D is this binomial integral beta integral, and the posterior is going to be of the same form, it's going to be f to the power MC times one minus f to the power NC minus MC. So after having done imagined, you would, we would have done these two arms in, in, in, in order, right, we first take 22,000 people and give them just the just like just a placebo, and then wait, because he can't do that, because that's not how randomization works. But if he would have done it this way, then after just this arm, we now have a confidence over what the unknown variable f is. Another question we have to ask is, how likely is it to observe the actual number eight have effected people in the treatment arm under the knowledge assumption that it's created by the exact same distribution by the exact same probability f. And the probability for that is key, we can actually compute a marginal distribution is this right? So it's the probability to observe the number eight in the treatment arm under this null hypothesis. And for that, we take the posterior over F under the treatment under the control arm, multiplied with the probability to observe MT under f. This is basically just a joint distribution over F and M T, and then integrate out the thing we don't know if it turns out that we can actually do that because of smart choices in our parameterization of the distribution. So how does this work? Well, I plug in the observation likelihood, under in the treatment arm, so that's the probability to observe eight under 82,000 Notice I'm using numbers right 22,000 At a cheap people in the treatment group, and eight observe people in the in the who actually got infected. And what's the likelihood, the probability of observing those? Well, it's the number of possible combinations to get eight from 32,000 times the probability to be infected, raised to the power of eight times the probability to be not infected raised to the power of 22,000 minus eight, multiply with this posterior probability, which has this quite simple complicated form as a closing bracket missing here. And now we can take this out this year is a number that doesn't depend on F, it's just a scalar. So we can drag it outside of the integral, this year is also a number that doesn't depend on F. So we can drag it outside of the integral. And what's left is two terms, both of which are of the form f to some power times one minus f to some power. So we can drag those together. And we'll get F to the sum of those two powers times one minus f two, the sum of those two powers. And now we have to integrate out that F. Now, conveniently, we know how to do that. Because there is a function that does that, for us. It's called the beta function is the integral as a function of the two powers over terms like x times one minus x? Yes, that's a question. Okay, so let's forget about the math for a moment. Now and try like, I'll try and do this by hand, waving my hands around. So what we're trying to say is that we made two different observations, if we made the observation of this if you've got got infected in this one arm and in the other arm, and the hypothesis is going to be that both of us observations were created in the exact same way. So one way to do that is to say, let's first consider, actually, we have to consider the probability for MT and MC under the, the assumption that they're both the same. But we don't know what F is. So one simple way to do that is to because so because under the assumption, these observations are IID, right? So we can order them in any arbitrary way, we can first compute the posterior over F, given the control arm. And then that gives us a remaining like, part of the evidence computation under the treatment arm. And that's what we're doing now. So if I first introduce just the, the, the data from the from the control trial, I get a posterior over this remaining F. And maybe I should really change this so that we don't have to keep pointing at it and say that as well. So and it's here should be a C, and there should be a C. And that's right. So if you first consider how likely it is to observe what you observed in the in the in the control arm, it's given by this distribution, I think this is clear right? Now, if you multiply this prior by this likelihood, you get the posterior over F, given just the first half of the data, which is given by this beta distribution. And now the remaining bit of compute is how likely is it to observe the thing we actually observe this this tree in the treatment arm, if it's produced by the same probability. So for that to happen, it would have to be drawn with this kind of probability. So that's a it's a likelihood for F, but it's also a probability for Mt. Five, this unknown F, which we don't know yet, but we know that it's distributed like this. And so we integrate that out to get just a probability. So this here is you know, if p of m, and F simultaneously, is P of M, given f times P of F. And that's the kind of the only thing that happens there. And, yeah, maybe they should, I should have added a line to just do this, in a joint way induced the fact that it's IID. But you could do that as well, right? You could imagine that your P of MT and M C, given f is P of MC given f times P of Mt given F. That's the idea assumption. And then do Bayesian inference kind of in order, right, you turn the numbers around that you first start with prior times only to the likelihood for the control arm that gives this posterior and now the remaining bit is the evidence for the like, together gives the evidence for the whole thing. Yeah, so what we're trying to compute, is let me just move the camera a bit so that maybe people can find see Yeah, I'll try and switch on the light on the blackboard. So what we're trying to compute is P of M F n mt under the null hypothesis, which is integral P of M F M T, MC. I'm bad with subject, given F. And it's the row times P of F, given a zero, B, F. So under the null hypothesis, we don't need anything else other than F to predict, so that that's enough. And that's the same as P of M treatment. Given F, and MC times P of MC, given f times P of F, given H zero, yeah. Now this page gives us this posterior. And all we're left with this, this part here. And that's the VPC up here. will switch off the light again. Thanks. And now Okay, so I've tracked a bunch of numbers outside of integral, and the only thing left to notice is that this year is another integral. It's just of this form, F to some power times y minus f to some power, and there's a function called B that computes these numbers for us. So what is this, it's just a ratio between two of these beta integrals, there should be a closing bracket here. And they're, they're multiplied by some binomial coefficients. They're all just numbers you can actually compute. As it turns out, conveniently, Sai pi does that for us, there's a function called beta binomial that you can just load and literally just evaluate, right? So what we have here is a distribution for the observation empty. So so far, I'm not calling empty yet, because I'm just instantiating, the class that will give us this distribution, and that class has three numbers as its input. It's how large is the test trial? How many observations have you made on the positive side, so how many infected people have you had in the in the control arm, and how many non infected people have you had in the control arm, that's NC minus MC. Now that we have this object, I can plot here, its probability mass function. So that's the predictive distribution for the data for the treatment arm data, and it looks like this blue thing over here. And the actual number we saw is eight, it's here. So I guess that already shows to everyone is exceedingly unlikely to see this. It's just not happening. So what I'm plotting below in is here, the log probability mass function, so that's the log of this blue curve, because you can't see anything over here, right? So to see what it actually is, I'm plotting in blue the probability mass function, and in orange, the cumulative density function, so that's the integral from minus infinity to actually sorry, from zero, because it's a discrete prediction y that there is nothing less than zero. So it's the integral from zero to eight, or to whatever number that is over this blue curve. And that's this orange line. And actually, I'm plotting it in base 10. You see me dividing by log, natural log of 10. So that we can think about it in log base 10. And you can see that the P value for this observation in this particular experiment is something like eight to the times 10 to the minus 39. So we would have to do well, something like 10 to the 38 experiments, to expect to see one event like this. That means it's extremely unlikely under the null hypothesis that the vaccines don't work for this to happen. Well, actually, under the law says that the vaccines don't work and the rest of the trial was correct, right. Okay, um, yeah, let me use a few more minutes to play with this a little bit. So, now that we've done this, actually, I can I'm here I've got the exact same code running, but now with a little bit of possibility to interact. So what I've done here is now imagine you are the person working for Pfizer or biontech, who has to design this trial, there is one such poor schmuck where to do this. And so, actually, I mean, we saw a case of this going wrong here and even with CureVac, who designed Miss designed actually their drug first, but also had problems with their trial to get to see an effect. So when you you remember actually in in, in pharmaceutical research, you have to pre register what your experiment is going to be, you have to hand in the study protocol. Call ahead of time, and you have to decide what your treatment is. So you have to decide what the what the doses is that you're going to give to people. And in which order and when and where and what the randomization protocol is. And you have to take a decision, right? How many people are we going to hire for this test? How long is the test going to run? And are we going to be able to see an effect or not? So what you could do is, and here's, I'm going to be creating a thought, again, I'm just so what happens in this in this plot is everything from down here is just plots again, right? So what happens in these five lines of code up here is that I've created a function that takes in the size of the control trial, the size of the treatment, while and the actual two probabilities for getting infected under the treatment arm and the control arm. Now, of course, when you design the trial, you don't know those yet, but maybe you can guess, like you can guess, you know, like you have publicly available data for how many people currently are infected. And you're hoping that your vaccine works a little bit better, maybe you're expecting a vaccine efficacy of like 50% or so that's roughly what's needed to get market approval. And then you can plug those two numbers in. And now what I'm doing pure purely for plotting purposes, I'd be scaling everything by 10 to the four so that I can show you numbers like four and 37. That Are you can look at look at otherwise, Jupiter interact just shows you zeros. So I'm just rescaling. And then I'm going to plot under the assumption that this is the this is actually the correct probability to be infected. If you are treated or not, I'm going to compute a most likely observation. So how many people do I expect to see in both groups, and this is easy to do, I just multiply the size of the treatment or the control with these probabilities and round. And then imagine I'm now this is actually the data that I would expect to see. And now I'm doing my test, I'm just rerunning the test that I've shown in the previous block. Now what you can see is here, under here, I'm initializing this with exactly pretty much the exact values that you have in the in the real trial. And what you can see here in this plot now is the same as before, but what I'm showing here is a predictive distribution over the observations. So how many cases I think I'm going to see, assuming that the vaccine actually works with this efficacy. Now, you can see that this, like, under this study design, there is a very clear separation, right? It was even if if we wouldn't have seen eight, but something else, actually we saw something quite likely. But then Well, that's not surprising, because I've estimated this eight from this maximum likelihood estimate for treatment. So that's also the most likely value, but it's kind of it would have been unlikely to see something that's can actually get close to the tails of these distributions, and also plotting the p value up here. So now what you can do is, you could imagine that if the vaccine would have worked less well, right, so if it had a much lower efficacy, then even if you had half as many people who were getting infected, you would still see a clear separation and a very strong P value. And you will only begin to have problems explaining or getting, you know, too convincing results if you get very close to them being basically the same. And the reason for that is that the studies are so large. So if I go back to the on the other end, if we go back to this was the original value for another thing we could do is we could make either of these arms much smaller. So if he did this with less people, if you use 12,000 in each group, then the distributions get broader. And of course they both get closer to zero because you'll get less and less people in the arms. And now the 2000 would still work right we've still got a very strong strong signal so they probably just were a bit cautious because of the overall like low cases and but if he were to go to something like you know 4000 5000 People in each arm then okay maybe let's go to ah yeah, even with this very tiny study you would still have been able to find such a very strong effects but if it were weaker right now we're beginning to see overlap, right. So with typically, an accepted result is something like 5% P value to be considered significant. I'm actually not sure about drug trials I assuming that we need a much stronger signal to be accepted. But for like to be able to publish a paper you would need 5% Which would be not the case anymore. After this right here. So 1.5 times that's Actually, that's okay. That's the 1.5%. Change one more. Oh, it's not yet. This really has, they're actually quite a strong, strong result that they did, right. Okay, so now, now it would be, you wouldn't be able to publish a paper about it anymore when you get overlap between these two distributions. Okay, so clearly identify, so decided to do a very, very strong test that actually provided a lot of ample evidence that this thing actually works. Okay, so, now I want to do two more things, I want to point out a few issues of tests. And someone asked him that in the chats. There are a few seconds ago, so maybe maybe I'll have to ask you to watch the watch the video, I want to point out to two common issues that show up with tests that people should be warned about. So one is actually not a not an issue, it's just an issue with the fact that we use tests in the first place, which you've probably all heard about if you have some kind of scientific education. So the main way, the main, like corner of the world where tests are used is in the sciences, as a basically a license to write a paper. So one thing you might be worried about is, if you have a lot of people writing papers, then everyone can just pick some experiment they want to do and report some numbers and just say, Oh, I did this experiment. And here's the outcome, and therefore my hypothesis is true. So you want to have some very basic way of checking whether an experiment that someone did actually surprising in the first place. So it's worth talking about worth writing a paper about. That's what the word significant originally was supposed to mean, a level of significance. And the scientific community, at least for papers, maybe not for pharmaceutical trials, as widely agreed that 5% for the p value is this kind of level that you have to reach to be called significant as a result. So what now happens, and this is a simplification, of course, is that there is this world of scientists out there, lots of lots of PhD students all across the world working in biology and chemistry, and physics and computer science and everywhere, and the social sciences. And they all do their experiment, having having actually an internal hypothesis about their sub being something interesting to study, right? Otherwise, they wouldn't do their experiment in the first place. And whenever they get a big enough P value less than 5%. Assuming they do the analysis, right, they get to write a paper. I mean, that's a simplification, but it's kind of what happens, actually. So what that means is, and this is an experiment, actually someone did a while ago, is if you look at the statistics, the aggregate statistics of those p values, you get a distribution like this. This is a paper from 2020. Where someone collected this is now possible these days, because of large scale repositories for for articles, it didn't, it wasn't possible until quite recently, that it pulled about a million hour while they pulled sufficiently many papers from one of these medical kind of preprint servers to be able to extract a million Z scores. So z is a standardized way of talking about variables that come from a Gaussian distribution. If you take something that comes from a Gaussian distribution, a variable x, you subtract the mean, and divide by the standard deviation. That's like a district like a standard Gaussian distribution, right? And no, z is where you are under Gaussian distribution. And they did a histogram. And you can see that here. Now, what's obvious about this histogram? Yeah, for this is a cut out region around two standard deviations, which is exactly the region in which you are not allowed to publish your paper, right. So one way of explaining this data is that not quite because this distribution is a little bit too much mass in details, but you could put a put a plot like a Gaussian underneath and kind of see kind of what happens here is actually everyone just draws noise, right? You just do enough experiments, about like all drawing from from the null hypothesis. And whenever you get something less than 5%, you accept. Notice, you don't actually reject you kind of accept, right you accept the paper, you reject the null hypothesis, but you accept the paper, you get published. And that's going to give you a distribution, something like this. I mean, if it were harsh, like this, you would actually get, like, just cut out entirely those regions. But thankfully, there are some people who get who find gracious editors who allow them to publish a paper about a non significant result. And you say I, you know, like this is actually an interesting experiment, because there seems to be nothing there. Right, and then you get this kind of baseload. There's also a spike at zero, which might be an artifact. I'm not sure there really shouldn't be one, right. But that probably says something about the null hypothesis being chosen incorrectly. Okay, so the takeaway from this is that this mechanism is flawed. In the sense that it is actually used indirectly as an acceptance mechanism rather than a rejection mechanism, in the sense that papers get accepted when they reach this level. And if you do this for one paper, it's a maybe a meaningful mechanism to use, right, because you're going to end up with a paper that talks about something that's surprising. But if you do this over and over and over again, then you'll tend to accept papers that also are just random, random noise. So one way to come to think about a p value of 5% means that out of 20 papers that report a p value of 5%. Typically, 19 are going to be about something of interest, and one is just going to be random noise. Right? Very basic speaking. And so what that means for you yourself is I mean, one thing is that you have to be careful reading, reading tests and p values in articles. But another one is that you might be yourself in a situation later on in life, where it's not like everyone buys your papers, but you get to design a lot of tests. Or you get to look at your data in many different ways. And then this, this problem may arise as well. So a simple trick to drive this point home, I want to show you an example that you will get to do as homework in details. So I can just show you a very quick story about it. So last year, as many of you know, the Bundesliga did not play in front of audiences. So the soccer players had to play on in empty stadiums with no one in the in the stadium data and it was very eerie, like you can see like the life life transmissions with live without the sound like it seemed really weird. These people running around with just you can hear the ball being kicked, and there's just three people at the boundary of the pitch screaming at them. And before the pandemic, you know, the football fans used to think, or maybe they still think that they are the 12th man, right on the fields on the pitch, it's really all about the you know that the crowd being around, especially for home matches as a big advantage for the home playing Team, that they are at home because they have all their fans, they are screaming at them. So clearly, there must be an effect that for that all these people have, I mean, I think some football fans actually use this as motivation for themselves, right? It's kind of like I'm part of this big team, because I go there and I scream at the other team. So now that we've actually done as a society, this experiment of having everyone play alone, we could ask what it is actually change anything, the outcomes of the matches actually change. So what I have here is the this is actually slightly outdated data, you're gonna use the I think in your, in your homework, the more recent one that I use the exact same table from last year. My apologies, I should have updated it, but and so this is from November last year, was was halfway through the, through the season. And so what you see here is for the first Bundesliga, so these are the 18 teams that are in that bonus Liga, what I do is actually the exact same thing we just did with the Pfizer biontech trial, which is that I use binary outcomes when a loss actually ternary outcomes win or loss of law. And then I collect along one, right, so for every single team, I check how often in the past they have won, rather than either lost or drawn against an against any other team. And it's easy to do that from this cool. Online Databases now open dB, where open Liga dB, where you can just download this data historically up until 2020, and collect it. So this is what I do here. And you will do that properly with the real data for your homework. And then I check for all the matches that they've played so far, how often they've won, and not one in 2020. Right. And now what you could check for is whether any one of these teams has performed differently to before. So the null hypothesis for each team is that they are as good as in the past, now that they are playing without the audience. That's exactly like the Pfizer biontech trial, there's a binary thing we're trying to infer the assumption is not about this is that these binary probabilities are the same before 2020 and in 2020. And then I compute these t values. It's just a call to this exactly this thing that we've just used in the in the Jupyter Notebook before and you get p values for every single team. And now you can see that a fisheye camera via had a low p value it at 2% p value which is considered significant, right? It's less than 5%. So this team seems to have performed differently to previous years. Is that right? Now here we can check whether you followed along with the argument Yes. So what we've done here is we've run this test 20 times, well, actually 18 times. And we found one team for which the P value is less than 5%. But we're expecting to see one such team, every 20 is time to do this test. Right? So if I do a test 18 times, and I find one of them, that has a result that is less likely than one in 20, maybe I shouldn't be surprised. So this is called a multiple testing problem. And of course, now, you could do even more tests. And you could do that at home for your homework. You can also say, Oh, what about if they it says a team that has lost more than in previous years? Is there a team actually, if you look at the data, you're free to do that, you can also create views along Home and Away wins and losses, and try and do a test there. And every time you do that, you increase the number of tests you do. Right, so it's going to be more and more likely that you'll sooner or later find something that is less than 5%, likely. And this is actually a very typical situation, in particular computer science and machine learning research, is you have your data, you come up with one thing you do another thing, this one doesn't work, this one doesn't work. And it's a very subtle kind of way in this kind of this in which this multiple testing bias comes in. So let's say you're writing a research paper about a new training algorithm for deep learning some replacement for Adam or SGD. And you know, you've you've tried a lot of things, the first five ideas didn't really work. Well, now you change the training set that you're trying this on, you do a different benchmark, you compared to some other method, you change the parameters of your algorithm, if you keep doing that, sooner or later, you're going to see something that looks surprising. And a lot of research actually is like this. So to fix this properly, you would have to make sure that as you keep doing more and more experiments, you become more and more cautious about what you accept. So you reduce the power of the test or the size of the test, while you're doing the experiment. The standard way to do that, in scientific literature, it's not actually used much in computer science literature, is to just consider the number of tests you've done, in this case 18 on you, and you divide the acceptance threshold, you're willing to accept by the number of test. So if you've previously, I would have accepted one in 20. And we found 18 tests, you should now at most, it should accept not at 5% anymore, but by you know, 5% over over at. This gives a much more cautious way of accepting actually, it's an overly cautious, it's a super strong, conservative estimate. And it's due to this Italian statistician called count log Bonferroni. And it's called the Bonferroni correction. It's very simple, right? You just keep track of how many tests you've done, and then you divide by the number of tests. Unfortunately, it's so aggressive that if you do more than a small handful of tests, it becomes exclusive of basically everything, and then you can't really accept anything anymore. Why is that? Well, without going deep into the math, think about how the extreme value of A of m draws from a probability distribution, for example, a Gaussian distribution, how that behaves with the number of draws. So if you keep drawing from a Gaussian distribution, how far away from zero is the least likely event going to be? And you can sort of tell, maybe you think for yourself that that's kind of asymptotically not going to get, like ever closer to details. Okay. I will, I will leave out, just mentioned this, like for a minute. So, one annoying, another annoying thing that just is good to kind of have in the back of your mind about tests and these kinds of observations is that people tend to use super, super complicated words, for all of these numbers that show up in these binary contingency tables or tests or, or, like whenever use an algorithm like a test that takes a binary decision to accept or reject something, then you can talk about it in very, very different ways. So if you have an algorithm, a test that accepts or rejects hypothesis, you can think of it like a test, right, like the tests that many of you now do constantly. We all do, right? Like the Coronavirus that are either positive or negative. And these tests they have exactly the same kind of behavior that they sometimes they accept something that you shouldn't accept, and they reject something that they shouldn't be checked, or they do it in the right way. And then because of this, like because this table has many modules, you can come up with complicated names for all of them. And annoyingly, because these tables are used in different communities, people have come up with different ways, different names for them. So as far as like the true positive rate, or the recall or the sensitivity, or whatever else, right and the false positive rate in the fall out and the probability of false at RM and the specificity and so on and so on. They are all related to each other very closely. So you're sooner or later going to encounter someone in your life you work with will talk about, you know, specificity, or recall. And then you will have to be okay, I don't remember this. Let me just open up Wikipedia and check this for you so that I know what you're talking about. And it's not a problem. So in the, in the in the exam, I'm not going to ask you what specificity is, right? It's just a word for the ratio between certain kinds of errors or positive outcomes. So why now now, maybe you notice that this is going to be the final thing I want to talk about. You notice that this is somewhat related to things you might see in the machine learning research paper, right? There's what we've built here. Our test is a classifier, a classifier that classifies an experiment into surprising or unsurprising, and it's sometimes classifies an unsurprising result into the surprising class, or a surprising result in the unsurprising class. And so a lot of the like, contexts concepts you're usually hearing about in classification tasks, or machine learning, kind of transfer over to here. This is essentially a test is kind of a classifier, well, it's essentially like a like a base case, machine learning problem. And that means you can construct things or objects for it from it that you might see in other contexts of machine learning as well, in particular, the so called ROC curves, receiver operator characteristic curves, if you've if any, someone at some point have to test to tell you about ROC curves, or like maybe check, let me just check hands up. Who knows, was heard about rockers before? Who's heard the term before? Who thinks they can explain them? Almost no one? Okay. So let me see if I can because rectum. So this term comes from British home defense during the Second World War, as you may, some of you might know, the Brits claimed to have invented radar, maybe they did, maybe they didn't, I don't know. And they had these like radar receivers out on over the British coast to listen out into the skies, whichever bombers and they had also had to classify right basically what that was, was typically ladies sitting with like headsets up trying to trying to listen in white noise from these receivers, like, like a sonar operator on a submarine waiting, listening for something that might be a plane. Right, and there was no computers, right? It's just a big analog machine that just goes in your in your head, every now and then they will hear something and say this is the this, this might be a plate, right? And the somewhat somewhat automated this is right, but not really a lot. And this, of course has the exact same characteristic, right? You can decide to accept a lot right to raise the alarm a lot. When you do that, you know, you send fighter planes into the skies, maybe you do that in the completely incorrectly because there's just I don't know, a flock of birds or something. And if you if you decide on the other end to be very cautious and only react, if you're really sure that there's something then you're gonna get bombs thrown on London. So there is a tricky decision to take, right? And here with Pfizer biontech, we basically have the exact same situation. So you're, you're designing a, a vaccine that might save the world, you know, and you're doing, you're doing a trial. And you have to make sure that you actually detect whether there's something or not. But every single part of the trial cost sign, I think all of us are now very appreciative of this fact, right, we were all sitting there in late 2020, waiting for this damn vaccine to arrive. And it's like waiting for the final P value to arrive to convince ourselves that it actually works. Right now we had a similar situation like a month ago with the trial for children, right for the they still can't be vaccinated because they had to wait for sufficiently many kids to get infected, to get a low p value. So if you design these tests, then you have to trade off the probability of a false alarm against the cost of waiting and potentially giving false non alarm. So a false negative. And I'm not I don't have the time to go through this in much detail. But what I've done here in this in this piece of code is created such an interaction again, it's based on the stuff you see above. But now for simplicity, I'm assuming that we just know what the the probability to be infected in the population actually is. So I've just fixed that to some number and instead what we can now play with is the size of the test. So how many people we put into our test, right if you put a lot of them in there, then the two classes will be very clearly separated from each other. And if you make it very small and the classes will overlap, and we can play with how many people what the in the rate of infection is in the treatment group. So basically what the vaccine efficacy is essentially right? If it's, if the if the problem If he's higher than set at 70, or the same roughly in both groups, then it's very difficult to separate the two from each other. And if it's, if the task is easy, then they're clearly separated from each other. And the final thing to decide is what our classifier is. That's our machine learning classification algorithm, we learn a classifier, right? So what is a classifier here, the classifier is just a black line in this plot, and you can move it around. Or you can say, if the number of cases in the treatment group is less or equal than three, then we say the vaccine works. If you do that, and this is the case, so the vaccine is actually the true vaccine is sorry, to vaccine is that is that something like? Actually, it's roughly about here, right? So it's seven, as this was pretty clearly separated, actually, then, if we, if we take our decision threshold here, then we are quite likely to accept the vaccine correctly if it works, but we're also somewhat likely to accidentally accept it, because Hang on, you're also likely to accidentally retract it because there's a certain amount of mass here, there's a few red lines left, right. So it might be that we're just unlucky. And by moving the special around, we can basically trade off false positives or false negatives, as you can see in this plot here on the right, so what these ROC curves are, these receiver operator characteristics are plots of the false positive rate against the true positive rate. And they typically look like this, like this kind of corner thing. So if you make the, if you make F a bit worse than they look maybe more like your typical average machine learning classifier, and they produce exactly the kind of behavior you would expect from a learning machine, it's just that the learning here is done by us by setting a vertical like bar. And the space is as trivial as it possibly gets. It's just there two classes, they can be described by distributions on a scalar discrete domain, and we affect setting a threshold. So it's like you're the textbook, one or one very first classifier you can think of, and it behaves exactly like it. So statistical tests are classification algorithms, and therefore, they have similar characteristics to machine learning algorithms. classifiers. Now, in this case, that is that the task was designed such that this classifier has perfect characteristics. No matter how you choose the threshold, you're always well, as long as you make the threshold large enough, obvious, you don't accept something that is extremely unlikely, you're always going to be in the top left corner of the Distribute of this ROC curve. And that's fine. That's perfect, right? That's kind of what you want for your classifier. Okay, so I wonder whether this final bit was all that useful? I thought I could do some connection to to other parts of machine learning. If not, then tell me in the feedback that I'm at the end of the lecture, please scan this QR code to give feedback. Here's a quick summary, what is a test? It's a statistical tool to detect data that is surprising in the sense that it cannot be explained well by the null hypothesis, but the hypothesis is what everyone presses presumably believes in before you do the experiment. The the way that these tests work means that if you reject the null hypothesis, that does not mean that you accept the any other hypothesis, whatever it is it you just reject that one hypothesis. And if you keep doing tests, you keep running different experiments, and computing p values every time, then there's a danger that every now and that eventually you'll get a result that seems significant simply because you've been drawing from the distribution of p values for long enough, and then you have to correct for that, in a way tests are classification algorithms. So they are related in terms of their behavior to classification algorithms in machine learning in general. And you can, for example, define ROC curves for these tests. As a final thing for your calendar. If you're interested. There's a research talk on Wednesday by Hans casting is a postdoc in Paris at ens. He will be talking about an advanced kind of machine learning algorithm that operates on programs rather than like learning learning to represent or predict from functions it learns to predict through computer programs in particular for the simulation of dynamical systems. I'll leave that up. I hope that everyone who wanted to provide feedback has seen has seen the QR code. If not, you can go on EDS and just open up the corresponding form. Thank you very much. And goodbye to everyone in the call 

