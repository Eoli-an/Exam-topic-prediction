Oh, right. Good morning, wanting everyone in the room. And in the call. Let me just check I think I think someone already posted something in the chat Okay, good, then let me rearrange some slides and share my screen so that we can get started. Okay, so there's not that many people in the call, and not many people here. So maybe this is a bit of a Christmas thing rather than a Omicron thing. Let me rearrange a few things should open up the chat. Okay. Someone wants to say something. All right. So every week, I don't I haven't started yet my point. The final thing I need to do before we can get started writing with Zoom. Let's see. Oh, there we go. Okay, so like every week, let's first start with a bit of feedback. So last week's lecture seems to have gone well, people were quite happy with the scores with it with the content and also an ad. Okay, maybe a little bit on the actually, I think this is probably a good score for speed and difficulty, but it's just to the right of okay, you know, quite a few people who thought it was an okay pace. The detailed feedback was that some people thought it was too fast. It's like four or five of you. And actually, this was quite interesting for me that still some people conveyed that it was maybe too much to assume that everyone knows PCA or has heard PCA before. So this is a good piece of feedback for me for next year when I do this again, maybe in two years. Because I keep getting sort of feedback in both directions, that PCA is a little bit too obvious. And you've seen it like 10 times before and maybe also that is actually new for some people in that we should be doing it. And several people enjoy the data sets and historical stories. Thank you very much. So then while I was standing outside, I was reading the newspaper actually the Tagesspiegel and I was reading about Aamir Khan course what else? And I read so I can say I can cite directly from the text this morning on page four. The Unfinished Hoffnung stories and German for just a moment. The Unfinished Hoffnung Omicron credits are not unshaken that is the data very on the Abba V negotiate and contracts for offside, heartsick, this language needs to be stated. So quick check. Who here thinks that Omicron is milder as a disease, although more infectious than delta? Raise your hand in the chat as well, if you like or in the nope, no one? Who thinks it's the same? One person thinks it's milder. No one thinks it's the same. No one wants to say anything. Right? Okay, if you can guess where this is going? So how do we know or not know that this disease is milder or or no not? This is, by the way, my five minute excursion at the beginning of the lecture to just bring back home while we're doing why we're doing a course like this. So actually, those statements are based on a paper that was published over the weekend by a group of researchers at Imperial College in particular, as like, I need a new Ferguson, these are people that are like they're like the corresponding people in the UK to some of the faces you might know in Germany, on what they know so far, about about Omicron. Of course, the UK would know because they have the most cases at the moment. And they do a lot of analysis on it. And what you can find in that in this paper is first of all, of course, a little bit of text of what they've actually done is a pretty short paper, I put it on areas, if you'd like to have a look, you can try it. And then there's a so what they did is they took two cohorts of people who were in hospitals in the UK over the last, what, two weeks or so roughly, right? So from the 27th of November to 11th of December with these with delta, so that's vam, basically, and more or less Omicron. So actually, they're looking for an S gene drop out, but that's essentially Omicron. And then compare these two cohorts of people to each other. And there's a wonderful table here that lists in a semi anonymous way. properties of these of these patients, for example, their where they come from in the UK, what their ethnic background, is that something that in the UK is always done. That wouldn't happen in Germany, of course, their gender breakdown, what kind of symptoms they have, whether they're symptomatic As symptomatic. And for each of these groups, they do to each of these features. They do this for the people who were as positive. So that's people who have built to biggie. And as negative, which is Omicron, typically, and then they report an odds ratio, a log odds ratio, that's just the logarithm of the thing on the right, and the p value. So there is at the bottom here, A Rose for reinfection. So whether people have had it before or vaccinated. And actually no vaccination, I think is somewhere up. Okay. But the more interesting thing is hospital status. So did these patients have to go to the hospital? Or did they end up staying at home? And here we have these two groups, right. So this is for the people who had Delta 260,000 People did not sorry, 207,000 people did not have to go to hospital and 15,000 did. And for the this is right. Now, they had 207,000 People with Delta, of which 1000 had to go to hospital and 15,000 people with Omicron, of which 24 had to go to hospital. And you can see the p value for this is point eight. So what does this tell us? Just to recap lecture, I think four or five, someone wants to type in the in the chat what that says. I'm going to start the sentence for you. There is another hypothesis here, which is that Omicron and delta are equally severe people have so severe means people have to go to hospital. And the null hypothesis that they have to go to hospital with the same frequency. And the p value for this is Oh, point eight. Now, Veronica Sojka. In the chat just just wrote it seems that Omicron is milder. You agree? On, say something? Yes? Yes, so if the p value is not very small, then we can not reject the null hypothesis. That's it. We don't know whether it's mild or not, we just don't know. It's not evidence to say that it's not milder. But it's also not evidence to say that it's mild. Now, okay, so you might actually, over the weekend, actually, look at these numbers. I had a conversation with my colleague for the parents about this. And we were like, Okay, open a sheets, it's not hang on. Why is this number so small? So. Okay, so that's 24 out of 15,000 versus 1292 out of two and quickly did like back of the envelope calculation, because it does seem like that number is much lower. So I started up a little Jupyter Notebook and typed in those numbers. Right. And made, said actually, I think I put those numbers wrong to be honest, though. So it's the wrong way round. Ah, maybe let's see if my numbers my my analysis just changes. I just added that just looked at this table again. Maybe I can on the fly, check whether my numbers now change? No, not really. Okay, good. Thank God. So here is a beta binomial distribution. So assuming that so we take the Delta cases as the knowledge as the the original distribution of those 200,000 people. And 1392 ended up in hospital, and now we have 15,000 new people who have only gone how many of those would we expect to go to hospital if they have the same rate of going to hospital? Well, that's this black distribution here. And this is the actual number 24? Seems like it's way lower, right? So the p value for this is two times 10 to the minus 19. So what's up? Do we have data? Are we sure that Omicron is maybe actually way Wilder? Are these exist? These people are just idiots at Imperial College. They don't know what they're doing advising the government in the UK, that would be really sad, right? Seems like there's a lot of evidence that this is a mild disease. Okay, I'll tell you the story. You're clearly not in the mood to have a discussion about it. Maybe I'll get to get you to discuss later. So if you go back up, and we try to read what they actually did, then. So there's a section here on statistical analysis to estimate factors associated with Omicron cases relative to Delta cases. We fitted logistic regression models to all Omicron cases. Using our two separate case definitions as above, using the following predictor variables, aha, each band that did that, we did not explore directions, given that the case numbers are so small. So they did what Professor McHugh told you to do in his second regression lecture. And they fitted a logistic regression model. So we'll have to guess what exactly it is. I'm guessing it's a linear model, given that they have so many explanatory variables. Maybe it's even additive. And so what could be the reason why they can't explain that, why they can't be sure that there is actually a milder set of cases here? Well, it has something to do with H band. So if you look at this x distribution, it's difficult to look at some bunch of numbers, you're right. But I so plotted them for you just now before I walk outside. So in dark, you see the people with Delta, standardized, and in red are the people with Omnicon. And I've made the bins as wide as the age groups are, so that the areas of these bins represent the total number of people relative to age in both groups. And you can see that these distributions are very different. So that delta group had a lot of kids in it. But that the dark group, right, but not so many young people, while the Omicron group has a much larger group of young adults, that's, you know, people who came in contact with Omicron first professionals who are traveling like, you know, active people will not like you, and not so many old people. And old people get severe diseases and young people don't. And so the model doesn't know now do we know? Well, maybe you can go out if you want to the numbers are in this in this table, right. So you can go and run your own little model, actually, you can't really because it's not the full data set. It's not the individual cases, it's just anonymized by individual features. But you might want to think about for yourself, actually, whether using a logistic model in this kind of way is the ideal thing to do. It's probably the thing that they know knew how to their to do it in like two hours or so right? Before they had to provide a report to Boris Johnson. So they quickly did it. But actually, she think about what each band actually means maybe it's something that should be modeled with a little bit more care. Nevertheless, we still just don't know, because this data, at least this model, this analysis does not provide using this model, if you just like decide to trust the P values, then we just don't know yet. So I'm showing you this data a to remind you that it's actually useful to know about logistic regression and p values and estimation. And also to understand that these these decisions that are currently so important for our society are taken by people just like you, well, then decisions aren't. But the reports are created by people just like you based on just a bunch of numbers during data analysis. So if you know how to do data analysis, you're able to at least understand a little bit better where this information comes from. Okay, so that wasn't actually what I want to talk about today. But maybe it's useful to understand that, you know, certain groups of people have different risks and mentioned, sometimes you have to be careful about how you treat these individual groups. Speaking of which, let me see if I can find a point to click to return to a data set that we used last week, to briefly where you might also wonder whether there is more structure in there that could be used and considered. So this is this label of faces in the wild data set that are very used, introduced last week as a kind of a sneaky tricky example, I showed you this rectangle of noisy, seemingly noisy data and and said, What do you do to understand this, and then we decided that maybe PCA is a good thing to do. And realize that PCA doesn't actually show us much structure for this data set is look just looks like Gaussian noise. And then I read I told you what if you take this data set, and you rearrange it into rows, and columns, then you get images of faces, to actually this data set contains, of course male and female faces. And as it happens, there is actually a file that I would have provided you with an in last exercise sheet that also provides the assigned sex or gender of these people just by name. And so here at the top row, you see five people labeled as female in this dataset and five people labeled as male in this dataset. I can wait last week we spoke about how to do PCA on this just as a quick reminder, PCA is a single line in Python. Essentially, we run the single add columns On this data set. So here x is this rectangle, this rectangular matrix I showed you last week that is of size, I don't know something like a few 1000 downwards, I think 7000 or so people by no 7000 is the number of pixels actually, on the Ribbon NSB SVD. On it, we gather a bunch of matrices out Q are the left singular vectors, we don't care about those, they tell us something maybe about structure inside of the along the people, D is a rectangular diagonal matrix, and you are the right singular vectors, those are the directions that provide the principal components of this data set. And if we start the order by size, if we start with the leading Eigen vectors, then we can project individual faces onto these Eigen vectors. So here at the top, I showed you again, that's what I already showed you last week, here, all the principal components. So you can see that the first principal component is just like a moon shaped face round, and like the generic human face. And then the further principal components are, I guess, kind of features of either the face or the lighting or the setting that they're in. So whether they are lit from the left or the right from above or below, maybe that's a lighting thing as well. And then we begin to see kind of eye shape maybe whether whether people are wearing glasses, or whether they have like I don't know, wearing hoods, or have a have a mustache or something right. And here downwards, you see a bunch of faces, 12345678 faces these, these actually the first eight faces in the data set that I just randomly grabbed. And they are reconstructed by projecting this face onto all of these principal components. And if you add them up, you see that as you add more and more principal components, you get closer and closer to the true face. So after those, I think this is 20 projections, you still can't really make out these people, but you know, it's getting in the right direction. Clearly, they look very different now from each other. So this is a way to compress a dataset onto a low dimensional space. And in such a way that the reconstruction error is minimized. The quadratic reconstruction error. So how large is the reconstruction error, actually, so I compute this in these roles. I take the data sets, I've now split up this data set, maybe I should have mentioned this before it to two groups, the male and the female ones, I'll show you the line for this, the PD obvious one, right, so I just say the female people are those where the sex label is zero. And the people that are those were the sex, there's one. That's as it happens, now this thing is encoded. And then now we have two different groups. So for each of these groups, we can project the singular, right singular vectors, the principal components of the original data set of x, right, we can project both of these groups onto these datasets separately onto these principal components separately, and then compute average reconstruction errors. So reconstruction error is the difference between the original datum and its projected version to the projected version is that we take x, we first projected on to the right principal components, and then be projected back out again, but we only project on the first few principal components, whereby I ranges from, you know, one to whatever 20 In this case, and then we square the difference, and we take the sum across both the N and the P coordinates. So both over the image and along all the images, and then divide by the total size of the data set, so that they are comparable to each other, and then I can plot those reconstruction errors. And as it turns out, the average reconstruction error as a function of the number of evaluations is higher for women than for men. The black role is the overall intersection error for the whole data set. And the red line is for the people labeled as female in this data set and the sort of golden yellowish line is for the people people have made. So PCA is a sexist algorithm question to you Why yes, please. So I we actually have the same answers in the chat and in the in the room, their first answers were maybe there are more men in this data set than our women. And one more explanation Actually, again, so someone said in the chat, and now here in the lecture hall, maybe there's more variance in hellings? Actually, it's a slightly different answer. So in the chat, he says, maybe there's more variance in hair length, you're saying maybe their faces are more covered. So actually more covered wouldn't necessarily be bad, right? But if you think back of this principle face, then that's actually like a, I don't know, like a covered face. Right. More variants in LA, this is a typical answer that I often hear is like, women are somehow more complicated, right? I've also said in previous years, someone, someone said, maybe it's because of makeup? No, there's more expression in the faces. Or, you know, hairdos are complicated men are all like the same basket, and the grooming is complicated there. But of course, it's a very good first idea is maybe it's because there are just more men in this data set than women. And so this algorithm kind of learns to detect male faces better. So actually, let's try that. So is this perhaps due to a different size of the group? So I asked, the little computer tells me how many men and women there are. So there are 10,000 people knows 10,000 million people and 3000 female faces in this dataset. Ah, it's an imbalanced data set, we just have more men than women in this dataset. Okay. So I mean, we can fix that a little bit. So I decided to run this analysis. Again, I clipped the data set. So I just take the first 2962 male faces, and then construct a balanced dataset from it. And you of course, you can debate whether that's exactly the right thing to do. We could also resample, the female faces, that seems really dangerous, because then we have the same face several times in the dataset, that's probably going to totally bias what PCA does. We could also randomly select these men, but actually, the data set is pretty random as it is. So maybe it's not that big of a deal to just select from the start, because it's bad design, you know, I should probably tell you that you should manually sample from the data set. Okay, you can check that for yourself, if you like. Now, I constructed this balanced data set, which now has roughly 6000 faces like two times 2962. And then I run the analysis again. So I run SVD. Again, on this clip data set. Now we get new principal components, it might look different. Like you're invited these things, by the way, of course, on EDS, you're invited to try for yourself what those principal components now look like whether they changed whether we now have, you know, long hair principal components, and we can run the whole analysis again. And here is the reconstruction error for the correct data set. On the left is the old plots on the right is the new plot. Hmm. So it has changed a little bit. It's this algorithm is now like bigger construction arrows. Now it's closer to each other for both groups, but it's still different. There's still the female faces are still less well compressed than the male faces, especially for the leading few principal components after about 18 or so they get pretty close to each other. So there must be something fundamental about PCA. It's, it's just a sexist algorithm. Why is it something about female faces? They're just more complicated? Is it? Is it maybe just something about PCA? It's an eigenvalue decomposition? Maybe Eigen values are a sexist idea. No. I can tell that this is like, you know, 2021 people are stupid. It used to be different. Like yeah, algorithm such as Texas. Okay. So What other ideas do we have? Okay, so I have we have this like, possible idea that female faces are just more complicated. Is this true, though? I mean, if you just look around the room, does it look like your female colleagues who have more complicated facial facial features than that, then your male colleagues? I don't really think so. When people on the chat, unfortunately, are in the call, unfortunately, can't do this. But so what happens? So I maybe maybe first of all, does anyone think that this is a problem? That we have an algorithm here that works better or worse? For, like half the population? Yes. So some people think it's a problem, right? Not everyone, but a few. So what do you do when that happens? Actually, maybe maybe before we talk about this, let me ask a question. Now that I'm going to ask again in a few minutes. Whose fault is this? I'm sure you've seen you know, you've all seen reports in the media about biased algorithms. Maybe you've read about, you know, all these problems with algorithms. You've probably read sentences like the AI bias is this evil AI algorithm that is sexist, for example. misogynist, or whatever. Maybe you've read articles that blame the company that built the to write for a while ago there was I don't actually have it in a slide. But there was a case that went through the media two years ago that Amazon research team of Amazon in Edinburgh have built a hiring tool. And not just Amazon does this many companies now do this, why there are also tools if people just just do that turned out to be severely biased against women, it just decided not to hire women. And if you read the press articles about this, there was a like there the I guess the Devon said, I read the messaging was more than Amazon is an evil company, because they did this. Like they, they the humans vote this, like the people at MSN wherever represented by their employer, they are evil, because they built this tool that is evil. So is the tool to blame? Or is it the people who wrote the code, so you write because you're the ones who are going to write code for machine learning in the future? Whose fall couldn't quite get? Good. So I'll, I'll try to synchronize the conversation on the chat. And here again, so the the sentence in the room is, it's the fault of the person who collected the data. Because it might have something to do with the data set in the statements in the chat, or I think the fault comes from the data set. So the people who collected it are at fault. And someone went nobody's it's nobody's fault. It's the data's fault. But it's our responsibility, how to use it. So somehow, it has something to do with the data, right? The data is there, at least. But I'm, I'm actually glad to hear that no one thinks that eigenvalue decompositions are generally a bad concept. And this is actually a reason why I do this with the with this label faces in the wild PCA combination, because PCA is really like the most straightforward algorithm you can think of, it's much harder to have an opinion about deep learning, right? Like if you build some big deep learning algorithm, or maybe deep network such as somehow I don't know that, but but PCA is literally it's an eigenvalue decomposition, right? You've had enough formal education to probably agree that there's nothing inherently ethical or unethical about computing the eigenvalues of a matrix. So it must have something to do with the data, the other ingredient to the algorithm, or just the way we use it, maybe. So one more statement on the in the chat, the average error could be misleading. Maybe they are, maybe they're just outliers for women. Okay, that's maybe actually also a statement about the data, right? It's another way of talking about it. So let's have a look at the data. That's maybe a good first thing to do. If you notice that there is a problem with bias in your algorithm. So I did that. Let's see. So I just said, I just took the data set, and said, well, let's have a look at those, those data data points that are well and badly compressed, what do they look like? Maybe that's a good thing to do. Right? So what they basically its back fixing, right? So what are the data points that our algorithm doesn't work well on? It's not hard to do this. And you can also do it of course, for like a big deep learning architecture, it doesn't have to just be PCA. So I took the data set, I computed the average reconstruction error for a particular set of like a regular dimension. So I chose the fifth one. So this is basically this data point. And at this point, by the way, I'm actually I'll keep using the original data set, I could of course, also have done the same for the corrected data set. Now I invite you to try that out. Right, it's a simple change to the code. So are corrected what clipped right rebalanced. So if I take the original data set, and I add the fifth dimension, compute the average reconstruction error, and then sort the data set by this average reconstruction error. I can go from the top and look at the first 30 Best compressed images and the bottom 30 Worst compress images and I'll plot them from the top the first two rows are the best compressed and the bottom rows are the worst compressed so what do you notice if I like the fact that you're saying it's something about glasses and wearing hats, and in previous years I He did correctly point out that it's just a bunch of dudes, right? It's all basically all men, I maybe I'm missing a woman in here somewhere. But I think these are all men. And you're at the bottom. It's a bit morbid, right? There's races. So actually, let's go a little bit slow here because I think this is actually a useful, very useful takeaway, like a way, please. Yes, so these are all people who stare straight into the camera. And that's not surprising, because these are people who are very represented by those five phases, you're at the beginning 12345. So they look like those principal eigen faces, they look like just a person staring straight into the camera. Now, let me first find out this, what we just did, is a very, very useful thing to do. It's maybe the first thing you should do if something breaks. And I can't stress how important this is enough. Because it may seem totally obvious to you. But it's shocking how few people actually do this. I can't tell you how many masters students, PhD students I've had in my group will come to me, thank God, it doesn't work and algorithm doesn't work. And even they might show me a plot like this. Right? I've already I thought maybe it's just different somehow, but it's still something that I don't know. And then at some point, I go like, well, good. If you look at the data, what know like how, what what Bob, let's, you know, open up a Jupyter Notebook, have a look. And then Ah, okay. So by the way, there's already other interesting stuff you can find. So for example, it turns out that the sixth worst compressed image actually just has a, there's just like a defect in the image. And there's one more like this here. It's just a weird sort of quadros circle in this image that doesn't belong there. So maybe they're just sometimes bugs in your data that you might want to read out. Right? I, I would be surprised if those if that is particularly more or associated with female faces, I think it's just a deep data processing problem. But just as a as a side observation, right. And now let's think more about what why it is. Okay. So what we've now realized is that the thing that makes a face badly compressible, under this dataset, at least, is if Well, I mean, a simple explanation would be people wearing hats. People wearing hats are badly compressed. Right? Okay, but it's not like only women wear hats, right? In fact, there's quite a lot of men here who are wearing hats. I mean, maybe ask the question differently. What do you think is the profession of these people up here? And what is the profession of these people down here? Nobody wants to guess. So I'll tell you a little bit about this dataset. Like last week, I said, the very first thing you do as a data analyst, or a data scientist, or machine learning engineer in whatever setting you're going to be working in, when someone gives you a data set, is to go back to the person who collected the data for you and say, How did you collect this dataset? Tell me more about it, right, provide a prior for me, please, like give me mechanistic information about this dataset. And then you might find out that this data set is called labelled faces in the wild, it was collected by a guy called Gary Huang. And what he did is he collected images from the internet of public persona, right? He used sources, like news articles, for example, that were the container images of of people, because of, you know, privacy rights. He couldn't use like a private data set that contained individual like people just average Joe's. So in fact, he collected famous people before who are in the news, because they typically have less of a right to their own face. And now people have figuring out that they that they discover certain people in this right okay, so they notice some faces. And of course, like if you collect people who are in the news, what do you get? You get politicians, you get celebrities, actors, musicians, and so on. And you get sports people. And you can see them here, right? They are politicians. And as a bunch of sports people, they tend to be very hats, because they're typically golfers or something right or sport tennis players actually. And then you get a bunch of it. No celebrities, Elton John, for example, I think, right? And, you know, yeah, people who might be, you know, somewhere on the red carpet or taking a nice picture of themselves for like some promotion campaign. Now, what's difference between politicians and actors and celebrities and sports people were two things. One is that politicians tend to sit straight in front of a camera, they're talking into a camera, because they make some statement, they come in, they sit down for a press conference, they look straight into the, into the camera, and they say, you know, whatever they want to say. And then you end up with a picture like this, right? Front and center. And sports people tend to be pictured in action, right, you can see here, someone you know, serving, like, tennis, tennis serve, and they're moving around a lot and wearing hats. You know, they're outsiders like that, like indirect elite. And actors, you know, tend to have these, you know, but one thing is, they tend to be more complicated clothing. And also like, they tend to not have these staged, nice photographs, maybe they are on the red carpet, they're kind of posing the, the pictures taken from a larger distance. Or maybe it's just like a, you know, like a promotion picture is sort of fancy, nice dressed up. But the other thing that's different about these groups is that politicians, on average, are more male, than actors in sports. And so, I didn't actually do this analysis, because I don't have a label set for who of these people are politicians and move them are celebrities and sports people? And this is actually very, very typical, right? There's an underlying structure in this data set, that we can maybe just guess we get a sense that that might be the problem. But we can't quite confirm that that's the case. Because what would we have to do? Well, we'd have to sit down and go through those 14,000 people or 13,000, people look at every single face and decide which group to put them in. We're not going to do that, right? It just takes too long. But if you just look at how well they are compressed, and kind of get a sense for what's actually going on here. So what could we do to fix this? Other than, you know, just well, actually, no, maybe I shouldn't tell you, right? So what what do we what would the what could you do if your goal is to make sure that the average reconstruction error for both groups is the same? Vote cut off. Ah, so for those people in the chat, the proposal is we could cut off the people who are badly constructed so we could remove them from the dataset. Does that sound like a good idea? So what that means is you're cutting off the sports people and the the actors. Like you're biasing an algorithm, because to infer towards a certain group of the population and against a certain group of the population. Right? So you think there is this set of people in the population? Who my algorithm struggles with? Oh, you know, what, just let's get rid of these people. They suck anyway. Right? And that's, of course, not a good thing to do. So if I like this, thanks a lot for your answer, by the way, sorry, I didn't want to like get people new disease. This is exactly why building algorithms that are kind of ethically okay is a really complicated job. Because some of the things you might want to try to fix the issue, actually have negative effects on other groups that you might not have thought about. So in this case, it's the group of sports people, athletes, and maybe, maybe celebrities. So another thing we could do is, we could say, maybe v v balance in each group. So we go through, and we say, we decide that we want to have an equal number of female and male politicians, an equal number of female and male athletes, an equal number of female and male sports people. And then that might actually reduce this problem, right? If our story, our hypothesis for how this problem was created this. I didn't do that because it takes too long. I didn't want to go through and create a dataset with 12,000 people and like individually look at every single image. And that's true. It's a common problem, right? If you have an actual data set, maybe your data set isn't just 1000 people, maybe it's a million people. There's no way you can do that. Well, maybe you could you could pay some poor schmuck somewhere in a faraway country, little bit of money to do that for you. But that's an ethically questionable thing as well. So what I did is, and I'm not saying that's what you're supposed to do. So someone in the chat says, I don't know if there is something along the lines of combining boosting and PCA. So what you're like, Okay, I'm going to generalize what you're saying. You're saying, maybe we could build an algorithm that by design ensures that men and women are equally welcome compressed. So we could look at PCA. And think about what could we do to this algorithm to force it to equally well compress both groups. So to put a constraint on the algorithm to ensure that it satisfies a certain desire. And that's what you'll do on your homework. So it turns out there is an algorithm like that. It's called fair PCA. It was invented by Samira Samadhi, who was a group leader here in Tuebingen. And you're going to get to implement her algorithm in your homework over Christmas. Which, by the way, is the last homework ever in this in this course, so well, before the project's right. So maybe that's some motivation to look at it. Okay, someone says maybe not weighted direction of the face as high the first eigenvector as high. However, this makes the model far worse, I guess. Yeah. So then it's just not PCA anymore, right. So we could decide just to not to use PCA. That's actually a possible recourse. It's just to say, Oh, this doesn't work. Let's not do it. Sounds a bit too straightforward. But maybe it isn't. Actually, there is one possible outcome, I'll come back to that in a slide in a moment, is to just not do certain things. So what I did is I said, Okay, maybe we can have like a, probably a very pedestrian solution. And it's for the tactic purposes, it's not the right solution. But it's one that actually does something interesting. So where's my mouse? What I decided to do is, I think, no, maybe one way to think about this is not in terms of what these people do for professional, what they're, you know what the job is, or something. But instead, I mean, if you look at this disparity reconstructed faces, one way to think about them is that he does have a lot of contrast. So these are images that have a lot of bright whites and dark spots. So if you think about the mesh histograms, they are probably very widely spread out, right. And heavy tailed. While those faces are pretty plain, they are relatively homogeneous, because it's just the front of the face, basically. So all of these faces have pretty much the same grayscale value across all the faces. And by the way, so I should have made me fix this as well. But here, I actually show I use pythons in show. And maybe it's the deducting thing to do a teachable moment anyway. So in show standardizes, each face to the range of the grayscale values, which is why they look equally dark kind of like, but actually those faces are relatively homogeneous. So that if you, if you use the same grayscale Color Map for all of them, these will be bright black and white images. And those will be you know, pretty much average gray. So what I did is I made some statistics. So I compute for each image individually, the means of remember that this data set is centered, right, so we subtracted the mean, but we subtracted the data set mean, so that's the sum of this matrix along the n axis, right? So here, instead, I compute the mean along the P axis, and also the standard deviation, and the skew and kurtosis. For those of you who don't know, the skew is the third moment and the kurtosis, the fourth moment. So this is the expected value of the linear function, the expected value of the quadratic function, the expected value of the cubic function, and so on. And then I plotted histograms for these two groups. So here you see, for these four different things, you can compute four different statistics, individual histograms, for in group in red, the women in this data set and in a yellowish gold mesh in the background, the men in this data set. And you can see that actually, they're quite similar. Of course, of course, that images of human faces, but they're not exactly the same. So in particular, it might be that they like, female faces have a little bit more a little bit higher standard deviation, it's just a tiny difference, but we're also just looking for a tiny correction. And then what I did is I took the data set, and just sort of standardized it along the the P direction as well. So what I do is I take for every single image, an individual mean, and an individual standard deviation, and then I subtract the mean and divide by this deviation. So now each of these images has mean zero and standard deviation one. So if you actually do this in practice, maybe let's keep in mind what we're trying to do here. But we're trying to compress this data onto a low dimensional representation. So if you want to have a faithful representation of your data set, what you've just done here is you've created two new numbers to store, right? If you wanted to reconstruct this data set after running PCA on it, which is what I do here. You could Vic, we could reconstruct each image by first protecting the low dimensional representation out through the decoder onto the high dimensional space, and then multiplying with a standard deviation of that image and adding the mean of that image. So to be able to do that, you have to store those means and those standard deviations, and those are two additional vectors each have a size of one principal component. So I just made the algorithm a little bit more expensive, in the hope of fixing its gender bias. So if you do that, and then again, project, the correct computer, the projection error, you will get this plot here on the right, this is the original plot is the new one after this correction. And now we see that so remember that we are kind of invested two dimensions. So we could basically start looking here, more or less, the Y axes are comparable anymore, because I've standardized each image, right, so the different numbers coming out now, the average construction error, but relative to each other, you can now see that these two groups are actually much closer to each other. In fact, the women are now better compressed than the men. So what I just did here is I took this data set that has a feature in it, which is maybe causal for this gender bias, which is that these people that have taken pictures of actually belong to different professional groups, and therefore their images are taken in different settings. And I decided to do an ad hoc change to this algorithm by taking the data set and changing something that actually is maybe the only associated but not causally linked to the thing that originally caused this bias. Right. And so that seems to have helped. Is it actually a fixed though? So someone in the chat already has provided the answer to this is no, because that algorithm is now arguably sexist. Again, it's just Texas the other way around, right? Because this red line is now even though it's less a below the average than before, it is below. So now the women are actually advantaged in this in under this measure of advantage or disadvantage. So this is our first takeaway, if you want to fix such a problem in an ad hoc way, then you could. But there's a bit of a danger in doing that. So the first danger is pretty obvious, there's no guarantee it's actually going to work. And it may introduce new biases, that may lead even harder to spot. So maybe this new algorithm I created is now this is nonteaching certain ethnic groups, rather than genders. But there's another more subtle problem. Which is I asked you before, whose fault is it, that this algorithm is sexist? And previously, your answer was largely if I if I'm allowed to generalize a little bit, it's the data's fault. Right? We we're not to blame, well, sometimes it's our job to like, you know, take care of this. But it's not our fault that the data is biased. But if you change the data like this, maybe it becomes a little bit our fault, right? Because now we've interacted with the data to introduce, like, actively a certain bias. And so we've kind of loaded some fault of our own, onto our shoulders. I just want to add to like, complete this argument about whose fault it is and whether it's the data sets folder not also so this is obviously this, by the way is a hotly contested topic in the machine learning, fairness and ethics community, whether a data set is that doesn't belong to anyone or it's just data can be fault can be at fault, but this is not never a human's fault. And I think that the answers, they are really complicated, actually. So let me just express this again. With Okay, so I already asked some of these questions, right. And you provided some answers, right? So our solution now actively involves changing the data set, and that's probably a bit of a dangerous thing, because it's kind of no babies our own problem now. This problem definitely exists across datasets and often existing datasets that people don't even think of as potentially problematic. So hands up in the chat and here in the room who has used a cipher 10 or cipher 100 datasets before? Maybe as an homework exercise in a deep learning lecture, maybe in a research project? No, no one. Have you heard of cipher 10? Cipher 100 or two people are raising their hands 345 Okay, at least a few other people who have voted on research projects or maybe homework for deep learning lectures five now who knows of cipher 10 500 as a few more hands in the room, who knows a feminist? Ah, okay, interesting. So cipher 10 is like the next thing after m NIST you try. And this is too easy for deep learning now, so nobody really wants to use it anymore for research purposes. So people use cipher tenant cipher. 100 is the data set that is was collected by was then a PhD student in Jeff Hinton's lab, Alex Krzyzewski is now in industry. And it consists of tiny images. They have roughly the same size as eminence digits, but they are RGB images. So it's basically three times the size of m&s for each data point, because it contains three different color channels. And those those are labels, in the case of cipher tender data with 10 different attributes, 10 different classes, it was meant as a replacement for Amethyst, and in cipher 100, it's 100 different classes. So of those people who just raised their hand and said, you've used with cipher, you've worked with cipher 10 or 100. Before, how many of you have looked at the images that you were training your deep neural network on? I mean, if you've actually like, loaded them, and made a plot one person to Okay, that's Oh, that's good. Ah, if you good about yourself, so did you notice something about those faces? Are those not the faces of all those images? No, okay, so they have a lot of mistakes. So that is true. Actually, this is actually quite badly labeled. So this data set, actually, so what I did last yesterday, nothing to do on a Sunday, I loaded cipher 100. And I printed out all of the images that were labeled as men and as women. And here we go, those they are. And there's also a category for child, I think, one for girl and one for boy, but I like that just use men and women, maybe you can convince yourself that those are not good IID samples from the population for actually for either group. And I'm not gonna go into detail of what's actually on those images. But it's maybe not the set of images that you might want to take that you want to use as a benchmark for computer vision. And yet, people are still using this data set. Actually, I have to admit, even in my own group, when we build optimizers, for deep learning, like methods research, we use cipher 100 as a test problem, because reviewers tend to like to see it. Although really, it's not a good idea. I mean, for methods research, maybe it's not so much of a problem, because it's I think it's unlikely that we might end up preferring Adam over SGD because of something like this, but still kind of a bit of a problem. So let's come back to whose fault it is. Do you think it's the people who collected this image dataset are completely not at fault for it? Like no one's to blame? It's just the data? Probably not right? It's whoever collected the data, we should probably put a little bit more care into how they got their data. So actually, I think what happened to you is that Alex, just use Google image search. So you just typed up 100 queries, and just dumped all the images that you got rescale them to tiny, and then you get basically just a website. It's just the images that Google provides, if you type all used to provide in 2009, when you type those search queries in, that's a long time ago, by now Google image search, of course, has been V balanced, and it provides very different answers. But these images of images survived, they're still around. So if you're the one making a dataset, don't offload your ethical tasks, or responsibilities to Google, or to any other source of data that you might want to use. Okay, so it is this blame game thing is, it's an important topic, which is why I kind of keep hammering on about it a little bit. So it is correct, I think to say that most forms of bias in machine learning algorithms arise from the data. That's a statement that is factually correct. It's very unlikely. I think that someone that deliberately builds an algorithm and designs the model such that it has its disadvantages or advantages, certain groups. However, it's also a statement that's a bit too Simple to use, you say, well is the data fault, nobody's to blame, right? Because there are people who have collected the data. There are people who have built the algorithm and not properly checked whether it might have this problem like whether the data might create a problem in the behavior of this algorithm. And there are people who decide to use algorithms in the wild without reflecting on whether they are potentially dangerous or not. So if you're leaving this lecture, or taking a job as a data analyst, or data scientist or machine learning engineer or Research Engineer, some of those tasks become yours. And it's at least necessary to have some good best practice to just look at datasets when they arrive, to think about what kind of implicit biases they might have, and maybe do a few tests to notice whether they have problems. And just to be just to make sure that I don't like go back to blame people. Of course, the people who do this actually are aware of this as well. So there's, like fairness research is a large topic in the machine learning community by now. It's an entire research community that has its own conferences, even workshops at a large conferences, there are books about the various I already made may have mentioned more, it's hard to it's now Mustang directory and tubing. And starting from January is already living here. We wrote a book recently about fairness. By the way, the person who collected abled faces in the wild Gary who is on the website of Abel faces in the wild, if you actually download it directly from the website, he has a big disclaimer about these problems, for example, says somewhere here that this data set contains a relatively small proportion of women, it should not be used for certain things. So the person is aware and that there isn't a bit of, I think, a complicated bind, that he can't just take your data set off the web because people want to use it. And it may seem wrong to just take it away as well. And there are variants of this problem, of course, across the research community, not just the research community, but society as a whole as well. So last year, that will last legislative period, sorry, not last year. So in 2019, actually start starting in 2017, after the last federal election, the German parliament gave itself a an orchid commission, it's one of these large inquiries that the parliament Parliament could do only create a two such commissions over the entire course of the legislative period. And one of them was on AI. And it's a long process goes over many years. And they invited lots of experts, I got to go to Berlin as well and talk about actually research funding, not fairness. But there are many other colleagues from across Germany were invited to we had to give evidence and witness reports and provided a report at the end. And I very much recommend that you have a look, it's a very good text. Actually, the politicians have thought a lot about AI, and as long sentences, long paragraphs in there about fairness, and how it affects how algorithmic decisions affect people, and what politics has to do to ensure that algorithms are fair to the population. However, the topic of fairness and actually, I mean, I'll get my ticket again, at the end of the lecture is definitely not solved yet. It's not an issue that has just been sorted out on a scientific basis or on a political one. And there will be much more discussion about this in the next decade or so maybe in society, in politics, and maybe also among you as experts. Certainly, you'll sooner or later have to learn about legislative requirements. For Fairness, once they have been implemented. The European Commission is working on an EU EU AI act. And the new coalition contract actually contains several sentences about the need for AI regulation. So how do algorithmic or formal solutions to fairness actually look like? That's the thing I'm going to do for the final half hour of this lecture. And so what I showed you before we looked at label faces in the wild was that an AI algorithm PCA just maybe like the simplest algorithm you can think of can be biased towards or against certain groups. And we I provided a very pedestrian, very ad hoc answer on how to make it fairer. Turns out it didn't quite work, but it helped a little bit in the right direction. Of course, ideally, you want to have a solution that doesn't just quite work, but one that guarantees that the algorithm is fair afterwards. And one that in the process of doing so absolves you of the responsibility to build a fair argument. And you don't want to be in this ethical bind, that someone may come later and say, Oh, your algorithm is biased, then you have to say, Hey, I noticed and I tried to fix it or do this thing, like wherever we scale the images, and then you have to explain maybe to a judge how you did this and why you did this and how it's the right thing to do or not, because it might turn out it's actually a problem. So you want to have a clear definition of how to make an algorithm fair, so that you can go in front of a judge and say, Well, I just said what the textbook says. I just did what the textbook says I did, I implemented this and now I can guarantee So this is the property that the algorithm has. So to do that you first need a definition of what fairness actually is. So what is a fair algorithm? That's a tough question, actually. And it's particularly tough for continuous algorithms like PCA, which do something really complicated, write it, take a data set. And then they project onto some low dimensional actually even a high dimensional basis, and do something with the data linear projection. So in your homework, you'll see how one can define a notion of fairness on such algorithms. But that's actually a really tricky thing to do. So let's step take a step back and think about the simplest possible setting you could have. And what kind of fairness notions one can define there. The simplest possible setting, I would argue, is classification. It's also maybe the most pressing thing, imagine you have an algorithm that takes decisions about people binary decisions, it labels them as in or out, it decides that people get a credit, or they don't get credit, that they get accepted to university or they don't get accepted that they get a job or they don't get a job and so on. In such settings, you want to be sure that the algorithm is fair. So these, this setting is typically formalized in this way. So we have to think about four different kinds of variables. The first one is x. So that's the feature a person has, it's how things are described to the algorithm. And some of those features are unproblematic, and some of them are called sensitive. So we'll call it we'll separate them out and say there are sensitive attributes a and other features x, or the task of the algorithm is to use both A and X potentially, or maybe just x, if you have to think about that, and take a decision about these people, it will have to provide a label, maybe it's a binary labels 01. Maybe it's a score, like a number between zero and one, a continuous number, a probability, a classification algorithm, right? And that those labels, they should probably have something to do with some training labels, right. So for example, you may have a data set that contains entries of a database of people who were given a loan by a bank, and then they did or did not repay that loan. So that's why they paid back or they didn't they defaulted, or they didn't default. And our algorithm is supposed to predict whether a new person with new features is going to default or not default on the loan, and therefore should be given a loan on. Okay, so in general, of course, all of these variables depend on each other. So I'm sure not all of you have seen directed graphical models yet. But if you have, then you might notice that this graph is fully connected. And so therefore, it doesn't mean anything, when you connect the graph, so I'm going to just say that there is a joint or redistribution of those four variables A are x and y. And if you're like me, the first time you see this, it's just too many variables to think about. So let me go slow through them. Again, there aren't sensitive attributes, they might be, you know, gender, or background backgrounds, ADH. And those, what essentially it will actually is, is hopefully not something you have to decide, but something that society tells you, or maybe actually the law housing. On that end, by the way, America is much further along than EU, there, it's very clear what attributes are to be considered. In US parlance, it's usually you know, race, age, gender, and so on religion. So in Germany is a slightly different, partly because we don't like to talk about race in Europe. And in France, you couldn't possibly have race as a feature. This is not allowed. But this might be something that is stored in here. So that's a question of the chest x and h are disjoint. Yes, so you could think of just for the sake of the discussion, you can think of in excess disjoined. But it's actually going to turn out to the bone metal. So what the algorithm will do is it'll operate on this set of variables, it will typically use x as an input, maybe a as well, and return a score that hopefully has something to do with y. And now, if you just sit back for yourself and think about what a fair decision would actually mean, you may come up with different definitions of it, I could ask you for it. But I also have already written some of them down. So I don't ask I won't ask anymore. One first thing you could, you could think is that what maybe the decision of the algorithm should be independent of this underlying secret activity for the sensitive, right? Maybe this is to be independent of someone's gender. Another thing you might want is that the algorithm works equally well for both groups, or for any group, right? So if we have, you know, men and women in our data set, like we had an example, you might want to make sure that the algorithm is wrong. People often are menaces. It's kind of a binary version of the problem we just discussed in little pieces in the way I'm going to think You might want to and that will seem a bit confusing at first we'll get to it in a moment you might want to have is that a score R should capture all the information that is in a, in the sense that the label the true label, y becomes independent of the sensitive attribute. If you look at the score, that's going to look confusing, we'll get to it in a moment, I'll tell you, maybe a first a third thing you might want is that your algorithm should just work. Right? It shouldn't be evil shouldn't do things that are somehow wrong. Right. Okay, so someone said, excluding sensitive labels from the data does not mean they're not discriminating against said yes. Okay. So that's an important thing to say. So let me do the boilerplate thing and say, The right answer is not to just throw away, because there could be things in x that are correlated with a, so the algorithm has to be biased, even if you decide to just hide a from the algorithm. Right? So that answer is definitely wrong, you can not just think I'm just gonna, I'm just gonna delete the column in my, in my database, that is called Gender and then everything will be fine. Right? ignoring a problem doesn't make it go away. Okay, so let's look at those definitions a little bit further. We'll do that quickly. So let me just recall something I said in the first lecture, because we are going to use it completely now, right. And so two sets of variables a and b are called conditionally independent, given a third variable, see, if you can write their joint conditional distribution as a product of two separate distributions. And we can, you can use the product rule of probability to immediately see that that also means that the conditional of A given B, and C is the probability for a given c and the same for B. Of course, vice versa, just as a quick recall. Now, maybe you may have noticed that, whoops, sorry, if you look at this, this graph, there's essentially three variables here that you can think about x is just there, right? It's just a data set, there's not much we can change about it. But basically, there's a y and argue the algorithm is reacting to like it's trying to predict y. And it's somehow in relationship to R. And what these statements down here actually are, essentially just all possible combinatorially forms of independence. On conditional independence, you can construct for r and y in context to A, they're just labeled it just have a name. And we can just check what they actually mean. So the first one is called independence. So a classifier are a binary is an algorithm that provides a binary label for a bunch of people that have sensitive attributes, A is called independent of A if Well, if it score and the sensitive attributes are independent of each other. If R is independent of r, or equivalently, is r is independent. Okay, that's fine. Right. And that's maybe the most obvious thing you can use as a definition of fairness. It just means that the, the, the, the decision that the algorithm takes is independent of the group membership. In particular, this means if you think of both aren't as binary variables, and you can kind of convince yourself that that means that the probability to return the binary score, yes. Given one group has to be the same as providing the binary score. Yes, given the other way. Right. That's literally what independent means. So that means the another way of phrasing This is to say the algorithm would accept people for both groups with the same probability. Okay, that makes sense, right? So here's another definition separation, separation is a technical term that means a classifier is a satisfies this notion separation gives, the score that the algorithm returns is independent of the sensitive attributes, given the label the correct label for this person. So if you know whether this person will be paid at all or not, then the decision is important that if you know, but if you consider the groups of people that we pay and don't be paid alone, then the prediction of the algorithm becomes independent of the the group membership Yes. Yes, it could. And we'll get to that in a moment. But if if your class if you build a classifier in such a situation, and it doesn't fulfill this, that it doesn't fulfill independence, right? It's just a formal statement gives you a requirement. And if your classifier doesn't, doesn't fulfill it, it doesn't fulfill this requirement, and then we can discuss about whether that's a good thing or not. So you can convince yourself actually, I'm not going to do this because of time that this separation statement can be in particular for this binary setting of two groups and two possible labels of the classifier it'd be represented by such a table. So that means the probability for the classifier to predict except given that the true label is accept and the group is a is the same as if the group is B. So that means the correct assignment right, so predict one given the true is true, while one is equally likely. For both groups, the true positive rate of this algorithm is the same for both groups. But it also has to hold that the probability to return one if y is actually zero is the same for both groups. So that means the false positive rate is the same for both. Remember that you can plot false and true positive rates against each other, and then you get an arrow C curve. We've mentioned this before and previous lecture. So the arrow C curves, essentially for both algorithms have to be the same for them to be considered separate, of satisfying separation. So this means we want an algorithm that performs equally well, for both groups, its decision may not be independent of whether somebody is in one group or another. But it may use the group membership in such a way that the chance of getting it wrong is the same. The third criteria is called sufficiency. And it's just the final remaining combination of independence that we haven't considered yet classifier is called sufficient if that label is independent of the class membership or the sensitive attributes, given the score of the algorithm. That in particular means for binary labels, and binary group membership, that the probability to be actually y, like positive to be labeled positive is the same for both groups when we condition on the score that the classifier assigns to. And you may notice that there is no nice sentence underneath here, because it's kind of complicated to translate this into a sentence that is easy to explain. Why is that? Well, here's another slide to think a little bit about this, like notion of sufficiency and what it actually means. So you can already think kind of look at this in the binary case, you can kind of notice that this means that the the probability for Y has to have something to do with our basically right, so the score of the classifier has to be a good predictor of y. Or at least it has to be kind of good in the sense that it's prediction makes that the true label independent of the underlying group membership. A way to think about this is that sufficiency is actually just another term for calibration. So I mentioned that the three slides ago, that's the fourth category, you might want this just that the algorithm works, that it just predicts well. And that's actually it turns out what sufficiency is. So here's why. So just use the definition of sufficiency. Again, it's just a copy from above. So calibration, could mean a property of the score, or it could mean that is we might call our calibrated if the probability to observe level one given that the score is r is actually just on. So if R is a real number between zero and one, then that's maybe a good property for a classifier to have, right? is the probability to see maybe one is actually the probability that the classifier collides? Or turns out that if a classifier has the property that it's calibrated across groups, so that for each group, no matter what they are, it predicts the correct label with the correct probability. Then a classifier that is calibrated by group satisfies sufficiency. But I mean, you can see that there is no in here, right? It's gonna be just video, but actually works the other way around as well. So in fact, if it turns out that's not straightforward to show control, but if there is a score are that satisfied sufficiency, then there is a function that maps from little arm to some other than L of R, that satisfies calibration by and you can actually learn such a classifier. So it turns out that actually how you do this, how you achieve efficiency is you take a whatever your classifier is, I'm just going to throw this out without explaining it further, because it's not that important. And you just consider a four binary classification set to just consider a logistic output function with two different like a bias and a weight term for each group. And you can fit those such that the classifier is always satisfies sufficiency. So it actually turns out that sufficiency is just a term that it's very convenient for the machine learning engineer, because a classifier that satisfies This fairness criterion of sufficiency happens to just be one that works well. So, the reason I put this into the slides is if you ever see a company tell you that they've designed a classifying classification algorithm. And it's fair because it satisfies this notion of concurrent sufficiency, and cites a bunch of famous papers about fairness, folds efficiency, you can go with that fine. But it doesn't actually mean anything. It just means that your classifier works. Right? It predicts the outcomes correctly. But that doesn't mean in any sense that it's fair in the sense of either independence, or separation, right, that it's equally good for both groups, or its decision is independent of these groups. In fact, it turns out that it's impossible to be any of these properties of fairness, at the same time together. So there are theorems that are actually taken from the standards and available mailbox, let's see that all of these three different kinds of fairness are mutually exclusive. So a set is assuming that the the sensitive attribute a and the true label y are not independent, this is actually what you asked about before, right? Assuming that the probability to have this property has something to do with being in one group or another, then it's not possible to be sufficiently independent at the same time. So if you decide you want an algorithm whose decision is independent of agenda, for example, you cannot expect that algorithm to be worked equally well, for many women. Separation and independence are also rigidly exclusive, or under some mild assumptions. They don't, can't hold simultaneously. So it's not possible to have a classifier, oops, sorry, that works equally well for all groups without explicitly considering group membership. So you know, it's impossible to have a classifier that cannot that is independent of attributes. And finally, separations efficiency are also mutually exclusive, actually, you'll be able to show this in your own gdb. Example. So if you're, someone tells you, your boss tells you make sure this classification algorithm is fair. You don't know which of these fairness criteria to use. And you can't just pick one, because whichever you pick, will automatically exclude the other ones. So you'll have to ask whoever is giving you this task to tell you whether you should have a classifier whose decision is independent of whether somebody is male or female, or member of certain other protective boots, or whether they want to make sure that the algorithm works equally well for both. And you could imagine that there are different settings that call for different such requirements. For example, if we want to take if you want to build a legal legal algorithm, and what maybe we shouldn't be doing it in the first place, right, but maybe we want to build an algorithm that decides whether someone should be in jail or not, or whether someone should choose a slightly less problematically, like maybe if we want to decide whether. So in keeping, there's an algorithm that decides which kids grades go into which kindergarten, maybe you want to make sure that the decision that his algorithm takes is independent of I don't know income of the parents. That's maybe a setting where independence makes sense. In a medical setting, maybe you want to build a algorithm that diagnosis certain diseases. There, maybe there are diseases that affect men and women differently, right. And then, of course, you want to have an algorithm that works equally well for both groups that doesn't accidentally you end up training it on men, and it ends up not working for women, even if it's decision actually is nominally independent of gender. So it turns out, politicians know as well. So in this rocket commission report, there's actually a sentence about this as well. And for the need for politics to act on this. They say, they're roughly two dozen mathematical formally, which can define fairness or measure fairness of algorithmic decisions. So they did a better job than me, they didn't just list three, that is like, they know that there are something something like 24 Roughly, and basically go on to say, Well, it's time for politics to act right and say, which of these requirements we actually want? Because the computer scientists can't decide it's not it couldn't be our job to say, Oh, we have to do it this way or the other way, right. Okay, so I'll let you this to discuss this on your in the chat. So, okay, nice, final 10 minutes. It's just basically a like two things I want to do. So that this the goal of this part of the of the lecture was to point out that a if you want to have an algorithm that guarantees fairness, you first have to say what fairness actually is. And it turns out, it's not so straightforward to do that because all or Three of the possible definitions you could come up with mutually exclude each other in general. I mean, there are special cases where they really are the same. But those special cases are usually corner cases, extreme cases that aren't true in practice. Yes. Very good question. So for those in the chat, the question is, these are strict requirements, but they are equality constraints, is it possible to provide relaxed versions of this fairness criteria? So that for example, we could say that the correlation coefficient between these two variables a and r are in a&r should be less than, you know, open to also, right. Yes. So there is actually a lot of research on this. I said before, where there's, there's a large research community that studies these kinds of issues. And it turns out that so actually, the story there just gets gets even more complicated. So depending on what what kind of message you want to send, you can send it in both ways. So it turns out that, yes, if you relax fairness criteria, then it's possible to find solutions that satisfy both relaxed requirements. But then you can construct settings that are subjectively unfair that most people would look at and say, Ah, that's really not what should be happening, that fulfill this relaxed version of that. And also, I think this is a little bit like, you know, this is a political decision, right, that we're talking about here. And I guess we know from other parts of democracy, that it's sometimes important to have bright white lines, to not just say, you know, we want to somehow, you know, work it out, set that number less than 0.05 or something. That That doesn't really work for legal scholars. Okay. So, with that, I want to, I want to end on a final point, which is, right, so we said, okay, so we started all the labor faces in the world, we noticed there is a problem, and we try to fix it by hand, we figured out that actually, you can do something about it. I like fiddling with the data. But it seems a bit dangerous to do that. Because now we've like loaded blame on ourselves for fiddling with the data. And actually, we didn't perfectly solve the problem. So we realized we have to define what fairness actually means. I didn't define fairness properly for PCA, because in your homework, instead of defining four classifiers, because those are simpler. And we realized that actually, those definitions mutually exclude each other. So we can't do all of these fantasy bacteria at the same time, but maybe we can decide to use one of them. Then the final question is, how do we actually enforce those? How do we actually create fairness in an algorithm? Now, that's definitely our job as computer scientists, right, assuming the politicians and the legal scholars have told us which kind of fairness they want? How do we actually realize it? So I'm going to pick out, actually, so there's, there's a lot, there's an overarching recipe, that is a pretty like basic classification into three different kinds of things you could do, we'll do those first. And then we'll talk about explicitly how you do one of them in one case. So and it's fairly obvious one of these ways up, this could also be like a, you know, economics class, if you think about how you could do a certain thing is a pretty obvious way of thinking about it. Given that you're thinking about an algorithm that takes data, and then does something to it, to do some analysis on it, and then produces a prediction, or classification, you could intervene on all three of these things. Right, it's kind of a tautology to do. So we could decide that you could change the data before we hand it to the algorithm. So imagine you're thinking of your classifier as a black box, like we already did in some of the lectures, right? You said, I'm going to do linear discriminant analysis or logistic regression, some fancy convolutional neural net data classification. And then we could say, well, if you bought very unfair, we could try to first pre process the data, such that certain properties of it can be for example, for independence, we could take the data set, and just put them next to each other a and y. And just make sure that the nyo independent from each other, do that by pre selecting data by randomizing the labels in the training data such that they become independent. But the nice thing about this is that if you do it, right, we can be we can be sure that no matter which order we want to apply afterwards, let's progress regressors. And that is always going to fulfill this property because there's always going to be the bad aspect of it is that if you do that we change the data. So we actually noticed this in PCA, because it could go someone said right you could go in a cut out all the badly classified data data points are classified but badly compressed ones. The downside of this is that we actually actively change the data. So if there is information content In the data that might be useful for some task, we've just removed it. The next thing we could do is we could take the algorithm and fiddle with the algorithm, so that it is guaranteed to always fulfill whatever our finest materials. So the homework you will do this week, if you do recording exercise is a case of exactly this. You are? Uh huh. You think the microphone is the wrong one? Okay, let me see if I can fix this. Yeah, now you should be able to hear me better. Thanks. Okay, so we could change the algorithm, right. So that's what you'll do in the homework, you'll take PCA and actually change PCA explicitly as an algorithm such that is guaranteed to fulfill some fairness criteria, it will turn out that in the homework, you do this by redefining what PCA does not to minimize a quadratic problem, or reconstruction error, but to minimize that quadratic reconstruction error, subject to some equality constraints. And then it's not a simple SVD anymore, but it's a semi definite program. But there are tools for that as well, that you can just call just like you could call sai pi dot Lin, I thought SVD or NumPy, Lian SVD, they are corresponding tools that you can use. And in a moment, we'll talk about how you could do that, how we probably could do this for this much simpler criterion of separation as a final thing. And that's gonna be the end of the lecture. And now I need to wake up my pointer again. And the third thing, of course, we could do is to post process so we could run our Black Box algorithm, whatever it does. And then at the end, you could add like a demon that sits at the output of the classifier, and just checks whether that classifier is bad properties, and guarantees that it sometimes flips the output around such that the benefit period is fulfilled. upside of this is that it's it treats everything below as a black box, you could not you can, you can think of it as a piece of code, right? That really comes at the end, you've just if someone has an implemented machine learning pipeline, you can just take the output of that and add like a fairness block at the end. That guarantees that you always get fair output. Unfortunately, as you'll see, in a moment, these post processing steps, they tend to be very damaging to the quality of the classifier, they'll typically involve reducing its performance, such that everyone, the algorithm performs equally badly on everyone, rather than equally good on everyone, because it's very hard to take a classifier you spent a lot of time trying to get right, and then make it even better for some groups. Instead, what you typically do is you take the group that is going to be advantaged and artificially make it worse Yes. I think I'll give you the example that might help clarify, in which census isn't the case. And I just want to point out because it's very important at this point to do this. Thankfully, the Ephesus aren't here today, but we have distributing sometimes come by. It's very important, why not? There's a fourth thing you could do. Who can guess what the fourth thing is? Maybe it's not too obvious for me to say it out loud, you could just not do it. You just decide that this is an application of machine learning, I'm just not going to do. It's just stupid, right? So if your task is to build a, you know, if the mayor of tooting and comes along and says can you build me a facial recognition system that I can put up in this market spray of tubing and to detect whether I don't know, some, someone I don't like is there. You could just say, Nah, that's not what I'm going to do. It's just not an ethical thing to do. And this is very important to point out, it's a very fluffy thing to say, it's easy for me to say, because it's not my problem, right? I can be the professor in the lecture hall saying I should maybe do this. Because of course, if you do it, it might just end your career, or at least your current job, right. But sometimes there may be settings, maybe tasks that are put to you in whatever future professional environment you're in, where you may have to actually raise your voice to say, Come on, this is stupid. Maybe you can phrase it as you know, the media's gonna shred us to pieces. If you try and do this, if this ever comes out of what we're doing here, that's going to be super bad for us as a company or as whatever, wherever we are, right as an institution. It's important to keep that in mind because sometimes people like get into their headspace where they can't like get out of it, and they just don't realize that this is a potential option. So let me finally show you a little bit overtime. Sorry, I'll quickly tighten this up how you would do this post processing or actually algorithmic processing step it's kind of sometimes the lines are a bit blurry between the two for This separation algorithm not for independence or independence is actually more straightforward. But for separation, because it might also help explain, like review what separation actually is. So I told you on a previous slide, that separation means a classifier, which returns a score are is satisfying this property called separation. If for both groups, in this case, we assume that there are two groups that might, of course, be more groups or groups, the probability to return score are given that the label is one is the same across both groups. And even that the label is zero is the same across two groups. So that means the false positive and true positive rates of the classifier is the same for both groups. So, I already mentioned when I bought this app that this means the RC curves of the classifiers must be the same, but actually an RC curve that was maybe not quite viable. I said, right. So here, here's a setting with two different groups. Let's say this is Group A, this is through poopy in so let's, you know men and women. And in both of these groups, there are subgroups, right, we satisfy some other criteria, they repay their loan, they don't be paid alone. Right. And there is a difference in the distributions for in between both groups. In this case, actually, the expected values are the same, but the I think the variance is different. So as you pointed out before, right, if the groups are actually the same, then we don't have to worry about this. But if actually, group membership has an effect on how we should predict, then that will not be satisfied, typically. So a classifier for this setting, what is a classifier, but it's literally just a decision rule that takes the X and returns y. Right? So you could basically, you could just decide whenever x is larger than something, we'll return the no label or dot label that's classify it, this is like the most trivial classification problem you can think of. So that's it. And now you could have a classifier that does the same for both groups. Or you could have a classifier that is different for both groups, it takes a as an input, to say, if the person is male, and X is larger than whatever, then it turned positive in if a person is female, and X is larger than some other numbers that are returned positive. If you do that, then depending on where we put those thresholds, you'll get a different true versus false positive rate. So here in green is the I don't know the group, and in blue is the age group. And because the only thing we can do is move the classifier around, right, you just you have to be on one of these curves. So if we want separation to hold, you can do two things. One is you could look at this curve and decided we're going to take the point where they intersect. And that's our classifier, ensure separation. But that's a bit stupid, a because it might be anywhere. And in fact, there can be settings, you can play with this yourself where those curves never cross. It could just be right, she mentioned that in one case, the two groups are clearly separated, and you'll get a sharp painting, like the corner segment. So that area and the other two groups, if you are very close to overlapping, you'll basically get a straight line. And then they'll never cross. But we can create a classifier nevertheless, that achieve separation as long as we make sure that we find a classifier who is true and false positive rates lie such that they lie in the convex hull of these two curves. So they are both below the blue and the green curve. How do we build such a classifier? Well imagine you could have so you could have a classifier that is called a random classifier, it takes an input. And then if you turn it with probability R plus, it doesn't, it doesn't care at all about the inputs, right? It's a one line Python code, that classifier if you choose the label to be y positive with probability R, it lie on that straight line, right? And lie on the straight line for R right, because the false positive rate will just be up. So now we that means we can take for example, if we want to, sorry, no, if you lie on this, the true positive rate will go up and down depending on what our artists right. And so now what we could do is we could we could decide that we want to be kind of here on this point. By taking whenever we are in the blue group, we take either a decision that the classifier from the blue group wants to do in a deterministic fashion, or with probability f, we take the random decision. And then what that will do is it to the sort of to a linear interpolation between the random classifier on the diagonal and the blue classifier. And that is a way of pushing the behavior, the performance of the blue classifier down until it's the same as the green classifier. And now we have a fair algorithm. And we achieve that by taking the group on which it's easier to fulfill the task and destroying the behavior of the algorithm the performance of the algorithm until it's as bad as the worst group And that's also maybe why separation shouldn't be the only fairness criterion to consider, right? Like in a medical setting, you don't want to do this because you'll just act, you'll just actively harm people who could be treated better, just because there might be someone else for whom we don't know how well to treat how to treat them. But Okay, with that, I'm at the end, sorry, I'm a little bit over time. So algorithms affect human beings. And that's a special task, or a special challenge in machine learning, that is maybe not so prominent in classic computer science. If you built algorithms that take data as an input, and change that behavior as a response to the data, then, and then take decisions about human beings, then the fact that they take decisions is a problem. And the fact that the data enters that decision may be cause of a problem as well. Typically, the source of the problem, formally speaking, is the data because it's quite rare that someone maliciously writes code that actively harms people. I mean, if you do that, then that's illegal anyway. But that doesn't mean that the person who wrote the code you is completely detached from the problem, you still have to think about, you have to make do, you know, due diligence to look at the data set to check whether the behavior of your algorithm to ensure that it at least works sort of to your own personal satisfaction, and maybe also to fulfill certain legal requirements for fans, those haven't yet been properly, in any in at least not very clearly put into law, but that's soon going to happen. And then you might have to do that. Anyway, it turns out, there are several different definitions of fairness, unknowingly, they aren't mutually exclusive. So typically, it's not possible to be fair in all possible ways. And if the public tells you, if you have a conversation with friends who aren't computer scientists, with AI, we should just build fair AI, then you can tell them that the story is a little bit more complicated, and that there's a difference between algorithms that are that take a decision independent of class membership, and one that works equally well for all groups of people and many other forms of definitions of fairness. Okay, please provide feedback for this lecture by pointing your phone at the feedback QR. This was, of course, obviously, the final lecture before the Christmas break. So a few admin things, there will be one more tutorials or two tutorials on Wednesday, but of course, no tutorial on Thursday. So if you are in a tutorial that usually happen is you don't go on Friday, sorry. So why not? Various Thursday and Friday, right. So if you are in the tutorials that are happened on Friday, then you can either skip the tutorial this week was not going to happen, there won't be a replacement for it, or you could decide to join one of the earlier ones on Thursday. And then there's the Christmas break, I hope you all have a relaxing break. Despite Omicron and everything that might happen, we'll see what the world is like at the 10th of January when we return. Who knows I I personally am not going to take the decision to just do the lecture remotely, but the politicians might for us. So if Teresa Baba decides that he can't teach and presents any more than you might see each other virtually otherwise, I am going to be back here in this lecture hall on the 10th of January unless I've got COVID Just like 30% of the population by then. And then I like my current plan assuming the lecture is going to take place is to do a bit of a special lecture a lecture for future if you like on the in intersection the intersection of the climate question and the AI question. If you want to invite additional people bring them along or put them into the Zoom call if they don't want to come to the lecture hall. And and yeah, and then we'll also start your projects. So this week is your final homework that is a piece of just a tutorial sheet. On the 10th of January, we'll talk about how you do your projects for the rest of time. Okay, thank you very much. Ah, so someone says the feedback isn't okay, I'll have to activate the feedback. So just stay in the call for a second play or stay in the on alias for a second don't close the page yet. I'll try and activate it sorry about that that's stupid I am trying to fix this just a moment Okay, so the feedback is now activated. So please try and reload your feedback sheet in a right now All right, thank you all very much and happy holidays 

