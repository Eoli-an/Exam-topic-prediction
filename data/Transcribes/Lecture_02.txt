am people on Zoom? Can you hear me? Yes. Perfect. Thank you Of course it was there we go. Are young Okay, kinda table from us. Hello and welcome everyone to the second lecture of data literacy. As Philip told you last week, he is away this week. So he asked some of the tutors to take the lead of the lecture today. And here we are. I'm Amelia And there is Lucas, we are both PhD students in Philips group. And we will also be tutors for this course. So we might meet each other at some of the tutorials. So it's been a while that we have done things in presence, I think for me, or at least let's have a look at since we both started our PhD during the pandemic, it's the first time we have the pleasure of do such a lecture in prison. So I'm very happy to see a lot of faces here today. And of course, we would like to do this very in an informal way. So please don't hesitate to ask any kind of questions and interrupt it will be very, you will also be very participate participate actively in this lecture as you will see later. Okay, so this is we are deadly to selection number two, and it is about collecting data. Lucas, does this thing work? So I have to turn it on somehow. Was it working before? I cannot move it from here. I mean, it's not a problem, but it would be nice to have a great start. Just mainly because we are on zoom here. Is it possible that the fact that there is sheets to be working check again. Here is working great, thank you. Okay, here we have a rough course outline. Probably you saw this slide already last week. To sum up maybe what we're going to do so when, when how the name of the lecture suggests we're going to deal with data. And the process of working with data or doing studies can mainly be summarized in three big parts. The first one is collecting data. The second one is estimating data, which includes finding structuring data constructing estimators and make predictions. And then when you've done your study and your experiment, the third part is asking yourself questions or ethical questions or fairness questions regarding the experiments you did. And this is what you will see at the end of the lecture 711. And the last part, the last elections will be about the silent techniques of constructing your experiment. So since we are here at lecture number two, at the beginning of the lecture, today, we will talk about how to collect data. And collecting data is indeed a very important part of the process. And it's perhaps one of the most important parts because by in the process of collecting data, you will also learn a lot about your data. And it is at least it's also the least formal part because there are not some clear objective rules about how to collect your data. But today, we will try to see what are the things that you have to pay attention when you when you collect the data for your study, or experiment. And, as you can see, and so there are mainly two points first how your data is collected will affect the results of your analysis. That's why it is very important that you that you do this in a very careful way. And this as a result, even if you're not actively involved in collecting the data yourself, you should try to know as much as possible about how the data was collected. And so today, we will learn how to design design data collection in observational studies. So what are observational studies, a little bit of definition, this is maybe a little bit of a tedious slide, because it's a lot of definitions, but that's what we're going to need for the lecture today. So data normally comes in is a set of pairs of x and y's, I'm sure you've seen this already. Where these are presumed to be related by a functional relationship. So y is equal of f of x. So in general, you've done this for sure when you do inference or when you want to do regression, what you try to do is to learn the function f so that you can that you know the relationship between x and y so that if I give you a new x you will be able to predict for a new wife and this variable sets names in statistics, x is known as the independent or controlled variable, whereas y is known as the dependent or the response. You probably also heard the word cause and effect, but causality is a very delicate word in sarcastic statistic, and you should be careful to use it. So, in this course, we will just talk about dependent and independent variables. And we assume that the set or at least the set index, I have the set n comes from a sampling distribution and sampling distribution is a distribution that you use for selecting samples from the population. In fact, ideally, you would like to do the your experiment on the whole population, but this will be practically not possible. So you will have to somehow select a sample from the population and for this you will need a sampling distribution. This is for what the variables the name of the variables are. Whereas the experiments can be divided broadly in three types. The first one are observational experiments. which are experiments where you can you have no control over the values of x. As the name suggests, it's just an observational experiment. So you just observe some data. On the contrary, on the contrary, active or control studies are where you can choose the values of x. And this will include choosing a sampling distribution, we will see this today how to do that. Then a bit more specific are interventional studies. interventional studies are when some participants can't get an intervention. So you do an intervention, some participants of the study to see the effects of this intervention, more formally, are experiment designed to establish causality with the mechanism f itself is changed. Try to think for example, if you want to evaluate a medicine, you would want a standard procedure is to choose a sub sample of your experiment where to give the medicine to see what the ethics of this medicine are, this is a very standard procedure in clinical trials. And then maybe the last category of studies are known as meta studies that combine studies have combined different studies that were already done, and combine the results. And are all studies that try to answer the same question. Nice, these are a lot of nice definition. But let's see maybe an example. So if I give you the hypothesis, let's say that we want to test the hypothesis that the vaccine vaccine tried to think about the COVID vaccine or the polio vaccine prevents the infection in C percent of the people are percentages of people. So if I want to do the experiment, what would you say? Is the independent variable? Or what are the variables here? And what is the relationship between them? Do you have any ideas? So you might think that one variable will be the vaccine and the other one is if you're if you have the infection or not, do you see a clear or something that might be a dependent or independent variable between the two? Yes. Yes. Very good. This is quite like intuitive that, of course, like the vaccine affects if you have an infection or not. So in this model, we would say that the vaccine is independent variable. And what could be a possible intervention for this experiment? I already suggested before actually. Or intervention in a sense, like for interventional study, as I said before, it could be that, for example, you select a sub sample of the of the people you're of the people you're doing the experiment, and you give some the vaccination and some a placebo to see what the effects of the vaccine are. This is special what they did also for the COVID-19. trial to trial, and what is what could be the sampling distribution for such a study? This is maybe a little more tricky, because theoretically, this would be the entire population. I mean, theoretically, we know that a lot of people have been vaccinated for example, COVID-19, and their data is somewhere there. But in this case, even in observational studies involves some kind of selection, because this will be practically practically not possible. And we will see this later in the lecture how we can learn how to sample from the population. Okay, another example here, this is a quote from a little bit refrains from the shea butter stock blog, which is a local newspaper, it's a bit outdated is the during the first wave of last year, May 2020. I'm going to just sum it up. So there is a local lab medical lab, which is offering work in tests for for antibiotic tests of COVID-19. So basically, for a little bit of money, you can enter the lab, and you can test whether you're positive or negative with COVID-19. And what the lab claim is that after or at least, what they told the authorities he said after the first nine days since they the service opened, 1000 737 174 people were tested, of which more or less 10% of the people were registered as positive or either positive test result. So on the same day, what the health authorities in tubing suggested was just 0.6% of the population of tubing was affected with COVID-19. And since you might notice the discrepancy between the two numbers 0.6% and 10%. What the lab concluded was that that percentage of the infected people in 2 million were more or less 17 times bigger of what they have compared to what they observed. And you may agree that presumably this is the case because 0.6% times 17 is 10.4%. So what asked this in this type of study, what what type of study is this? If you really think of the three types mean, the type of experiments that I told you before? Is it an observational study and active study? I interventional study. Yes. Very good. Yes, it is an observational study, because of course, you don't get to choose who, on whom to do the experiment who's just wait there, and you have people that come to take a test. And what is the sampling distribution? I actually basically already said this. Yes. Exactly. Are people who come and take the test? Do you think? Do you think this is the I mean, of course, this is an observational study where people don't take the test. But do you think there might be a better choice for something distribution? Or is there something that maybe is a bit skeptical about using this sampling distribution? Yes, people that are? Exactly. Very good. So you very good. So you might agree that if someone goes and takes the test, my somehow affect the fact that they are positive or not, because there is a reason why they go and take the test might be that they have symptoms might be that they had a contact with someone that's had COVID. So this is something that intuitively is true. But what we can do is that we phrase this things that are intuitively through with the language with mathematical language, or with the language of probability theory. And so for that a little a little recap from last week. I think you saw this already last week. But I will go a little bit quick through this slide, which is just a reminder of what you saw, but please interrupt me in case you want clarifications. So when two variable x and y's, we can define the joint probability, okay, the joint probability P of xy and in this case, the variables x or capital and the realizations of the variable are denoted as a small letters. And we can define the distribution p of x, y, which is the probability of x being equal to x and y being equal to y. Once we have the joint distribution, we can derive the marginal distribution. And the marginal distribution is derived just by summing up all like for one variable is it it is by summing up all the values of the other variable, in this case, I just sum for all the values of y and y of t x, y and negative effects. This is in the discrete case. But in general, if the variable is continuous, you can think of it as it's this to be as an integral. Then very important, the conditional probability like given two variables, x and y, you can define the conditional probability of p of x given y, which means what is the probability of x being equal to x, given that y was equal to y. And this is defined by this formula. And the conditional probabilities invites theorem, which is probably the most important theorem in probably are in Bayesian statistics for sure, as the name suggests, and maybe in a nutshell, it is the by assume describes how to compute the posterior probability on x, which is the proportional to the likelihood for x, which is this one in blue. The likelihood for x because it is seen as a function of of x, I try to think about it as how probable it is to see y as an effect of x as x x by varying times the prior of x and the prior is a prior belief on X before you get to see the data. And then on the numerator in green, you just have the evidence term for x, which is a normalization constant constant to make this integrate to one. Good. Once we have this nice definition, now we can see what is what it is for two variable to be independent, x and y, as I said, independent if and only if the joint distribution can be factorized into margin distribution, that means that p of x comma y is equal to the product of the two marginal distributions of X and Y. And from that follows that P of X given Y is equal to p of x. And this, this is quite intuitive in the sense that if x and y are independent, then knowing that y happen shouldn't affect the probability of x. Because the probability of x given that y happened doesn't affect it. So it is equal to the probability of X. So this is quite intuitive. Do you have any questions? Please feel free to interrupt for any kind of question. Good. And now that we have this definition, we can actually rephrase what I said before with this notion of independence. In fact, what the problem is in this in this experiment, is that the variable being infected with COVID-19, as you said before, it's not independent from the variable going and get a test. So knowing that you go and get a test may affect the variable being infected with COVID-19. And what the study does is something from the proper sampling distribution with this probability of being infected knowing to get a test instead of sampling from the general probability of being infected. And this may result in something bias. So bias is a word that is commonly used in our daily life. But here it has a very specific technical meaning and Lucas will talk more in detail later about the mathematical meaning of bias. Think about it, maybe for now just the thinking that you're estimating something that on average is correct. And sampling bias means that you're taking the sample in such a way that a part of the population is more likely to be taken than others or some parts are less likely to be taken others. And to avoid sampling bias, actually, what what the experiment should satisfy is that the the experiment that people use for the experiment should be representative for the population. So mathematically, I say it's in the formula that the test quantity you care about an experiment should be the same about the test the probability of the test quantity in the population. In other words, you want that the frequency of the quantity of interest in the sampling population is equal to that frequency in the experiments. So the feature you care about in the experiment should appear with the same frequency also said probability of features in experiment is equal to the future features in the population. And we can say this also a bit more in a mathematical way, precisely, what you want to do is to take IID samples from the population. I think you've probably heard this term already, but we can see it here again. So what does IID means independently and identically distributed. So I independently as I said before, it means that for every 2x I and xj that you choose from your from your samples from your n samples, you will you can split the joint probability distribution as the product of the two marginals then identically means that they all they all have the same distribution. So the probability of x being equal to x, we'll also be able in the experiment will be equal to the probability of xj being equal to x for every i and j that you choose in your n samples. And from the population I said before is that the sample from the experiments like the sample that you choose for the experiment is representative for the population. So the probability of X in the experiment should be equal to the probability of X in the population. And this is important because we I IID samples, we will be able to construct unbiased estimators that are set before our numbers that are correct on average, and Lucas will speak about this a little bit more in detail. So this is yes. Do you mean that we don't even know how many people Yes. Yes. So, we are trying to estimate the quantity Well, I mean for for taking, then it will be another sample probably right. It will not test on population, but we will be constructing an experiment to estimate the quantity I think it will be another type of experiment and you will have to choose the can you make an example? For example? Yes. Yes. Yes. Well, then you will probably be have to be careful about how to choose a sampling distribution. So which people you choose for taking COVID? We will do that later in the lecture. But I mean, is this not analogous to that, that you will try to be to draw them in an independent way without being affected by other variables? For example, if they're taking the test or not? We will arrive. We Well, we can say that if we draw the samples if IID, there is a way to construct unbiased estimators, and we will see this in the second part of the lecture. Yes, we will see the method afterwards. Maybe this will be a little more clear. Yes. So this mathematically, this is very nice and formal. But in practice, this can be very hard to achieve. So, for example, try to think that you want to do an experiment on the population of 2 million. Let's say it as we were saying, saying before, that if they're possibly affected with COVID-19, but try to think maybe also, something else if you don't want to think about that, but let me how many people to begin, have a dog or smoke, how would you sample from the population? Now you as a student of the University of Tuebingen would have to do this experiment. And I asked you to to draw a sample from tonight. What would you do? Do you have any ideas? For lots of resources? Good luck. But how would you get their numbers? I mean, to get them to show up at their door, so you would go and bring basically the houses Well, you could also just go and drink at different times of the day. Yes. So I don't get early people or something. Exactly. That's also very good thing that you pay attention to try to do it in a random way so that you don't select only a category of people you have. So that's what you would do you go and drink to the houses. What do you have? The unrealistic because they don't have so many resources to study any other ideas? But it's still a good idea. Is there a way maybe you can contact the people somehow? I mean, ideally, of course, I guess you would like to ask the booger on the list of all the people assuming and but this, I'm sure, for privacy reasons, they will not give you the list of people into being and with the phone numbers and addresses. So maybe you can do something else. I know that maybe you're too young generation we are to, to, to actually get to this. But the way it used to be at a time, I don't know if you remember, I'm sure you all had that in your houses a list of the numbers of your city? Do you remember them? How are they called? Well, what were they? They're known as phone books. Do you remember them there that yellow pages, I don't know, if you ever use the phone book, I think I used this when I was seven or so. And not even just I saw my dad using it. But this is what you're going to do today because we will ask you to form groups. And we have here some and we will ask you to form groups. And everyone else groups will receive a phonebook of the unit of the evening. And we will ask you to draw samples through a dice of with a dice. And you will have more or less 10 minutes to design the algorithm to draw 10 People from tubing and then we will have a discussion about it. So try to maybe I don't know if we should go one per line or just choose the you can do like four on this side and five on the other. Yes, I will. Okay five on the other side, I will give this my hand Hello, you can be like I will leave one here. Maybe this tried to form or somehow we get one and then you can just go where one is okay. I have a question about your bias slide. Yes. It says evidence for x. Yes. So is it not just evidence and it's calculated using x? I mean it's not evidence for x. If you marginalize over x there's no x I mean that if it's correct to say evidence for x this is this evidence turns in this in this case it was having for X coming to you that you're marginalizing over the variable x so x is the is the realizations happen in the set X Exactly. But then it's more like it's just evidence and it would be evidence. Okay, so for the people on Zoom, we have prepared PDFs. I'm gonna try to send in the chat right now. Let me see if this works. Okay, so you can just own them and work with them and try to figure out a strategy for it sampling among the entries of the phone. Okay. So the audio in the Zoom call should work again. Maybe, that me, let me know if this works. Yes, thank you okay, so come to an end. Take your seat again please. You have to wrap it up. Otherwise we're running out of time. Okay, so let's start with the evaluation of the experiments. So we prepared a couple of questions. Yeah. To guide the discussion a little bit. So maybe we can answer the two first questions in one in advance. So how do you select randomly among the pages? And how did you then pick a position on the page? Yeah. diocese, and you start to roll the hundreds. Okay. So we got from one to five. Take that. One, and we kind of zero that from here. Cool. And so, okay. So you basically just inject this one sample that is like that? overshoots? Yeah. Okay. So but we're all the pages in the book for Tuebingen. So maybe we should have made this more clear, but it doesn't really change the strategy a lot. But it's only like the first I don't know, 200 or three pages where for tubing and then that comes like a cotton ball and stuff like that. But okay, um, it's basically, you would just reject the samples and throw dice again. And so your strategy still works. And right. Okay. So this would be the strategy for the pages. And then what about the second question? So how did you then select a random person? Page in the page here? Yeah. First row columns, like one or two questions column, and so on. And then we did another role like similar binary search, basically. So the first three digits for the lower half the other ones for like, geometrically speaking, like, okay, and trust me roll another one. To decide what we're gonna do. It looks like it's uneven. Okay, sorry, repeat it again. So, I, I understand you do have like a binary search basically. So you throw a dice and then if it's like, okay, and then we do that. Okay, and then until you finally get one single person number you're gonna start. Wow, okay. Okay. Okay, um, I think probably most of you had a similar strategy. So you basically first thought about how you can compute a random page. And then you thought about, I think many of you did this binary search thing. Maybe someone with a with a different idea. Before we continue, yeah. We, we thought about that first, but then we saw that some pages contain large advertisements issue with issues. For example, if you do the ahahaha call in one call, there's a big art advertisement. Then there are there are obviously fewer numbers. But if you roll the dice, each column has the same probability probability to be chosen. So yes, I'd say half the numbers in that column and there's double the probability for each chose Correct. Instead what we did we just looked at some of the pages were cute advertisement estimated What about probably the highest number of numbers per page and then we similar with how we chose the page we just throw a number between one 200 And just take that number just count take that if the page doesn't contain numbers, we overshoot and we stop stop Okay, so you already thought like a lot about how to avoid the sampling bias if you get like those huge advertisements any any comments about that or other other things to compensate for bias other sources of bias maybe also yeah but the other entries in the column so the problem is not only like just at home and there's only like six to six minutes to six months or more likely a patient and power away people have some luck more probable when people want to come back and depending on your strategy reject the sample outside so you throw up and over but saying there are no numbers at the door to reject to get that from this page but from choosing the page. So you sample each possible spot of number of equally probable and physical location questions so what if a person comes to lions and with what chance do you do with this 50 5050 Okay. So that would be a way to compensate for it yeah. Okay, so the kind of rejection sampling strategy okay. Okay yeah yeah, so, the number of entries or phone numbers among the pages also vary. So, even if you manage to draw IID sample from the page number that may already contain a bias in there, number of space how do you take the maximum of space you but like that's, I mean, still, if you have different number of people on the pages itself, then yeah, so maybe rephrase it because I think it was, like, geometrical on the actual page like, in centimeters. Okay, like he does then binary search such. Yeah. And if you land on ad he like goes back to rolling pitching stuff that could actually work. There was another comment. Yes. That might work. Yeah. Okay, so maybe let's wrap up the discussion because it's fun, but it's just takes too much time. So let's summarize a little bit. And so we noticed that it's actually quite hard to draw IID samples from a data source, especially if you have no control over the data source. So here are the numbers and the data already comes to you in a very unhealthy way in the phonebook, which causes problems. So this is one issue. A second issue is that even if you can draw IID samples from the phonebook, so from your data source, then this data source might already be biased, because think about which people are in this phonebook, probably none of you is in the phonebook, and you all have been tuning in, or most of you do. So if you want to draw IID samples from tubing, and then maybe just taking the phonebook as a data source already introduces some sort of bias, because there are likely more elderly people in there. And students are probably under represented there. And remember that 1/3 of the population of tubing and our students, so it's quite a problem. In this case, this is already quite a bias. And a third point would be that there are multiple sources of bias, and you really have to be careful how to avoid them. So we had that maybe there an issue is, of course, depending on our strategy, but an issue might be that they are not the same number of people on each page. The same problem goes for the columns. And some people go over two lines, because the name is too long for one, and some only cover one line. So there's all certain sources of bias in there. And maybe a fourth points, and I will show you later a strategy to basically compensate for a bias used due to sampling. But of course, this only works if you are aware that you actually that there is a bias there. So this is also or this can also be an issue because you can only compensate for things that you are aware of. Okay. So in the first part of the lecture, we have mainly talked about how and why it is important to you had to think about ID sampling. And this is usually just the first step in the data analysis. So you probably have a question in practice, like, what is the, the proportion of people in the beginning that are vaccinated for example, and for for this m, we can, we can derive an estimator. So for this number, and this is what we're going to do in the second part of the lecture. So we're going to take a look at estimators for things that you're interested in. And these estimates will be based on this IID data. And we're also gonna look at certain properties of these estimators. But before we get to the more interesting bits, we have to we have to get some definitions over with. So the first one is the definition of the expected value. So if we consider a random variable X, taken value small x with a density distribution denoted by P, and we have a real function f, so this function maps from the sampling domain into the real numbers, and then the expected value is of f is given by this integral. So the way I think about this integral is you basically have those little X's which are in your sampling domain. And based on this you come to with an output f of x. And this output is then weighted by how likely it is. So basically, and then you sum over all possible outcomes of the random variable X. So basically, what you compute here is an average where the sum of the function of the outputs f of x, where the x's come from the board probability distribution p. So basically compute an average of outputs according to the density from P i hope this makes sense. Otherwise just ask questions. Okay, and now there are particular choices of F that are interesting, okay, this thing stops working. Okay, and for a particular choice of F V gets to the definition of the moments. So the case noncentral moment often distribution with density PA is given by the expectation, which you can see here. So in this case, the function f would be the monomial x to the power of k. And the K central moment of PIR is given by this expression. So, x minus the expectation of x to the power of k. And there are particular moments that are again, interesting. For example, the mean is the name for the first noncentral moment, because here like T is just one. That means in the integral we have x times P of x. So, the kind of T locations in the sampling space are weighted according to the probability. So, you can think of this as or it tells you something about the location of the distribution rights. Whereas, for example, the variance is defined as the second central moments. So, k equals two. And so this would be the x corrected at the expected square distance between x and x expectation. So, this tells you something about the width of the distribution. And you can show that this is equal to the, the second noncentral moment minus the first noncentral moment, which is the mean squared. And the standard deviation is simply the square root of the variance. Now, if you don't have like a scalar quantities, but random vectors for x, then you can also define something like a variance, which in this case is the is called the covariance matrix. So, it is basically defined similarly to the variance in the one dimensional case, we have the second moment minus the first moment squared, and this matrix will be a square matrix and on the diagonal, you have the variance of the individual vector entries. And the off diagonal elements will be the covariances between individual entries of the vector. Okay, so that's it for the definitions. So, now we can get to the actual estimator, which is called like the simple one that I'm going to show you is called the Monte Carlo sampling estimator. So, the first question you might have is what is actually an estimator. So, an estimator is a random variable, and it's defined to approximate some property of the population distribution. So some property of P and basically, so let P be the density of this distribution, and then we consider the property here, phi, which is defined as this expected value. So, the expectation of f 100, the distribution pier will be denoted by five. And this is a very common case in data analysis. So, you want to approximate some expectation, of course, there are other things you might be interested in, but we're gonna only focus on this one case, but it's actually quite common. And you can define an estimator for this quantity, which is phi hat. It's called the sampling or Monte Carlo estimator. And it's simply defined by this average of function outputs, where the inputs, the x i's are IID samples from PA. Okay, so this is the setting. So maybe we can gain some intuition about why this could be a good idea as an estimator. So earlier, I told you that this file, this expectation of F under PA is basically or can be thought of as an average output, where the weights are kind of given by the how likely the, the observation is. And that's a very similar case here as well. So we also have an average here, just over a finite amount of data. And, of course, we don't have an explicit weighting by some probability density function here. But if you think about what random samples x i are actually likely, well, then the answer is probably in regions where the density is higher, you are more likely to sample x is then in regions where the density is lower. So you kind of have this implicit waiting here, because x is where the density is large are actually also more likely to be drawn. Okay, so that's maybe some intuition why this formula could be a good idea. And we're gonna take a look at this more mathematically, right now. So as I told you, the Monte Carlo estimator phi heads, as all estimators are random numbers, simply because the samples are random writes their IDs. samples random samples, we come to the estimator based on the samples. So the estimator itself is also random number. And the first natural question I would say is, okay, if it's a random number, what will the expectation of this number be? So let's take a look at the expectation of Fiats. So I just plugged in the definition here, so we have this average over a function outputs. And now we're gonna use a property of the expectation operator, which I want proof, sorry. And so the expectation operator is actually linear. So that means if you have two random variables, x and y, and the constant scalar, alpha, then you can, the first property that we're going to use is that we can separate a scalar from the expectation. And the second one is that the expectation of a sum of variables is a sum of expectations. Okay. So we're going to use this property right here. So first of all, we can separate the scalar. So this will be alpha one divided by n in this case, and then we can apply the expectation to the individual summons in this sum from one to n, because of the second property. Now, what is this? This is a quantitative overlay that we have a name for. Fire. Exactly. So and why is this fire. So remember that X is there are IID samples from P. So that means they're all distributed according to p. And because of that, the expectation of this thing is always five, for each and every of those x i's because they are identically distributed. We don't need the independence here. But we need that there, they all come from P. Okay, so that means we have fi in the sum. So we have a total of n times five divided by n, so this cancel, and we get five. Okay, so sorry. So this property here, that's the expectation of the estimator is the thing that we want to know about is called unbiasness. So in this case, we say the Monte Carlo estimator is unbiased. And it has nothing to do with like social bias or something like that. It's simply a technical concept that tells you that on average, the random number in expectation is the quantity of interest. Okay, so is the so if I compute a Monte Carlo estimator, Will it always be correct? But it'll always be the correct number. You're shaking your head? Why not? Me. Okay, but I mean, I just proved that, like, I'm being a little bit mean. So I just proved that this Fiat is always is always the correct one, right? You're basically already answered. So it's only true an expectation, right? So you're correct. If you do this, like many, many times this estimator, and then you come to the mean offense. So then this would be an approximation, actually, it would still be, but we'll be unbiased, but it will still, like have offset properly. But theoretically, in expectation, if you do this experiment, like infinitely many times, then this will yield fire on average. Okay. So this is nice, of course, because I mean, it's a pretty intuitive property rights are a thing that you would want to have for an estimator that it's unbiased. But there is not a thing in practice that matters. And this is the variance of the estimator. So for example, you would want an estimator that gets more precise if you provide more data. So if you increase this end here, he would want the variance to go down. So let's check if this is true. Okay, so let's consider the variance of Fiat. So according to the definition of the variance, this is simply the square distance between the random number and its expectation. And as we've just seen, the expectation of Fiat is five because unbiased. Okay, so then let's plug in the definition of Fiat, which is this average here, if I'm going too fast, and just stop me, okay. And now, so here, phi is outside of the sun. And here, I pull it inside. So why is this true? Maybe we can think of it from right to left actually. So if we write this phi inside of the sun, we actually have it n times because the sun goes from one to n, and we divide by n again, so the two ends cancel, and we can we basically arrive at minus five. So this still holds true, okay? It's just a little trick to pull the fight inside of the sum. And now we go to expand this sum, because eventually, we would like to apply the linearity of the expectation again, that we just saw. And this doesn't work we have because we have to square here. So we have to get rid of the square first. And the way we do this is just we multiply two sums. And the way to sum sum multiplied if you multiply, like, if you think of it as two brackets, basically, and each of the brackets contains a sum, then you have to multiply each entry of the first brackets by each entry of the second brackets. Okay. And this is exactly what I do here. So this is why we arrive at this double sum, because we have to. Yeah, because we have to go through every combination of multiplying this with an index, I buy this with an index j. Was this clear? Okay, as long as there are no complaints I just carry on. Okay, so now we can actually add, so two things, basically. So we multiply those two brackets, so we get a total of four summons, which you can see here. And I pulled the expectation inside the sum, because of the property we saw earlier, the expectation is linear. Okay, so we cannot draw the expectation of this number, then we also have the expectation over this. Can some of you may explain why I wrote down is equal to phi squared here. Yeah. Find out. Exactly. So you used that phi is actually a scalar. Right? So this is pretty important, then you can pull it out, it's not a random number, it's just a number. Okay, so you can pull out five, and then you just have the expectation of f under p, which is also five. So you end up with five squared. The same goes for this here. And the expectation of a constant is just this constant, because there's no randomness at all. So we can all ya know, F minus two phi squared plus phi squared. So yeah, you're right, this expression. This is just copied from the slide before. So now, we're going to take a look at this expectation. And we're going to distinguish between two cases. The first one is that J is equal to iron. And this is the rest of the of the of the inner sums of the inner some goes from J equals one to n. And we separate the case where j is equal to is from the sum. So if we have the case that J is equal to ai, then we just have f of x i squared in there. So this expression here minus five squared, and this is the case of error, I is different from JM, so we've just copied from above. And now we're going to use a second property of the expectation that is basically a two liner. So I recommend you just do it for yourself to see if you if you can do this little proof. So for independent random variables, x and y, we have, we have already seen that the joint density function factorizes, right, and this is exactly what we're going to do use the proof if you will do it. And you can see that the expectation of x times y also factorizes into the individual expectations. So kind of this property from from the PDF carries over to the expectations. So we can apply this to this expectation, because now we have x i and xj are IID samples, right? So they are independent. So that means we have the expectation of a product of two independent random numbers. So this expectation factorizes into two individual expectations. And again, using a similar argument as before, this is again, phi squared. Yes. Variables, XY and Z, because we sample them. Yeah. So because x i and xj are independent, this also applies to f. So f of x i and f of x j. It's a little detail that I didn't want to mention, because it makes things maybe a bit more complicated, but it still works. Yeah. Okay, so now what we have here, so because this is five squared, and we can plug it in here, we just have zeros into some right, so there's some just drops completely. And what is left is just this year, which is written here. And this is just the second moment of f minus the first one squared. So by definition of the variance, this is the variance. And now as before, we have n times this variance divided by n squared, so one of the NS cancels and we end up with this expression for the variance. And this is O of n. one divided by n. So this drops, the variance drops linearly in N. And that means that sampling converges quite slowly. So the expected error, which is the square root of the variance drops as one divided by the square root of n. So that means, if you want to, if you want to make your estimator twice as good, on average, you have to provide four times the data. Okay, because of the square root. So, there are basically two perspectives on that, actually. So, the first one is, if you want an unbiased estimator, like the Monte Carlo estimator, this convergence rate is actually optimal. And our Monte Carlo estimator achieves this rate. So this is a result from the common low bound if you want to read about it, but I'm not going to, I'm going to spare you the details. But you can, you can show that this convergence rate for an unbiased estimator is actually ideal, okay, so it's the best you can get. But if you are willing to accept maybe a small bias, then you can get a better variance. So smaller variance. So you see, there's kind of a trade off, if you want to go with unbiasness, then your convergence will be slower. If you're willing to accept some buyers, it may be small, but some buyers, then you can have a much faster convergence, because you have smaller variance. So there's a trade off between the two. Okay, so this is basically it for the estimator. But we so far, we've always assumed that we can draw IID samples from P. And this may actually not be so easy, as you've seen in this phonebook thing, because of sampling bias, for example. So now I'm going to present to you a way a principled way to compensate for sampling bias, which is called important sampling. Yeah, sure, sorry. The last line, you mean this one. So, so if for a variance if you have this convergence rate, and we have to basically take the square root over this, and we get squared over this. So if if, if you believe me that the variance of this quantity is equal to this quantity here, then you can just apply the square root at both sides of the equation. Because maybe the square root is better, it's easier to interpret, because it's not a square error, but a metaphor, the error, maybe it's more in the order of, of so you can basically have a measure for the deviation between you know what I mean? So it's Yeah. Yeah. Yeah, it's the sound of the patient, right. Wait, any more questions? Okay, so let's continue with important sampling. So if you're in the setting that we cannot produce samples from P, but only IID. From Q, we can still produce an unbiased estimate, kind of surprising Leah, and the important sampling or reweighted sampling estimates. And this is based on this core insight I wrote down on the slide here. So phi was defined as the expectation of f under a PA, which is this integral. And now we just multiply this by one, and they express one by q divided by q. This is just one under certain assumptions, that it's not zero and stuff like that. But in principle, this, this should do. And now we have basically a more complicated function, namely, F times p divided by q. And here we have again a density function q. So according to the definition of the expected value, this is simply the expectation of this more complicated function consisting of F p and q, under the density Q. So the important thing here is that we have the density Q in the index. Okay, so what does this mean? We can actually compute this fire based on samples from Q. So given n IID, samples, x is from Q, this is the important bit not from P, but from Q, we can just write down basically this is the Monte Carlo estimator, not for this expression, what we did earlier, but for this expression, so we just take the samples from Q and feed them through our more complicated function, which is f times p divided by q. And this is exactly what what I wrote down here, just that I summarized, P divided by q in this new function in the sampling weight, it's called W. Okay. And you can show basically, by construction, this is true that this estimator is unbiased. That means that the expectation of this random number is again fire which is of course, nice. Yeah. Maybe a different example for you and maybe from the soul fight could be, what is the proportion of people in tubing and were vaccinated? And P would be a uniform distribution among all the citizens of TBN. And the sampling distribution maybe would be the phonebook. Which would be a bad choice, by the way, but yeah. Okay, so the downside of this estimator is that it can be very imprecise. And the reason is, so if we compute the variance of this thing, basically, completely similarly to what we did before, we can show that this is equal to one divided by n. So the convergence rate is the same as before, but we know half the variance on the queue of this of this function. So f times p divided by q. And why could this be a problem? Well, if we go back to the, to the estimator again, so imagine a case where p is actually large p of x for some sample x, if p is large, and Q is almost zero, then you divide by something that is almost zero. So this will be huge, to be a huge number in a second. So that means that this weight will be basically unbounded, so it can be arbitrarily large. And the whole estimator may be completely dominated by this one sample with this particularly high weight. And this is why this estimator can be so imprecise, because the estimator might depend on a very fewer samples with higher weight. Yes. That's true. I don't know. So I think, basically, it's true, but I think you have to distinguish between knowing the density and being able to sample from it. That's, that's a, that's a difference, right? So here, we need to be able to evaluate the density at an arbitrary point in the sampling domain x i. But so it might be that you can do this, but it's, it can be that you cannot generate samples from this density. Okay, so for example, think about the Gaussian density function rights. So you can evaluate this easily, because you can just write down what the density is enclosed for him. But it's actually not so trivial to sample from this because you need kind of the cumulative density factory function inverted that cumulative density function, it's already approximation because we cannot compute the integral, exactly. Stuff like that. So already, for this case, it gets pretty, it's really not non trivial to sample from a Gaussian, but you can still evaluate the density easily. So you have to distinguish between the two. And here, you're absolutely right, this was my last point. And if you want to correct for sampling bias, this only works if you're aware of it. And if you can, therefore, because you're aware of it, you know that there's that P and Q are not the same, but you have to be able to evaluate this ratio. So this is the assumption for importance sampling that you can do this. Okay, any more questions about important sampling? Yep. Can you hear me? Sorry, see an example of this ratio. So in the form of example, if p is like a uniform random sample among the people of tvm, if you know the number of people in keeping and say 100 1000s, then P would be one divided by 100 1000s. And if you settle on a strategy for your phonebook samples, you can also maybe come up with a formula for the distribution queue that you're actually using. Because maybe, you know, okay, I can sample it from the pages. And then you can write on a probability for that and so on. So you can maybe derive what what the actual the distribution is, will not be a uniform distribution. And then you can compute these ratios. So by the way, this function Q sampling from the phonebook would be a bad choice, because, of course, there there are people in keeping that have. So the probability that might appear is one divided by 100 1000s. But he was zero because they're not in the phonebook. So this would be a bad choice because you're basically dividing by zero. So yeah. Yeah, we say that correctly, like the biased and like, cue for people on the front of advertising would be with the higher end Yes. Exactly. And this basically tells you that how you can compensate for it right? Because the probability would be higher, the skew is larger than the P should be. And therefore, these people are given smaller weights in the expectation. Clock. Okay, yeah. Maybe I should Okay, is there is there a question in zoom? Yeah. Can you hear me? Yeah, I can hear you. Okay. So I want to just a confirmation on q, q of x. Yeah. So P of x, for example, is describes a probability of a wider population. And with QX, we are actually sampling from this population. Exactly. So we assume that we can sample from Q, and then we compute this ratio, p of x divided by Q of xy. So we assume for important sampling that we cannot sample from p, because it may be just too complicated to do so. But we can sample maybe from a simpler distribution Q, where we can actually create samples and evaluate the density, such we can then compute the weights. So the way actually describes how many times the actual probability of a sample is larger than the probability of the same sample in a smaller distribution, right? Yeah, you can think of it this way, I guess. Yeah. So the weights basically tells you how to compensate. So if Q is smaller than PA, so you sample it less often than you should, then this weight will be larger, because to compensate for this less sampling. And if it's the other way around, if q is larger than PA, on average, you would you would observe this more often. So we have to give it smaller weights to compensate for it in the expectation. Okay, thank you. Yeah, he will come. Okay. So, let me summarize a little bit what we discussed in this lecture. So we have seen that there are multiple types of experiments. And in observational studies in which you want to identify expected values, often, the ideal strategy is to sample IID from the whole population. And we saw that, it's actually quite hard to come up with a strategy that is unbiased. So to come up with a strategy for IID sampling. And but in this case, we have access to IID samples, then we have this Monte Carlo estimator, which is unbiased, as we've seen, and it has this, let's say, Good convergence rate in the sense that it's ideal for an unbiased estimator. But of course, there may be more sophisticated estimators that are biased, but converge much, much faster. Yeah, samples from the population P are rarely accessible. So I showed you a principled way how to compensate for that due to improper sampling. And but of course, the assumption here is that we can at least evaluate this ratio of p divided by q. So you only can correct for biases that you are aware of. So only when you realize that you're sampling actually from a distribution q not from P, you can correct for the bias by giving the samples the importance weights. And sometimes we simply cannot observe the quantity of interest directly. For example, you might be in a setting where you can measure positions of an object, but you might actually be interested in velocity or something like that, then this is also a little bit different. But I guess Phillip, will tell you more about this in about two weeks. Okay, maybe one more thing. I think Phillip mentioned this earlier, there is this methods of machine learning research seminar, and you can find all the information here and the first talk will be on the third of November by Frederick Winslow, which I really would recommend to you and there is no lecture next week because of public holiday. So you will see Phillip in two weeks again if you guys don't have any more questions, we can Yeah. assignments like there's no assignment next week. Do you remember how it was with the exercise sheets? I would say so as well, but we can check that you know, so assume that there is a sheet and I think we were told that there is no assignment. Okay, then the question was, if we got like just put in bled it for two weeks for like the next summer. Oh okay. I'm not sure I don't get tutorials this week so I'm not sure if it's takes place or not but we can of course check and just let you know Yeah. Or do we still have to submit on on Monday? We got a check. Sorry. It's uh okay. Any more questions sorry okay, yeah, yeah, you can just come up to me and begin to scan it yeah ah, this thing. Okay, I think Philip didn't put it on the slides, right. This contact tracing thing that people can basically look into the lecture hall. Oh, I didn't know I think we don't have the QR code right now. So sorry, but maybe next. Okay, last thing please provide feedback okay. This will be very valuable also for us because this is basically the first lecture we have ever given. So feedback would be would be really appreciate it. Thanks so much, yes. So I'm going to end this call on Zoom. Okay. Bye, guys. 

