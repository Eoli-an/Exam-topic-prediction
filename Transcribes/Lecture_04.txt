Okay, can you hear me? No? It's okay. Thank you that's good. Thank you check Whoa, okay. For the past, let's get started. So I hope there's someone in the chat who has say something all over the well, I'm seeing something right now. So I hope that you can hear me now. Can anyone hear me? Yes, good. Okay. So welcome everyone. Good morning in the in the call and in the room. This week is new insofar as we have I think 25 of you here in the room Rafi and 70 in the call it an interesting development. I wonder why that is? Maybe let me know in the feedback. Why you've decided most of you to stay in the call? The past, right? Someone says, okay, the bus strike? Well, that's a good excuse. Okay, then, let me get started. So here is feedback from last week. wary, roughly speaking, you still seem quite happy with the course most people think it's at least average or better. Most of you think it's a little bit fast. But not way too fast. Actually, I think this is from the professor's perspective, this is a good distribution for speed. If it were right slipping in the middle, it probably means that it's a little bit too because students tend to have a bias towards easiness. I also made bivariate plots for how your scores for these three different categories distribute against each other. You can look at those later if you like. But interesting thing too. While not maybe not so interesting thing to notice is that the kind of people who like the course tend to think it has the right speed and the kind of people who do Don't like it tend to think it's too fast to heart. And of course, there's a strong correlation between people who think it's fast, and people who think it's hard. So maybe I shouldn't be asking the second question about difficulty and speed anymore, it should just be one question. Because it's strongly correlated anyway. Use your detailed feedback. So almost everyone who, who bothered to write something positive about last week said that they liked the two examples, lighthouse and the odd ball example. Things that people didn't like, they didn't understand this, usually, basically all of the math, but some people also said the code was a little bit difficult for them to understand. And, of course, I understand that when I do when I show Jupyter notebooks that you can't always, especially with the example I did last week, which was very, you know, actually, most of it was just writing some convoluted code to do some polar coordinate transformation. I'm not going to spend much time on it in the lecture today, I'll do more Python again. And actually, there'll be a few lines that maybe we have time to look at. On more kind of administrative things. So some people thought that, okay, apparently I didn't always repeat questions that you asked you in the lecture hall, to the people in the call, I tried to do this, I'll keep trying to do it. Actually, I should close the chat so that I can see when someone posts something new in the chat. Apparently, there were audio problems and zoom towards the end, very sorry about that. I don't know much I can do about this. And some people thought it was actually even better to write math on the blackboard, obviously, I can't, well, if I when I do this, then people in the court can't see it. So I have to think about how to do that. And then people think it's too fast. So this today, I actually have quite a lot of content. But actually each of the individual steps is relatively compact. So let's see if I can do it. Well, you can tell me afterwards whether it was too fast or not. So now I've already had to fight with my hardware again. Hmm. This is this stupid clicker that stops working every now and then. And now because of zoom, I don't have a. You go. So what if you do so far, the first two lectures, we talked about how to collect data, how to get your hands on data in the first place. And last week, and the week before that, he forgot two different ways of thinking about how to collect data. One was to avoid bias to make sure you're drawing observations directly from the distribution you care about. And the other one was to maximize information gain, to ask questions that maximally bisect your data, or multisector data such that you can quickly hone in on the right answer. And also mention that the right answer usually, in fact, is lies somewhere in between, you want to do experiments that actually tell you something, but also you want to make sure that your experiments don't lead you down the wrong path. And then the second half of last week's lecture, we began to talk about what you do when you have the data. The main message I sent then, and I'm going to do this again, today, multiple times over is that there's only one correct answer to inference problems. So two questions about quantities that aren't directly observable in the data. And that answer is given by Bayes theorem. So if you if there is something you don't know, then you write down the probability of everything, both the stuff you've got to see and the stuff that you'd like to know about, and then apply Bayes Theorem. In principle, that's the entire answer. It's just an annoyingly for real world problems. Sometimes it's hard to do this mathematically numerically speaking on a computer. And so we then have to resort to some kind of approximation. So we'll do that today as well. And also, sometimes people just want to know one answer to a question. They want to hear you make a guess, basically. And that's when you try to construct an estimate an estimator a quantity that is, hopefully has good properties, relative to the correct answer to the question that you don't know, we found that one, maybe really important way of constructing such an estimator is to take the likelihood or the posterior and find its mode, maximize it. Typically speaking, that posterior mode or likelihood mode has good properties. We already saw one great such property, which is that if you increase the number of data points, asymptotically, you'll be close to the true answer. You also saw maybe on your exercises this week, that that doesn't always work. And sometimes there's some caveats in particular in the likelihood, has a pathological shape. So it has, for example, spikes on one corner right at the boundary of its of its domain, or if it's a symmetric distribution with a lot of mass fire in details, and then the mode can be a really bad estimate. So what we would like to I'd like to do today is to go one step further and look at this question that comes here at the end. Actually, maybe I should dim the lights here at the front a little bit. Let's see if this works. Probably too dark. No. So okay, maybe then you can see my code better later on. So once you've made your your your estimate, once you've returned your best guess, of course, one thing you'd like to do as a good scientist, computer or natural is to estimate your error. How incorrect is this case going to be? To draw error bars? It's also clear that you're not going to be able to guess the correct error. Because if you could, you could just subtract it from your estimate. And then you'd have the correct answer, right? So that's not going to be possible. But you'd like to estimate the range that the true answer lies it. And today, we're going to do three ways of getting this confidence estimate. This is called Confidence, this estimation of error. And I'll spoiler alert, there'll be three different answers. The first one is last leg last week, do Bayes theorem and look at the posterior and think about its shape. The second one is approximate the posterior think about his geometry around the mode and construct some estimate of confidence from that. And the third one is actually going to be a very pedestrian way of answering this problem, which is just imagine a few new data sets. And there is going to be a trick, a funny way of how to do that, and just rerun your code n times get n different estimates. And then that distribution of those estimates is going to be a guest for how wrong your estimate is. And there's going to be one running example we're going to use for the entire lecture. And it's going to be a deliberately constructed set that actually not constructed a deliberately simple one, but a totally unconstructed. One a real world one. So we all remember, these days, we really remember about a year ago, actually last November, by ontic, Pfizer, this collaboration between a German startup and an American multinational pharma pharmaceutical company, announced to the world that they had completed a phase three trial, which confirmed that their vaccine candidate, according to this trial, was highly effective at preventing infection with COVID-19. So the question you might have as a layman is how did they actually do that? How they came out? How did they construct their estimate in their trial? And how did they come up with the press release that they send out in the end? So I wonder whether I could also already show you the code, actually, I can show it to you now. So there's that there's going to be a Jupyter notebook here that will go through slowly. Here's the story, or 18th November, that's actually almost to the day, after three days, a year ago, they announced that they had a 95% effective vaccine. And you can only think about what that means. So vaccine efficacy in I spoke about this in the very first lecture is defined as the relative difference between the probability to be infected if you're not vaccinated and vaccinated. So to change the probability relative to the underlying probability to be infected. So it's how many percent less likely are you to get the disease if you're vaccinated. And so actually, you can find this Jupyter notebook on areas you can download it, there's a link here where you can click this is the this is going to lead you to the press release. And this actually leads to the to the, to the study design, it's a PDF document, I recommend you have a look, because it's actually really interesting how they design these kinds of trials. They're super elaborate this protocol here, as you can see, as a lot of pages, and this is the protocol that was like that was released before they actually did the trial. Right. So they decided how they were going to do the experiments before they went out into the desert called pre registration is a very good idea, because that avoids you moving the goalposts, as you're doing your experiment, and it's in this field, it's clearly a standard thing to do. Now, I need to figure out how to go back to the stupid zoom thing taking away right. Okay, so just as quickly recap, our randomized control trial works. So what they did is they found somewhere in the world, a bunch of people, actually they found 44,000 people in total, and then divided them into two groups equally and randomly. The idea being that, of course, this group of people, they are probability to be infected by the virus is subject to all sorts of complicated external externalities, right? Where they live that what time of the year, what they do professionally, maybe their age and their gender or whatever else, right? And you would like to get rid of all of those potential effects or average over all of them. And what the simple way to do that is to cut all cause links between being separate being put into the placebo group or the treatment group. And you do that by randomization. So when someone comes in, they everyone gets accepted. 44,000 people got accepted to be part of this trial. And then right at At the very end, when they are given their injection, there is some smart randomized protocol that decides for each individual patient completely separately from who they are, whether they get the treatment or whether they get the placebo. Okay. And now, what actually happened was that in the treatment group, there were eight people who can contracted an infection, and 162 in the control group. And now the question you could have is, is this actually enough? So these numbers look pretty large, 44,000 people, that's a large group of people, it's like half of Tubingen. But in the end, almost none of them actually got the disease. So they basically all vaccinated in vain. But only 162 of the control trial actually got the disease. And eight got it in the in the group that actually got treated. So what do we do in such situations? How do we estimate the unknown thing? And actually, what is the unknown thing? So let's first talk about what is the unknown thing in these kinds of trials. You can also type into the chat if you want to. efficacy is the unknown thing. That's right. And efficacy is a function of two different variables, right? It's the difference between the probability to be infected if you're part of one group, or part of the other group. So there are two numbers, probabilities, and those probabilities lie between zero and one might be just real numbers. They might be like 20% 1% 99%, whatever, right? So what this is, is an instance where he has to have to estimate a single scalar quantity, a number between zero and one. And that quantity happens to be a probability itself. That's really confusing the first time you hear about it. And so I've got a slide for it. The good thing is, we're not the first people to worry about this problem. And you can imagine that it's maybe not even a new problem. It was first maybe properly this studied by why is it not working? By the person who invented probabilistic inference, that was not Thomas bass, by the way, almost base vote, a funded article about the doctrine of chances that didn't really contain the idea of a prior or for posterior, he just thought about likelihoods. The person who invented probabilistic inference is PFC Ma, the Marquis de Laplace, like larger than life figure who lived through the the French Empire he was at some point, I think, the the Chancellor of the French Empire under Napoleon. And before that, he was already like a eminent French POLYMATH. He wrote about basically everything, but also about probability. And in his book on the, the analytic theory of probabilities, I'm not going to try to pronounce it in French, I'm just going to get it wrong. He has a chapter chapter six, about the probability of future events derived from observed events. Like then there was a big philosophical question that had been around since the ancient Greeks, about how do you know that the sun is going to rise tomorrow? Night, you get up every day, every day, the sun goes up comes up, who tells you that it's going to be there again, tomorrow morning? It's a classic kind of, you know, ancient philosophy question. And Laplace was maybe the first one to give a proper mathematical answer for it. That led to actually that's led to a rule that is now called Law School of succession. And it actually is exactly our problem. It's, we keep doing experiments over and over and over again, they are IID to each other every time we treat someone with a vaccine, they might get a virus, they might get infection afterwards, so they don't and every time we treat someone in the control group, which is to say we don't treat them, they might get the virus or they might not get it. So there's an unknown number of probability between zero and one, which we need to estimate. So what do we do if you need to estimate an unknown quantity? You can shout it out loud. Yes, thank you very much. Someone is willing to do it, you have to do Bayesian inference. So we have to write down the probability of everything. Let's first start with a likelihood. So the probability to observe a certain number of successes and failures, given that the probability to observe a success is pi and a failure is one minus pi. By the way, success and failure may be danger was words positive and negative or a and b or zero and one outcome is given by Well, the binomial distribution, right? It's the probability for the thing to happen to the number of times that it happened times the probability for the thing not to happen to not raise the number of times that it didn't happen. multiplied with, you know, this correction factor for how many possible choices there are, for one thing out of a sum of two things, the binomial coefficient, and that's just a normalization so that the whole thing is a probability distribution. That's our likelihood before, we didn't call it that many times before. So now we need to do Bayesian inference. So we have to multiply this likelihood with a prior divided by the evidence, and that's going to be our posterior. That's it. So what's our prior going to be? Come on in the chat wrote p of pi. Thank you very much. Yes. That's a symbol. That's the name of the prior. But what is it? Actually, this is going to be a function of this variable pay, and what should it be? Ah, so in this setting, okay. So there's answers in the chat as well. And I Okay, there's 63 answers. Wonderful. So let me find an order, I'll tell you here in the room that there is an answer in the chat that says 5050. So 5050 is two numbers, right? One half and one half. But the thing we care about is a probability is a number between zero and one, there are infinitely many possible values between zero and one. So we can't say the probability is either zero or one. And we'll put 5050 on each because the probability might be 20%. It might be 30%, it might be 90%. And we have to put up a P of pi for every value of pi, where pi is a number between zero and one, not either zero or one. I think it's really good that you mentioned this, because that's the common misconception about this problem, right? The thing we're trying to infer here is a real number between zero and one, not a binary value is 001. Okay, the next answer in the in the actually, the answer that now is has also been typed into the EU. Citizen answers in the chat, someone just wrote a Gaussian distribution. Okay, that's a good kind of knee jerk reaction, we'll come back to that later. So by the way, if you're talking about a Gaussian distribution, let me just point out that we're trying to infer a number between zero and one. Gaussian distributions are defined on all real numbers, even for minus 67 and plus 89. But our domain we're trying to infer on lies between zero and one. So that Gaussian is really maybe not the right distribution to start with someone in the chat set a uniform distribution. So I don't know anything, right? Every probability has the same probability every page with the same probability. Good, that's a good choice, maybe, because that we don't know. And are two people who said a uniform distribution. And there was also a proposal, both you in the room and in the chat to say, Oh, we could look at the background infection rate of people in the population. And use that to inform what we are going to expect. And if you think about this proposal, you can probably feel in your head, like the emotions rise up that define the debate between Bayesian and frequentist. Because on the one hand, it seems kind of dangerous to us to do that, right? Because we want to define this perfectly, like cautious, pessimistic experiment, to ensure that there are no accident, we're not accidentally becoming overconfident about how good this vaccine works. And who knows, maybe the background rate that we're inferring is actually wrong, because there were some problems with reporting in this country where the experiment is done. And on the other hand, it seems really silly not to use that information, right? Because we have it and it's kind of like this informs all of our thinking about this virus. So why do we ignore it? Well, there isn't really a good answer to that, right, you could like put yourself in two different minds and either do it one way or the other, we'll do it in a classic way. Let's just use a uniform prior. So a uniform prior is a function that assigns probability one for every real number between zero and one, that is a probability distribution, because the integral over one over the unit cube is one. So that's a probability distribution. Thankfully, or interestingly, you can also write this one function, trivially, as this object as py raised to the zero times one minus pi raised to the zero because anything raised to zero is just one. So it's just one times one is one, fine. And that's convenient. Because if you think about it, that means that our posterior is going to be a product of this function and this function, and both of these functions are something pi raised to some power, times one minus pi raised to some power. So what's going to happen is that we literally just add the numbers on the exponents of these two terms pi and one minus pi. And that's almost our posterior, obviously, there's going to be a function that is just pi raised to some power times one minus pi raised to another power. So this function, even though it's gonna have an interesting shape, you'll be indexed by just two numbers. And, and I have a corresponding slide for that, or Python notebook. So here is our function. I've implemented this here. And the super fancy thing, the only thing that actually really matters, the only line of Python that you have to look at here is this one. So this is me using sai pi stats to box to evaluate the this probability distribution, this happens to be called the beta distribution. Why Oh, actually, let me go back. But this is annoying thing, that for this to be an actual posterior, we have to divide by the evidence, and the evidence is the integral over this thing. So it's the integral over, I've already divided out those binomial coefficients, because they're both in the denominator in the numerator, so they cancel. But we need to do this integral over pi to some power times one minus pi to some other power, the PI. And that integral is called the beta function as a historic relevance, it comes from it was a study by Euler, when he was trying to extrapolate the factorial function. That's how we ended up with the binomial coefficients and gamma functions and so on. And annoyingly, when Laplace did, wrote his paper, his book, he didn't know how to compute that number. Because it's an annoying, complicated function. So we just called it B. DITA, actually, he didn't call it that leshawna did, but he just used it, and said, I don't know what that number is. So we'll have to come up with an approximation, we'll get to get to that later. But today, you know, it's 2021, we have computers. So there's a nice sci fi code that just evaluated for us. And it does this for any value of positive or negative outcomes. So we could do this here in this room. I don't know whether you're infectious or not. But what I can do is, I can go around and check, maybe how many of you are wearing glasses, that's an innocuous thing to do, right? So I'd like to infer what the probability is of a person on this campus wearing glasses. So what I could do is I could go around, I start with my prior prior is a flat distribution, I don't know what the probability is, it's just a one function. And now I look through the room and the very first person I see happens to be wearing glasses, actually. So what I can do is, I can go, I'll increase the count to one. And now I've seen zero people not wearing glasses and one person wearing glasses. So what I see is just the likelihood, the likelihood is just what's the probability to see someone wearing glasses, if the probability to see someone wearing glasses is pi? Well, it's pi. Right? And that that's this statement is so trivial that it's already confusing again. So maybe you have to think about it for a second. Why is that the likelihood of because if it were, if there were zero probability to see someone wearing glasses, it will be zero. And if there is one probability to see someone wearing glasses, it will be one and anything in between. Okay, now I look at I'll go around this way, see the next person who's not wearing glasses. Now I've seen someone who is not wearing glasses, by the way, I already know after the very first person, that the probability to wear glasses is definitely not zero, because I've just seen someone reading glasses. But if I now see someone else who is not wearing glasses, now I have to multiply in one minus pi once and pi times one minus pi happens to be this little lumpy distribution, the next person sitting next to him, I can tell those people in the chat actually is wearing glasses, so I'll multiply to raise the count once. The next person afterwards is we can already save some time, there are three more people who are not wearing glasses 123 And then the next person after is not wearing glasses, either and not wearing glasses, not wearing glasses not wearing glasses, and then they appear to people who wear glasses, one to someone who is not wearing glasses. And yes, did I miss someone in my counts? No, hopefully not. So this is the binomial distribution evaluated as a function of pi. So the binomial distribution is a probability distribution over binary counts right about how many pluses and minuses I see. So it's a it's a distribution over binary numbers. But as a function of its its its parameters Sita, it looks like this. Given that we've seen four positive counts at nine negative counts, and this is actually the opposite. Now I could keep going. You can play with this yourself after Isn't it? Yes. And what you see here in the bottom in the red and green are the individual likelihood functions. I've just multiplied them all in. So you can see how many times before we multiplied, and that's our posterior. That's it. So, of course, you can now check where the mode of this distribution is. I mean, you could kind of guess, right? It's just over 33%? Because it's just four over nine, sorry, for over four plus nine. And how confident are we in this estimate? Well, this confident that's it. Right? This is everything we know. And now we can do the same thing for our prices study. Oh, Germany, I should, of course, call it the biontech study. So here, we have 22,000 people in each group, and eight and 162. So that's like doing this experiment we just did. But with m being eight and or n being at an EMD being 22,000 minus eight, in one case, or while with 168. And the other. And this is what our posterior looks like. It this actually is not a posterior, I'm just plotting the binomial effectually doing this here. Okay, so let me just tell you what I do. So I've loaded. Not yet here uploaded from sai pi stats, the binomial function that makes it convenient. And I make a plotting grid that I've like goes over some domain that I've cunningly chosen. So notice that the x and y boundaries here are not zero and one, sorry, x limits are not zero and one. Why? Because I mean, we've seen eight people out of 22,000. So that probability to be infected is going to be tiny. And if I made a plot between zero and one, you won't see anything, we'll just be white and a tiny spike right at the corner. So I've zoomed in. And these are the two different groups, they have different x limits. That's, of course, a dangerous thing to do. But otherwise, you don't see anything. So I have to zoom in. And what I'm doing is I'm plotting this function that you see up here, literally this function that this binomial probability mass function for the count of people who are infected in the control group, and the total count of people who are infected in the country everywhere in the control group, as a function of this plotting grid. And that's what it looks like. And now, this is what we know about our group. And so the main takeaway, maybe at this point is, uh huh. So this will mean that we're not perfectly certain about the vaccine efficacy. And of course, we are not. And you may remember a year ago, when this number 95% came out, there were also talks about how it might also be this and this good, and so on. We'll get to that in a moment. Okay, so, so far, so good. This is really what you'd like to do. That's your vision answer. And in this case, we can well almost do it. But notice how we haven't yet spoken about the vaccine efficacy, we've only spoken about these two numbers. So what we'll need is actually a nonlinear transformation of those two numbers, we'll need one minus the other divided by one. So the one minus the other divided by the one, right. And that ratio between two random or there's going to have a tricky distribution that is not easy to write down, there isn't a good political way of just writing it down. And so I can't make a simple plot of that, at least not this simply, I'll show at the end of the lecture, a plot like this, that shows the distribution of a vaccine efficacy. But it's not going to be just one line like this, because there is no closed form posterior for it. Because the likelihood for our observations, there is no convenient prior to multiplied with, and no convenient integral to do that comes from the standard toolbox to just plot the posterior. So in such situations, you would like to give some answer to what the error bar is going to be. And now we can come back to what we did last last week. So it's a quick reminder, last week, we spoke about the maximum likelihood estimate, which said that if you have to produce a point estimate from your posterior one, not so stupid idea is to find the mode of the posterior distribution or the likelihood if the if the prior is boring, then the mode of the likelihood is essentially the mode of the posterior. And if we showed this already, I already mentioned last week, if that the likelihood has some meaningful properties, if the model is consistent if the number of samples rises to infinity, approximately asymptotically, the point estimate arising from the maximum likelihood scheme and the true estimate are going to be the same under the assumption that the likelihood is correct. If the likelihood is wrong, anything, you know, it might just be wrong. There might be anything. Now if you'd like what I'd like to do today is to talk about how we can quantify how incorrect it's going to be. And I'll actually summarize this again, I already have this here on this slide, I quickly went through it, there is a basic answer that says, actually, actually, I can tell you how the proof is going to work by waving my hands about, which is checked that there are no calls nothing in the nothing in the chat. So if asymptotically as n increases, the you can see me in the here as well, if the the point estimate and the true value get close to each other, then we can do you know, first year undergraduate math, do a Taylor expansion of the log likelihood. Actually, the log posterior, so the joint Subala, what is the posterior the posterior is P of data and variable. So prior times likelihood, divided by some constant which doesn't matter. Right? It's just, it's just scaling. So we can take that joint, marginalize out the data, and then do a Taylor expansion in the difference between the estimate and the true value. If the two are close to each other, which they are, as importantly, then a Taylor expansion to low order to second order is going to be enough. And we're going to get a lock posterior approximation that is a quadratic. So the the approximation if you take the exponential of that is the exponential of a quadratic. And that's a Gaussian distribution. So asymptotically speaking, this maximum likelihood estimate will be distributed like a Gaussian. And the only thing that that doesn't matter is actually like a Gaussian centered on the correct value. And the only thing that matters is how wide is that Gaussian going to be. And that quantity, we have to give a name to it happens to be called the Fisher information matrix. Fancy word. But actually, that's it, we just need the second moment, the second term in the Taylor expansion, the curvature, that's going to be our covariance. So here's the fancy math for that, let me first have to define a little bit of a few objects. And here, okay, this is one moment where I have to take myself and go slow. So we're going to assume, as we have now done throughout the last few lectures, that we're doing IID experiments. So there's a repeated experiment, each of which is independent of the previous ones and identically distributed. So our likelihood is a product of individual terms. That means the log likelihood is a sum of individual terms. And what we're finding is the maximum likelihood estimate. So that's the object theta that maximizes either the likelihood or the log likelihood because the log is a monotonic transformation. And this is what this is, right. So we'll call the log likelihood l n, because it depends on how many n observations you've made. And how you can define an object, I've told you, we're going to do Taylor expansions. So we'll need gradients. And I'll define an object that is called the score function, the score function is the derivative of this log likelihood with respect to the parameter degraded tell from the word score function, that it's really just a technical quantity, it's just something we need. So it's not going to be a particularly interesting interpretation to it. And now, the Fisher information is called is defined as the variance of the score function under the distribution for the data defined by the variable, Sita. So let's go slow. But now what does this mean? So as a function of the unknown quantity, Sita, in our case, it's the unknown probability to be infected, if you are in either of the two groups, as a as a function of that, that that variable, we can, given that variable draw data, right, so if you knew what the probability was to be infected, then you could create an artificial dataset, you could just draw binomial distributed numbers with this parameter. And then under that distribution, you could evaluate the score function for every single datum and sum them up. And you get so that that's some will have a variance, right? Because we get lots of lots of draws of such datasets. And if you compute that variance, if you average over these many, many data sets you might have drawn, you get a number out. And that thing is called the Fisher information. That's a somewhat confusing thing to look at at first, but it turns out that it's actually a really interesting quantity. So you'll deal with that a little bit in your exercises this week, in the theory exercise, and you'll find a few things to find a few nice properties of this object. The first one is that its expected value is actually zero. And it's totally not not obvious intuitively, why it should be. It's really just a piece of, you know, first year undergraduate math. calculus to show that It is, it has nothing to do with that with fancy, you know, statistical quantities, it's really just analysis, or calculus. And because that is the case, you could then show and we'll do that this weekend, your exercises as a second step that this, there's a bit of another way of writing this Fisher information, which is that it is minus the expected value under the data distribution of the second derivative of the lock, like likelihood. So it's either the variance of the gradient of the likelihood, or the expected value of the second derivative of the log likelihood. And at this point, you don't have to understand why those two things are the same. You can do that in your homework. But it's useful to think actually more useful to think about the Fisher information in this form, because what it says is, so we have our p of x and theta, and our joint distribution over the data and the quantity we care about. And what this object does is it tells us, if we integrate out x, if you integrate all the data, what the average curvature of the likelihood is, at any value, Sita, we might want to evaluate it. And now we can use this to quantify how precise our maximum likelihood estimate is going to be. So there's a nice theorem, which I'll quote without proof, but actually, I'll tell you how the proof works. And but I'm not going to write it down, which says this is a kind of like a master theorem for today that under approximate regularity conditions, so you can probably imagine what they are like so that like you has to be twice differentiable almost everywhere, the probability in both in x and in Sita, it's one we can think of the variance of the maximum likelihood estimate the square or its square root, so the standard deviation of the maximum likelihood estimate as the covariance of a Gaussian distribution or in other words, if n goes to infinity, so as you get more and more data points, the standardized distance between the maximum likelihood estimate the thing we can construct, and the thing we'd like to know the actual value of theta is a Gaussian random variable. So that's a fancy way of saying, approximately, if you have many, much data, maximum likelihood estimate is distributed like a Gaussian random number, where the mean of that Gaussian, the center of it is actually the correct value. And a standard deviation is given by the inverse of the Fisher information matrix. And what is the Fisher information matrix? Well, it's the expected value of the second derivative of the log likelihood. So actually, this is also how the proof works. We just repeat, we already knew from last week, that asymptotically, maximum likelihood goes towards the true value. If we are close to the true value, we can do a sec second order Taylor expansion around our estimate. And approximately writes the likelihood as up to second order, right as a quadratic log likelihood as a quadratic function. And that's going to be a good approximation because we are close to the truth. And so if the log like to the separate attic function, a second order Taylor expansion, then the likelihood is the exponential of a quadratic, actually minus a good reading. And that just happens to be a Gaussian distribution. And that's actually, so there's one problem is that there's X in our expansion of the, of the posterior or of the likelihood, so we have to integrate it out. That's why there's an expectation in the Fisher information matrix. But you know, that's it, actually. So, let me just do that actually, on for our, our example. So here's the derivation for this, this Fisher information matrix for the the joint distribution of our binary unknown probability and the observed data, how many people were infected or not infected? So for binomial distribution, the actually I have this further up, maybe I should, you know, like this. Yeah. So this is our distribution, we are dealing with God, I mean, I can just get rid of this. So so this is our binomial distribution. Let's look at we need first the score function. So for the score function, we need the log likelihood for the log of this object, and then we have to take the derivatives of it. So let's first take the log. If you take it Can we get this object here, I'll go slow this year is going to be the logarithm of a binomial coefficient, but there is no F in here, no probability. So it's a constant. For our purposes, I'll put it at the end, just call it a constant, because in a moment, we'll take a derivative with respect to f. And then that constant is not going to matter. Here, if you take the log, we get n times log of f plus n minus m times log of one minus f. Now the score function is the gradient of this object with respect to f. So the derivative with respect to f, well, so the derivative of the logarithm is one over f. And the derivative of log of one minus f is a chain rule, first one over f, and then the inner derivative, which is minus one, so that would be a minus here. And now we can just rearrange, there's different ways of writing the score function. All all three of those are equally valid, they're just rearrangements. This is called the score function. Now, first of all, we'll need the maximum likelihood estimate, we already did that last week, and you did it in your exercises. So I mean, it's also straightforward, we just set this expression to zero. And then you can easily see so for example, over here, right, if you set a zero over here, you can multiply by the thing down here below, unless f is zero, or one, but that's at the corners. And we can find that that's not actually the mode, then we just rearrange and find that f is m over little m over capital N. That's our estimate. Now, we need to compute either the variance of this, or the actually maybe just first first make a little plot. So here is our mode. I mean, that's boring. And that's all it with its mode gate, not surprising. Now we need the, close this. Now we need the Fisher information matrix. So for that, we need a second derivative of the log likelihood, because that's one way of computing it. Either we compute the variance of the score function, we compute the second derivative of the log likelihood and its expected value. So what's the second derivative? Well, we need to take this function and take the derivative with respect to f again, then we have a second derivative. So what is that going to be? It's the derivative of the derivative. And that is, here we have m over F, so you get minus one over f squared times M. And here, we get the same thing minus one over actually, now it's first a plus one over f squared. And then the inner derivative again is a minus, so there's another minus, we have minus minus, you can take the minus outside, remember that the Fisher was actually the minus expected value. So those minuses cancel, we have to compute expected values under the binomial distribution that is just just vanishing up there of this object. So we need expected values for M A, just n actually just m because it's a distribution for how many successes we're going to get if we do a certain number of experiments of this object. And here, conveniently, you can, you could actually, you could do the derivation for yourself. But I can also just tell you that the expected value of m, under the binomial distribution is just the probability times the number of experiments we do. So f times n. And the probability of n minus m, of course, then it's just one minus f times f, that's happens follows by linearity of the expected value. That's also kind of like it's, it's really the result you would expect for the expectation, right, because what else is going to be but you can also do this for yourself, show by actually, it's a simple thing or exercise to show this for yourself, maybe I'll leave that to. And now we plug that in, so we can plug in the values here, we just plug in n times f, so one of the F cancels, and here we plug in n times f, so we can take the N outside of the bracket and one of the one minus f cancel. And we're left with this as the Fisher information matrix. It's N over F times f minus one. And now, the theorem that I showed you said, so this is this here is a function of f, right? Obviously, for different values of f, we get different curvatures. Now, the theorem says that asymptotically, the, the maximum electric estimate is distributed like a Gaussian random variable centered at the correct value with a variance that is given by actually either first point, the inverse of the Fisher matrix at the correct value. But that's a bit useless, because we don't know what the correct value is. But it's also equal to the Fisher information matrix at the estimated value. The reason for this is that we're doing behind the scenes a Taylor expansion to second order, and that's symmetric around which way round we do the expansion is going to have the same curvature if you do it at one point or the other point. And so what we can do is we can evaluate this Fisher information matrix at the point estimate. So our point estimate for F maximum likelihood estimate is m over n, we plug that into F, and we get this as the Fisher information matrix. It's m times n minus m over m cubed. So roughly speaking, this is going to be a function that drops like one over n squared. I, because there's an N in here, essentially write an M will typically grow with n as well, because the expected value is n times f times n. And so one of the ends will cancel, and we'll get a function that goes like n squared, we need the inverse of that to get the variance. So our variance will drop, like one over n squared. So the error, the standard deviation, which is the square root of the variance will drop like one brand. And we can plot that. And I'm actually doing this here. So what you see in this plot is, this is the development piece of Python code, the rest is just plotting, and computing. First, the Fisher information matrices at the maximum likelihood estimates for these two distributions for the control group and the treatment group. And then I'll take this inverse and the square root of this thing to get our aerobar standard deviation. And then plot for plotting purposes, a normal distribution that is centered at the maximum likelihood estimate, and has a standard deviation given by this scale. And those two plots are here on the right hand side ones for the control group and one's for the treatment group. And you can see actually, in the treatment group, you can kind of see it, there's a difference between the Gaussian approximation and the actual likelihood for the control group that like there's sufficient data basically to be pretty much perfect for this approximation. So remember that in the treatment group, we only had eight cases out of 22,000. So eight is still seems a bit shaky, right? They might, they might just be nine or seven, and then it would look different. So we also really close to zero. So this distribution is really not symmetric, the Gaussian has to be symmetric, so we get something that is a little bit off. But for the for the control group read sufficiently far from zero and sufficiently confident with 160 odd people that will kind of know what's going on. Okay, so let me go back. So what So what this essentially means is that, if you can't do Bayesian inference, one thing you can do try to do next is to find the mode of your likelihood, compute this object called the Fisher information matrix of the likelihood, and call the minus the inverse of that the standard deviation, sorry, this the variance of a Gaussian that quantifies the uncertainty over the maximum likelihood estimate. So think, again, about what you have to do to do that. So in this case, might just be able to compute it in closed form, like for this distribution, then you can make a plot directly like I just did. But you might also not be able to compute the expected value in closed form. So what you might then do is, you find your mode, then at the mode, you draw new data sets, compute the hessian of the maximum likelihood estimate at this at your maximum likelihood estimate of this joint distribution, actually, well, the action with respect to just at that raw data set with respect to the parameter, and then average of all of them, that's going to give them Monte Carlo sampling estimate of the Fisher. Alright, that's going to be computationally a little bit tricky, because you have to draw this data. So a very simple thing you could do is to just not draw any data, and just pretend that you only have drawn one data point, one set of data and put it at the most likely value, which is the one you just had, right the data. And you can that you're estimating the entire official entire expected value from a single value, which is just the hessian of the loss of the log loss. So the log likelihood at your actual data set, and this is called the Laplace approximation. It's a simpler form of the Fisher information matrix. And so by the way, so of course, there's a matrix version of that if you have multiple variables. And just like there's a multivariate version of the Taylor Taylor's Theorem, there's a multivariate version of the Fisher, which is involved this matrix here, the official information matrix. And if you don't do this expected value, if you just leave it out, then you're doing something that that lasted, what 200 odd years ago. So I'm actually behind you, you can see that the two pages where he does this, don't have to read the whole thing. But up here you can see the value that is most probable is the one which maximizes the likelihood. And then how do we compute the shape of the likelihood and afterwards? Well, the class notice this without knowing There's this annoying data integral that he doesn't know how to solve. And then he gets busy doing Taylor Taylor expansions, which you can see down here and behind the gray bar, and then find some, you can sort of see here, the first and second order terms in the expansion, he does them and then find the value in the end. And so what that means is actually he makes a plot mentally, just like the one we did here, just without the averaging over data, but essentially, you're going to get pretty much exactly the same plot. Because at this regime, we have such high confidence about the those two values that those approximations are essentially the same. So here in this simple setting, and a phase three trial for a vaccine candidate, all of these approximations are pretty much the same. But imagine you're doing this with a fancy big AI model, some deep machine learning problem, then you're not talking about a single variable, but about maybe a million or a billion variables, the weights of your neural network, you'd like to know how confident you are about those weights. And you can still do the same thing, you find the mode of the log likelihood, what is that, but it's the training point of your neural network, those of you are taking the deep learning class know that that's what you do, right, you just minimize the loss, you can think of the loss as a log likelihood, you find the train point of the of the deep net, and then at the training point, compute the second derivative of the loss via automatic differentiation. And that gives you a matrix to invert that matrix, you might have a minus in front, that's your covariance of a Gaussian, approximate posterior. So these, this approach scales all the way from scalar variables to pretty much arbitrarily complex problems, as long as the likelihood is sufficiently regular, as long as you can compute second derivatives basically. Okay, so maybe it's a good point, at this point to ask if you have any questions, because otherwise I'll move to the final, most most trivial trick. Hmm, yes. Yeah, actually minus the expected but that's, it's just assignment. So so far, our story has been? Could you repeat? Oh, sorry. So first of all, there was a question in the audience that I should have repeated. The question for everyone in the call, which is still at three people, it was. So just just to make sure we understand. In the multivariate case, if you have if you're talking about multiple variables, the Fisher information matrix is the expected value of minus the Hessian, so that matrix of second derivatives of the log likelihood. And as a question in the chat, could you repeat how the variance is approximated was one over the log likelihood? So I'll go back to the following direction. This so what the theorem says is actually this is this equivalently I shouldn't write on the blackboard. But So equivalently this theorem, says, Sita, the thing we don't know can be approximated if we have enough data as a Gaussian when the variable within a mean given by theta hat, the estimate and a variance given by the inverse negative Fisher information matrix. So the inverse of the hessian of the picture for this, okay, I'll draw something on the blackboard anyway. And I'll try and draw it as big as I can, so that maybe you can even see it through the camera of this call. Is so there's a posterior or likelihood it looks like this. We take the logarithm of it. So if you take the logarithm of this, it's going to look like something like this. Right? You find the mode, which is at the same point here and over there. At that point, compute a second order Taylor expansion. That's a parabola. And then, we go back up Take the exponential. And the the exponential of a quadratic function is a Gaussian distribution. And that's it. That's the entire thing. So Fisher and score and they all just big fancy words. The reason we're allowed to do that is because we know that if you have enough data, this point estimate, and the true value we're looking for, will be close to each other. So this quadratic approximation will be decent. But early on, of course, the true likelihood might not look like this at all, it might actually look. It might look like this. And then you better not make a Gaussian approximation. Because you can't even there isn't even a second derivative to compute. Right? Or maybe just like this, there's no counter proximation to make here. But once we have seen a lot of data, we'll be very confident about the correct value. And then it makes sense to make a Gaussian approximation. And that's what Laplace did, because Laplace was concerned with, you know, predicting whether the sun's going to rise tomorrow, and he'll assume he assumed he had enough data, and then it's all fine. Ah, so that's a very good question, actually. So the question for everyone in the chat is, how do we know that there is enough data? And also there's a there's a question in the chat. That is, actually okay. I'll quickly do a question in the chat, because it's easier to answer. Is this connected to the central limit theorem? Yes, it is. Actually, the proof requires the central limit theorem. But the size limit theorem is essentially also a Taylor expansion. It's just another convoluted way of talking about Taylor expansions. It's just if you have if everything is big, right, then the region about which we're doing this approximation is small. So the quadratic approximation is going to be good and asymptotically. Everything is Gaussian. And so now, the question here was, how do we know that we have enough data? Hmm. So I'll give you the right answer. You do Bayes theorem, you think about the posterior. But of course, the reason you're doing this in the first place is that you think you can do Bayes theorem. So this is in this setting. It's easy, because we can plot everything. I can tell you that in deep learning, this is a genuine problem that people at the moment are discussing in the scientific community. So there's a there's a lot of research at the moment on Bayesian deep learning, how do we assign meaningful uncertainty to deep neural networks? This is important because uncertainty is important for everything. If you have uncertainty, you can, you know, be robust to adversarial attacks to outliers, you can think about privacy and precision of the estimates. And you can you can use it to control how aggressive your self driving car drives, or whether it stops and switches itself off and so on. And there, the question is very much. So first, how do we construct approximate posteriors last? But then how do we know that it's good. And then suddenly, things typically get really expensive. So what you can do is maybe do this like, offline on a big data center, you rerun the experiment a lot to try to find you try to explore the loss landscape around basically you want to check whether the posterior is actually develop approximated by a Gaussian. And in 1d, this is relatively easy, you can think of like computing a third term and the Taylor expansion and check how large it is relative to what it should be under a Gaussian. But the third term in a high dimensional space is going to be a tensor with cubically many entries in the dimensionality of the problem, so you can't just hope to evaluate that in closed form. There are more questions up there. Ah, so you're raising a very smart point. That one thing you could do and actually this is something that is sometimes done, if the distribution is not very Gaussian, you could imagine that you know, if it looks like maybe like this, or like this, or actually even like this, then maybe there is a transformation of this space, this 01 simplex, such that this distribution does look more like a Gaussian, or it looks more like something we can do attractively so the first thing is maybe you can just think about attractively like in this case, I mean, the beta distribution is tractable to modern computers. But in the case where it's not, then you somehow have to come up with a representation of the posterior that is somehow well approximated by something that you can track. And Gaussians are just really convenient because Gaussians are defined by matrices and vectors. And these are objects that computers are really good with. Right? That's the one thing that GPUs are really good at are multiplying matrices vectors, so Gaussians are the correct quantity for this. Another thing that computers are really good at is adding floating point numbers. And that's what you need to do for this exercise. So that's also good. That's another good distribution. In fact, there's a whole slew of such good distributions. They're called exponential families. And Professor Mackey is going to tell you about the next term in his probabilistic machine learning class, I guess, I don't know. But maybe. Okay, so there's a question in the in the chat as well, and then one here, and then I'll move to the final bit I want to do. So in general, someone writes in the chat, it can be said that the data in the study was sufficient, because the variance seems pretty, pretty low in the plot. Nope. Ah, so actually, let me go that I forgot to actually do this. Let's talk about this for a moment. So there's a line up here that I haven't actually talked about yet. And it's produced by this piece of Python code here. Oops. There we go. So the question, of course, is, so how confident actually, are we about this estimate of 95% for the vaccine efficacy. And so far, these plots are only about these two variables, P of being infected, given that you're treated, P of being infected, given that you're not treated, what you're looking for is the difference between the two divided by the control groups probability to be infected. And so ideally, we'd like to compute the posterior of that variable. And that will require us to write down a likelihood at a prior for this variable divided by the evidence, the problem is that the likelihood function, the function that predicts the outcome of these experiments, given this vaccine efficacy, is going to be a little bit tricky. Why because there is this p of being infectious and effective given that you're in the control group in the denominator. And that's nasty, which just makes the distribution have a weird shape. We'll see the distribution in a moment, because I'll produce it, but it's just going to make it numerically a bit a bit complicated, we're going to get an integral in the normal nominator with a rational function. And you know, from your, you know, first your undergraduate math courses, that integrals of rational functions are a bit of a pain. So it's going to be tricky to do this in closed form. But what we can do is you can say, Ah, so we could use this Gaussian distribution that we just constructed with our Fischer or Laplace approximation, and use it to construct credible ranges, like almost worst case bounds on what the value might be not actually worst case, because worst case, might I mean, distributions have full support on 01. So the worst case is that we're at zero and one, but that those are silly, you can sort of take the regions that contain 95% of the mass of this distribution. So that for Gaussian is mean plus minus two standard deviations. Or maybe just one standard deviation, even in the quoted, I would, and use them to construct credible ranges. And then on those credible ranges, consider the worst case possible combinations to get bounds. So this is a very pedestrian thing to do. And it's actually what I did here in this piece of code. So we construct, we have a point estimate, it's called F H, C for the control group and F h t. So f hat treatment, f hat control for the two groups. And now actually, we just return that. So the maximum likelihood estimate is 95.06%, for the vaccine efficacy, so what is that it's just the difference between the two maximum likelihood estimates divided by the maximum likelihood estimate for the control group. That's the 95%. That was in the press that everyone heard about. And now we could say, what would what would be the vaccine efficacy if actually, there was like the worst kind of combination of errors. So we are over estimating the probability to be infected if you're under control group, but under estimating the probability to be infected, if you are in the treatment group, so we have to add and subtract one. So right, so the probability to be infected is lower in the treatment in a control group and higher in the treatment group, divided by the the number that makes this even worse, make it larger, and then you get a lower vaccine efficacy estimate, which is our 90 90% estimate. And if you do the other way, round, right, we just switch all the signs around so that you get the best case estimate, we get something over 100%. Actually, interestingly, it's over 100%. Because the Gaussian distribution doesn't know anything about the fact that our distributions are bounded on 01. So it will happily put mass outside of the of the domain. And, and we'll get an estimate of over 100%. So this these are actually you may remember numbers that were in the press as well. So if you follow the news a year ago, you might have heard that this new vaccine can always Oh 95% effective, and actually, it may be even better or a little bit worse, but probably better than 90%. And that's exactly how those numbers reconcile actually not exactly how to construct it. We'll talk about how they are constructed in a second. Unless there's one more question, and then yes. So yeah, so here, I, this is the standard deviation, the square root of the inverse Fisher, because the inverse Fisher is the variance and the square root over the standard deviation. And yeah, I should have put the two in here. Okay, so homework exercise, if you want to, like twos everywhere in here, there and bear, and you're so in front of FAS, if you just multiply by two. And actually, this is not going to be the same estimate, right? Because you're not multiplying in all the F's on your DS. So you can't take the two outside and cancel it, it's got to be a different estimate. But it's probably got to be very similar. And Mississauga in the chat also asked, Could you please restate the relationship between official information and the Laplace. So it's, I mean, it's essentially just this. So you, Laplace is one one easier, because we just don't take an unexpected value over the data, we just take our current data that we actually have, and say, That's the expected data. So one way to motivate that is to say, Well, if the data is large, it's going to be typical, it'll be close to the expected value. Another way to think about this, which neatly leads into the next final segment of this lecture, is, you could think of your data set as a draw from the distribution of all data sets. And then think of the point mass that is defined by that one by our distribution, and sorry, the point mass that is our data set as an approximation to the entire data set. So that sounds silly, but it's going to lead to the thing we'll do next. So there is a distribution over data. It's complicated, and we get one data set. So this point, mass is the crudest possible approximation, you could have to the entire distribution. And it's stupid, because it's just one point. But it's also not totally bad, because it's a draw from that distribution. So it's not going to be here or there. Because that happens, very unlikely. So that's the motivation to do lots of observations. Another way to motivated this argument by authority is to say, Laplace did it and 17 something and can't be that bad. Also, there's lots and lots of empirical evidence that this actually works really well. Okay, so now we've we've encountered three different two different things. So far, actually three, maybe, or two and a half, for how you can quantify confidence about an estimate. The first one is Bayesian inference, right on the full posterior, and just show it to the world and said, This is what I know. And that's the correct answer. If your model is correct. The next best thing is to compute your estimate, compute the second derivative of the log likelihood adds your estimate, inverted yt minus in front and call that the covariance of a Gaussian distribution for your estimate. And if you take the expected value of all possible data, that's called the Fisher information matrix, if you don't take the expected value, this is called the Laplace approximation. And if there's enough data, they're very close to each other. Now, actually, so at this point, people used to say, oh, but you can't always compute your hessian, it might be complicated. Writing donations is so difficult, and they're matrices, and they're so high dimensional. And actually, that's not right anymore, thanks to automatic differentiation, and packages, like pytorch, and TensorFlow and Jack's, and what they call computations is actually straightforward these days, even for deep neural networks. So there are libraries now, including actually some from my own research group, it just helps you do that in one line of code. So we should just be doing this. However, there is a even all the way. That is if you really don't want to put in the time to do automatic differentiation on your code. You don't want to re implement your code in jacks or in pytorch, or whatever. You really just want to get a confidence estimate, in the simplest possible sense from the perspective of the developer, the person writing the code, not from the computer's perspective, then there is something called the Bootstrap. And it stems from the 70s 80s or late 70s. Actually, it was probably around for longer, but the word bootstrap arose first in this paper, and then I think 17, and it's a really, really simple idea. It's based on this idea, just I just drew on the on the Blackboard as well. So okay, we'll read through this in a moment, I'll just tell you the story. What you could do is you just take your dataset, and you use it to imagine new datasets and how do you do that you just draw with replacement from your data set. So your data set consists contains n realizations. You take those and draw in new realistic By sampling with replacement from that array, and then you re compute what your maximum likelihood estimate would look like on that withdrawn data set. And you do it again and again and again and again. And that'll give you a distribution of estimates. And those are called the bootstrap estimate, Bootstrap, because of the saying, of pulling yourself up by your own bootstraps, right? You're taking the data that you have, and you're imagining more data from it without actually getting more data. And the motivation for this, why it is maybe a good thing to do is actually really, really hand wavy. It is, well, if you have a data set, that is consists of a bunch of direct measures at points, then the density of those direct measures is approximately the true density. So by drawing from those discrete instances of data, you're almost drawing from the real distribution of data. But of course, this is only true, if you have a lot of data. Right? If you only have three data points, then this is really wrong. So you shouldn't be doing that if you don't have enough data. But in our case, in this phase three trial, we have, you know, 22,000 people, so, and eight cases in each, maybe we're allowed to do that. So here's the slightly more fancy way of saying this. So consider a data set that is IID drawn from some unknown distribution, p of x given theta, the data set contains n numbers. And we have a way of constructing an estimator from this data. So for example, maximum likelihood, you take x, you plug it into our likelihood function, you maximize it, you get out theta hat, this is called T, this T is called a test statistic, just a number that you need to compute. So now a simple way to do confidence estimation is to simulate experiments for P from one to capital B by drawing the instances of the data in this like in this imagined simulated data set with replacement from the original data set. Now for each such imagined data set, we can apply the function t get an estimate. And then that's a distribution and empirical distribution over the estimators you might want to look at. And you can compute whatever you like of that distribution, for example, you could compute the variance of our estimator by computing a variance. So that's an unbiased estimate of the variance as we saw last lecture, or two lectures ago. This is called a bootstrap. And the motivation for it is that we can think of it as Yeah, approximating the true distribution of data sets with a bunch of Dirac deltas. So what would that look like, for our, for our beyond tech experiment, so here is the Python code for this. And again, I should tell you that you know, what to look for. This is the actual code, everything afterwards, it's just making plots. So what I'm doing is I'm saying, first of all, we can talk about how large we want our resampled data sets to be. So the natural thing to do is maybe to say that the resample data set should be as large as the original data set. In this case, NC is the same as empty, it's 22,000 22,000 people. But you could make it smaller, just as an interesting thing to do not not to quantify uncertainty over your point estimate, because then you should be set it like this. But you can by reducing that number, you can basically imagine what your confidence would look like if the original data set were smaller. And that's an interesting thing to do. And I recommend you try this for yourself with this code. We're actually also with the exercise code that you get this week, because so spoiler alert, the exercise sheet, the coding exercise, on this week's exercise sheet, is pretty much exactly this with a different data set with the federal elections of this year in Germany, and the only difference is that you will have multiple parties rather than two possible outcomes. So I'm telling you that because a maybe that gives you the feeling that this is an easy exercise, and it is if you've seen this, but you have to go through this to understand it. And I recommend that you try that it's an easy bonus point for the exam. So maybe this is getting you to actually do the coding exercise this week. Okay, so we need to decide how many such bootstrap samples we draw. So not how many data points will be in each resampled data set. That's the number line above. But how many imagined data sets are we going to create? And because this is going to produce a distribution, we should probably set that large. So I'll first say, you know, maybe we draw 200 data sets. Let's hope this just works. Yeah, that it'll look like this. So each so what I do is okay, so how do we actually do this? So I I do Okay, this is slightly fancy NumPy. So I produce a random choice. So a random set of indices of an array of length, and C, so that's the length of the control trial. So I'm drawing indices from the 22,000 different people out because the names of the people are irrelevant, I could arbitrarily permute them, I could just say is actually, I've taken the data set, which is 22,000 people in it. And I've rearranged it so that the people who are infected are at the front, these 160 odd, they are the first 160 Odd indices, and everyone afterwards was not infected. And then this is a way of drawing such a dataset. So I draw random indices of this length of the right size. So this is a matrix of that size. And then just check which of these, that's the final line here is less than the actual number of infected people in that arm of the trial. And that's my new V samples case counts in that in the control trial, and the same for the treatment arm, and then I compute maximum likelihood estimates for both the treatment and the control arm probability to be infected, and then compute vaccine efficacy is for those maximum likelihood estimates. So these difference divided by visual, and then I make a bunch of plots. So here you see those as dotted grey lines, equate green dots, the the values of those maximum likelihood estimates for ratio of infected or probability to be infected in the control and treatment arm. And in red, I've drawn in the Gaussian approximations from the our official business above, you can see that those lines line up nicely, it's kind of a good approximation of a Gaussian distribution. And here's a histogram of those maximum likelihood estimates for the vaccine efficacy in orange, and in black is our maximum likelihood estimate for it 95% And in dashed our pedestrian constructed worse than best case bounds at 100%. And 90%, you can see that this distribution is a bit wacky, because I've just drawn 200 points, if I draw even less, it's going to be even worse, right? So it will be pretty bad. But if I draw a lot, I'll get a nice distribution. It also takes a while to run this. And it's going to be a nice kind of bumpy distribution for what the actual vaccine efficacy probably is. And you can see that this is not a Gaussian distribution. Because as I said before, this distribution doesn't have a simple closed form, shape. And you saw that it took me a while me this computer here, maybe two seconds to do this computation. For such a simple thing right there, just four numbers that make up the entire problem 22,000 times two, and eight, and 160 something. Now imagine how much harder it is to do this with a deep learning problem, where each little green dot there would be a retrained deep neural network on a reimagined data set. So if each of those gray dots takes three days to train, you're going to need a lot of co2 to produce this distribution. And you'd rather not do that. So instead, you'd rather do something like Laplace, which gives you this red cross in the middle. It's less kind of, like white. So if you derive estimates from that, they're going to be less expressive. It's just this these black bars, but you know, they're not that bad given that you can do them in one go rather than into the four. Okay. What else did I want to say about this? Ah, so maybe you can just try that. Imagine that you what would happen if he replaced if he imagined that we had a smaller data set? So we do something like this? Oh, it's not even going to work. Why? Because there's probably a divide by zero. Yeah. So why is that? Because if I draw 1000 People from the particular treatment arm in the original data set, there are only eight out of 22,000 who are were infected. So if I redraw a sample of size 1000, probably quite typically, I'll get the data set that doesn't contain anyone who was infected. So it looks like that estimate will be 100%. And then there's probably going to be a divide by zero error somewhere. But if I go to 10 to the four Okay, so now we'll get an estimate. But you can see that our estimates for the, the probability to be infected, have this these stratified lines in between. And those are discretization arrows caused by the fact that we're redrawing datasets from a finite set of examples, right? So they'll typically be For 5678 cases, maybe nine or 10, depending on how we besonders. And that's going to give these specifications and you can see that they are only arriving in the treatment arm on the controller, because the control arm has 160 odd people. And there we already smoothing out the solution. Okay, so that's why it's like you shouldn't be doing this, if the data set is too small, you might just get an error actually a divide by zero bug or something that is very simple to do. If you have code that computes the point estimate, it's trivial to rewrite it to produce this uncertainty estimate. And now let me just finally compare this to the actual answer you would get if you do this in a Bayesian fashion. So how you would do with the blues innovation fashion, I can do this in like just three lines, I've already told you how to compute the posterior for these two distributions, P of infection given treatment and P of infection given control. They're both these beta distributions that we've plotted before. So I can actually plot those this posterior distribution, because it's independent for the two arms independent by design, because the trial was designed as a randomized control trial. So I'll get these two estimates. And I'm just plotting here, basically, two such lines, right. So each of these, each of these distributions looks like like one of these. And I'm just plotting them as an outer product. Next to each other, actually, I even do this somewhere here, I literally compute the outer product of these two PDFs, and plot them, you can see this this black distribution. And you can see that this is basically the same distribution that we have above, it's just much, much easier to interpret much nicer because it's closed form. That's the full Bayesian answer. And now we can do that from here on out, we can do the same as on the line above. And I actually do this, here, I literally pretty much copied over stuff, like the plotting code is just copied over. So instead of drawing with replacement, I'm just asking this Peter Pusteria. To draw random numbers, this is this RVTs is random variants, or random variables, I don't know, I'm being being drawn of the right size, and then compute this estimate again, and you get this orange empirical distribution. It's the same number of samples as in the plot above. And you can see in the in like faint behind, that the distribution from the bootstrap estimate is very similar as a tiny difference, it's just maybe maybe arguably a bit smoother, because this works in closed form. Now, maybe as a homework exercise, if you want to play with this code, think about a setting in which these two estimates will be quite different. But I can already tell you what it is, it's the case where you have low data, right? If you don't have a lot of data, if you're not super confident about what's going on, then those distributions will look quite different. And you can revamp the code, you can change the obvious numbers in the code to produce these kinds of plots. And then eventually, you might hit a case where the Bayesian answer and the bootstrap answer actually quite different. can already give you a hint that that sort of things you might want to look out for. We had estimates at 100%. Or further down below, right, where you get weird spreads of the this kind of bootstrap estimate for the confidence because of discretization errors, basically. Okay, so that's it. If you can always do Bayesian inference, someone wakes you up at night and says, I have an inference problem, what do I do Asian inference. But then, people will ask you to make point estimates. And when they do that, you return maybe as a first knee jerk reaction, a maximum likelihood estimate. Why, because as an typically, it's going to be like a Gaussian distribution around the true value. If you have enough data, I Gaussian distribution that has a mean given by the true value and the covariance that is given by the inverse of the negative Fisher, no, actually just the inverse of the future, because the future is the negative ation of the log likelihood. And if you can't even do that, if you can't even compute expected values of a second derivative, well then just compute the second derivative of your data set. That's like pretending your data is one sample from a distribution. And you're computing a sampling estimate from a single sample. That's called the Laplace approximation. And this is the method of choice for uncertainty calibration, or quantification in super complicated models, like deep neural networks, for example, or large, really large simulators, as long as the likelihood is sufficiently smooth. If the likelihood is not smooth, if you have it has weird discontinuities, if the code you're using to compute the estimate is has branches inside where you can't even take the derivative through, then things get really tough. And then maybe you want to ask Professor maca about how to do that, because he's the expert on it. And finally, if you have a piece of code that you really don't know how to compute gradients of, what you can do is you can just imagine a few new data sets, data sets large enough you can do that just run the entire code. And you use it to produce estimates, samples. And that's called the Bootstrap. Or am I allowed to call it simulation based inference? No scattered the qualities of it. Okay, good. Okay, that's that's the story of how to how you can construct aerobars for your estimates. Next week, we'll talk about how to test hypotheses. And that's going to be the end of our three week, rapid tour through all of statistics. Essentially, I like to ask you to please provide feedback, as always, by scanning this QR code to that now or go on EBS, and you know, go on the video, it's visible in the feedback feedback folder. Please do that now. And I hope that everyone has had it. Okay. I'll wait for a moment to let you that you take your QR code picture. And as always, as every week, the final point is maybe an item for your calendar this week. We'll have a talk on Wednesday, by Martin tap, actually, he's going to be given his talk in person he's coming to visit us from Aalto University in Helsinki in Finland. On Wednesday, he's going to talk at the lecture hall of the Max Planck Institute for intelligent systems, but also on Zoom. You can also find the Zoom link on the on topstar too. And he'll talk about connections between Bayesian nonparametric and deep architectures. So if you care about large scale Bayesian inference, in particular in with lots and lots or infinite numbers of parameters, maybe this is an interesting talk to attend. Are there any questions? Waiting for someone to type something in the chat? No, then thank you very much. I hope that you'll find a bus that brings you back home if you're looking for and also, thank you very much for the call. And goodbye. I hope that I see many of you next week here in the lecture hall again when the buses are not striking anymore 

