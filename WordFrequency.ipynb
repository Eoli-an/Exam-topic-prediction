{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordFrequency.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CovpFnO2x_Od",
        "KbczwHhX7R62"
      ],
      "authorship_tag": "ABX9TyPgAU34+j7vHaplaZenCoLf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eoli-an/Exam-topic-prediction/blob/main/WordFrequency.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Files"
      ],
      "metadata": {
        "id": "xRoQ15RIxyTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM6ZNhrtxh2a",
        "outputId": "35147593-c1bf-4bb3-bdd4-93dd68ed3d02"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_text_file(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        return(f.read())"
      ],
      "metadata": {
        "id": "Ub7uMmF-v4DT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lectures = [read_text_file(os.path.join(\"Transcribes\",file)) for file in os.listdir(\"Transcribes\")]"
      ],
      "metadata": {
        "id": "gfwIustEwqVA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = [file for file in os.listdir(\"Transcribes\")]\n",
        "#a.sort()\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjIQOUJKEDg1",
        "outputId": "1660945e-3350-495e-d0eb-1c6fcaa81141"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Lecture_07.txt',\n",
              " 'Lecture_11.txt',\n",
              " 'Lecture_05.txt',\n",
              " 'Lecture_08.txt',\n",
              " 'Lecture_03.txt',\n",
              " 'Lecture_09.txt',\n",
              " 'Lecture_04.txt',\n",
              " 'Lecture_02.txt',\n",
              " 'Lecture_06.txt',\n",
              " 'Lecture_10.txt',\n",
              " 'Lecture_12.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "['Lecture_07.txt',\n",
        " 'Lecture_11.txt',\n",
        " 'Lecture_05.txt',\n",
        " 'Lecture_08.txt',\n",
        " 'Lecture_03.txt',\n",
        " 'Lecture_09.txt',\n",
        " 'Lecture_04.txt',\n",
        " 'Lecture_02.txt',\n",
        " 'Lecture_06.txt',\n",
        " 'Lecture_10.txt',\n",
        " 'Lecture_12.txt']"
      ],
      "metadata": {
        "id": "eXSaSzFiEwGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploration"
      ],
      "metadata": {
        "id": "CovpFnO2x_Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word frequency\n",
        "tokens = nltk.word_tokenize(lectures[0])\n",
        "freq = nltk.FreqDist(tokens)\n",
        "freq = sorted(freq.items(), key=lambda item: item[1])\n",
        "freq[-100:]\n"
      ],
      "metadata": {
        "id": "tT3Ydmg8zbme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word frequency of nouns\n",
        "# TODO rather restrictive, i.e. regression and logistic regression are two different words\n",
        "blob = TextBlob(lectures[0])\n",
        "freq = nltk.FreqDist(blob.noun_phrases)\n",
        "freq = sorted(freq.items(), key=lambda item: item[1])\n",
        "\n"
      ],
      "metadata": {
        "id": "Hj7RzuBZ13Ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO does not rlly work\n",
        "is_noun = lambda pos: pos[:2] == 'NN'\n",
        "tokenized = nltk.word_tokenize(lectures[0])\n",
        "freq = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
        "freq = nltk.FreqDist(tokens)\n",
        "freq = sorted(freq.items(), key=lambda item: item[1])\n",
        "freq[-20:]"
      ],
      "metadata": {
        "id": "fGhg19Ms2viF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokens with overall highest frequency"
      ],
      "metadata": {
        "id": "KbczwHhX7R62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined = \"\"\n",
        "for l in lectures:\n",
        "  joined += \" \" + l\n",
        "blob = TextBlob(joined)\n",
        "freq = nltk.FreqDist(blob.noun_phrases)\n",
        "freq = sorted(freq.items(), key=lambda item: item[1])\n",
        "freq[-20:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PI7nkRxX7Se5",
        "outputId": "026e96e1-c005-46ee-bbbf-04ab13c75755"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"'s kind\", 32),\n",
              " ('maximum likelihood estimate', 32),\n",
              " ('null hypothesis', 33),\n",
              " ('python', 34),\n",
              " ('log likelihood', 35),\n",
              " ('logistic regression', 35),\n",
              " ('machine learning', 35),\n",
              " ('ai', 40),\n",
              " ('fisher', 42),\n",
              " ('oh', 46),\n",
              " ('linear regression', 52),\n",
              " ('germany', 52),\n",
              " ('bayesian', 60),\n",
              " (\"ca n't\", 72),\n",
              " ('well', 78),\n",
              " ('yeah', 79),\n",
              " ('pca', 85),\n",
              " ('right', 125),\n",
              " ('gaussian', 153),\n",
              " ('okay', 434)]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# absolute occurances of sensible noun phrases of top 50\n",
        "# 9844 noun phrases\n",
        "# (trivia - he said okay 434 times)\n",
        "result_all = {\"gaussian\":153,\"pca\":85,\"bayesian\":60,\"linear regression\":52,\"fisher\":42,\"ai\":40,\"machine learning\":35,\"logistic regression\":35,\"log likelihood\":35,\"null hypothesis\":33,\"maximum likelihood estimate\":32,\n",
        " \"iid\":31,\"standard deviation\":28,\"linear function\":28,\"principal component\":21,\"random number\":19,\"high dimensional space\":19,\"dimensional space\":19,\"laplace\":18,\"p value\":17,\n",
        " \"control group\":17, \"monte carlo\":16,\"information matrix\":15,\"linear model\":15}\n"
      ],
      "metadata": {
        "id": "AdfD15wX9UBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# High frequency tokens per lecture"
      ],
      "metadata": {
        "id": "WZxjjvnj865g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lecture 07\n",
        "# 870 noun phrases in total\n",
        "[('binary distribution', 3),\n",
        " ('regression function', 3),\n",
        " ('% probability', 3),\n",
        " ('negative log likelihood', 3),\n",
        " ('log probability', 3),\n",
        " ('odds ratios', 3),\n",
        " ('poisson', 3),\n",
        " ('decision surface', 4),\n",
        " ('binary outcomes', 4),\n",
        " ('bernoulli', 4),\n",
        " ('average weight', 4),\n",
        " ('linear regression model', 4),\n",
        " ('causal link', 5),\n",
        " ('exponential family', 5),\n",
        " ('maximum likelihood estimation', 6),\n",
        " ('bayesian', 6),\n",
        " ('link function', 6),\n",
        " ('linear model', 8),\n",
        " ('omega', 9),\n",
        " ('regression model', 10),\n",
        " ('nonlinear function', 10),\n",
        " ('log likelihood', 11),\n",
        " ('linear regression', 17),\n",
        " ('linear function', 19),\n",
        " ('logistic regression', 28),\n",
        " ('gaussian', 31),]"
      ],
      "metadata": {
        "id": "qx9SjCNVAzs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lecture 11\n",
        "# 966 total\n",
        "[('unit tests', 2),\n",
        " ('% confidence', 2),\n",
        " ('design process', 3),\n",
        " ('open science', 3),\n",
        " ('real world', 3),\n",
        " ('involves data', 3),\n",
        " ('software engineering', 3),\n",
        " ('machine learning algorithm', 3),\n",
        " ('neural network', 3),\n",
        " ('pdfs', 4),\n",
        " ('deep neural network', 5),\n",
        " ('pdf', 7),\n",
        " ('github', 7),\n",
        " ('ai', 8),\n",
        " ('machine learning', 8),\n",
        " ('git', 23)]"
      ],
      "metadata": {
        "id": "uEuOA_cyBj6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lecture 05\n",
        "# 874\n",
        "[('true positive rate', 2),\n",
        " ('false positive rate', 2),\n",
        " ('qr', 2),\n",
        " ('base explanation', 3),\n",
        " ('possible explanation', 3),\n",
        " ('iid', 3),\n",
        " ('control group', 3),\n",
        " ('study protocol', 3),\n",
        " ('exact test', 3),\n",
        " ('probability f.', 3),\n",
        " ('unknown variable f', 3),\n",
        " ('power times', 3),\n",
        " ('classification algorithms', 3),\n",
        " ('vaccine efficacy', 4),\n",
        " ('treatment group', 4),\n",
        " ('machine learning', 4),\n",
        " ('control trial', 5),\n",
        " ('rejection region', 5),\n",
        " ('beta distribution', 5),\n",
        " ('mt', 5),\n",
        " ('p values', 5),\n",
        " ('roc', 5),\n",
        " ('fisher', 6),\n",
        " ('treatment arm', 6),\n",
        " ('control arm', 9),\n",
        " ('bayesian', 11),\n",
        " ('gaussian', 11),\n",
        " ('p value', 11),\n",
        " ('mc', 12),\n",
        " ('null hypothesis', 23)]"
      ],
      "metadata": {
        "id": "3ZI57SgYCBww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lecture 8\n",
        "#total 980\n",
        "[('who', 3),\n",
        " ('generative dimensionality reduction', 3),\n",
        " ('orthonormal basis', 3),\n",
        " ('inner derivative', 3),\n",
        " ('optimal choice', 3),\n",
        " ('svd', 3),\n",
        " ('low dimensional representation', 3),\n",
        " ('geoff hinton', 3),\n",
        " ('global structure', 3),\n",
        " ('tc', 3),\n",
        " ('qr', 4),\n",
        " ('quadratic loss', 4),\n",
        " ('loss function', 4),\n",
        " ('reconstruction error', 4),\n",
        " ('singular value decomposition', 4),\n",
        " ('standard deviation', 4),\n",
        " ('dimensionality reduction', 5),\n",
        " ('low dimensional space', 5),\n",
        " ('eigen', 5),\n",
        " ('euclidean', 5),\n",
        " ('gaussian', 7),\n",
        " ('fisher', 7),\n",
        " ('lda', 8),\n",
        " ('principal components', 11),\n",
        " ('dimensional space', 16),\n",
        " ('high dimensional space', 17),\n",
        " ('pca', 44)]"
      ],
      "metadata": {
        "id": "opfS8VEeCjRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lecture 3\n",
        "# total 838\n",
        "[('maximum likelihood', 2),\n",
        " ('machine learning class', 2),\n",
        " ('x i', 2),\n",
        " ('s square', 2),\n",
        " ('consistent estimate', 2),\n",
        " ('binary logarithm', 3),\n",
        " ('conditional distribution', 3),\n",
        " ('binary questions', 3),\n",
        " ('information gain', 3),\n",
        " ('euclidean', 3),\n",
        " ('standard deviation', 3),\n",
        " ('kl', 3),\n",
        " ('random numbers', 4),\n",
        " ('iid', 4),\n",
        " ('information content', 4),\n",
        " ('log likelihood', 4),\n",
        " ('sufficient statistics', 4),\n",
        " ('qr', 5),\n",
        " ('monte carlo', 5),\n",
        " ('square root', 5),\n",
        " ('shannon', 5),\n",
        " ('conditional entropy', 5),\n",
        " ('sigma square', 7),\n",
        " ('bayesian', 9),\n",
        " ('maximum likelihood estimate', 9),\n",
        " ('random number', 12),\n",
        " ('possible outcomes', 12),\n",
        " ('gaussian', 12)]"
      ],
      "metadata": {
        "id": "PEr6o_SoFCtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blob = TextBlob(lectures[4])\n",
        "freq = nltk.FreqDist(blob.noun_phrases)\n",
        "freq = sorted(freq.items(), key=lambda item: item[1])\n",
        "freq[-50:]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuw6zh77AhDL",
        "outputId": "8a79bc9e-0e86-4700-a853-4e91ed01d4a7"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('maximum likelihood', 2),\n",
              " ('machine learning class', 2),\n",
              " ('whole data', 2),\n",
              " ('x i', 2),\n",
              " ('s square', 2),\n",
              " ('consistent estimate', 2),\n",
              " ('design experiments', 3),\n",
              " ('actual value', 3),\n",
              " ('binary logarithm', 3),\n",
              " ('conditional distribution', 3),\n",
              " ('binary questions', 3),\n",
              " ('possible answers', 3),\n",
              " ('information gain', 3),\n",
              " ('scale tips', 3),\n",
              " ('blackboard', 3),\n",
              " ('left hand side', 3),\n",
              " (\"'s straightforward\", 3),\n",
              " ('tricky business', 3),\n",
              " ('annoyingly', 3),\n",
              " (\"n't matter\", 3),\n",
              " ('euclidean', 3),\n",
              " ('standard deviation', 3),\n",
              " ('kl', 3),\n",
              " ('n times', 3),\n",
              " ('zoom', 4),\n",
              " ('random numbers', 4),\n",
              " ('iid', 4),\n",
              " ('information content', 4),\n",
              " ('log likelihood', 4),\n",
              " ('nice property', 4),\n",
              " ('sufficient statistics', 4),\n",
              " ('thanks', 5),\n",
              " ('qr', 5),\n",
              " ('monte carlo', 5),\n",
              " ('square root', 5),\n",
              " ('shannon', 5),\n",
              " ('conditional entropy', 5),\n",
              " ('yeah', 5),\n",
              " (\"ca n't\", 6),\n",
              " ('good thing', 6),\n",
              " ('sigma square', 7),\n",
              " ('unknown quantity', 8),\n",
              " ('well', 9),\n",
              " ('bayesian', 9),\n",
              " ('maximum likelihood estimate', 9),\n",
              " ('right', 11),\n",
              " ('random number', 12),\n",
              " ('possible outcomes', 12),\n",
              " ('gaussian', 12),\n",
              " ('okay', 35)]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(blob.noun_phrases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln3V9croBV5e",
        "outputId": "a24ad271-a870-405f-eb42-f18d956471aa"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "838"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    }
  ]
}